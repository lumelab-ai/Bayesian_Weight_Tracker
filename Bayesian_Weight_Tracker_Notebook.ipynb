{
 "cells": [
  {
   "metadata": {
    "id": "6d2347d7b8b5e651"
   },
   "cell_type": "markdown",
   "source": [
    "# **Bayesian Weight Tracker**\n",
    "\n",
    "**Version**: 0.9 (Pre-Release)\n",
    "**Author**: [@dsandux](https://github.com/dsandux)\n",
    "\n",
    "This notebook contains the full analytical pipeline behind the *Bayesian Weight Tracker* project. It explores how daily habits ‚Äî including sleep, nutrition, physical activity, and hydration ‚Äî influence long-term body weight changes. The analysis is built around a **Bayesian Structural Time Series (BSTS)** model using **TensorFlow Probability**, with rigorous preprocessing, uncertainty quantification, and interpretability tools.\n",
    "\n",
    "üìò Full documentation and technical details are available on the project‚Äôs [Wiki](https://github.com/dsandux/Bayesian_Weight_Tracker/wiki).\n",
    "üß† Code contributions, issue reports, and feedback are welcome on the [main repository](https://github.com/dsandux/Bayesian_Weight_Tracker).\n"
   ],
   "id": "6d2347d7b8b5e651"
  },
  {
   "metadata": {
    "id": "7177fa55ab781122"
   },
   "cell_type": "markdown",
   "source": [
    "# 00. Environment Setup and Dependencies\n",
    "This cell installs all required Python libraries for data processing, statistical modeling, and visualization. It ensures compatibility with TensorFlow Probability and supporting tools such as NumPy, Pandas, and Keras, enabling a smooth execution of the entire analysis pipeline.\n"
   ],
   "id": "7177fa55ab781122"
  },
  {
   "metadata": {
    "id": "9778c55eccdca5a2"
   },
   "cell_type": "code",
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install pyarrow\n",
    "%pip install openpyxl\n",
    "%pip install tqdm\n",
    "%pip install tensorflow\n",
    "%pip install tensorflow-probability\n",
    "%pip install matplotlib\n",
    "%pip install networkx\n",
    "%pip install keras\n",
    "%pip install tf_keras\n",
    "%pip install ipython\n",
    "%pip install statsmodels\n",
    "%pip install scipy\n",
    "%pip install seaborn"
   ],
   "id": "9778c55eccdca5a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1e027750fc1ea742"
   },
   "cell_type": "markdown",
   "source": [
    "## 01. Health Data Preprocessing Pipeline\n",
    "---\n",
    "\n",
    "This cell reads raw health data from `dados.xlsx`, performing robust validations and outlier treatment. It converts weight from kilograms to grams for precision, then calculates 15 essential daily indicators before saving the processed data to `dados_diarios_preproc.parquet`. This prepares the dataset for subsequent analysis and modeling."
   ],
   "id": "1e027750fc1ea742"
  },
  {
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "initial_id",
    "outputId": "b39fe286-0ca6-48d7-c858-3bcf433a3a6c"
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. Basic configurations\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = Path(\"dados.xlsx\")  # Input file path\n",
    "OUTPUT_PATH = Path(\"dados_diarios_preproc.parquet\")  # Output file path\n",
    "\n",
    "# Minimum columns required for the pipeline to run\n",
    "REQUIRED_COLS = {\n",
    "    \"data\", \"peso_kg\",\n",
    "    \"gordura_pct\", \"massa_magra_pct\", \"circunferencia_cm\",\n",
    "    \"energia_total\", \"energia_atividade\",\n",
    "    \"passos\", \"minutos_em_pe\",\n",
    "    \"fc_repouso_bpm\", \"hrv_medio\",\n",
    "    \"duracao_sono_min\", \"minutos_deep\",\n",
    "    \"calorias_consumidas\", \"carboidratos_g\", \"proteinas_g\", \"agua_consumida_ml\",\n",
    "}\n",
    "\n",
    "# Columns that cannot have negative values\n",
    "NON_NEGATIVE = {\n",
    "    \"peso_kg\", \"gordura_pct\", \"massa_magra_pct\", \"circunferencia_cm\",\n",
    "    \"energia_total\", \"energia_atividade\", \"passos\", \"minutos_em_pe\",\n",
    "    \"duracao_sono_min\", \"minutos_deep\",\n",
    "    \"calorias_consumidas\", \"carboidratos_g\", \"proteinas_g\", \"agua_consumida_ml\",\n",
    "}\n",
    "\n",
    "# Quantiles used for winsorization\n",
    "QUANT_LOW, QUANT_HIGH = 0.05, 0.95\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. Utility functions\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def read_excel_unique(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Reads the Excel file, normalizes column names, and removes duplicates.\n",
    "\n",
    "    - `strip()` leading/trailing spaces\n",
    "    - converts to minimal `snake_case` (lowercase)\n",
    "    - if names are repeated, keeps only the first occurrence\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "\n",
    "    # Normalize headers\n",
    "    new_cols = []\n",
    "    seen = set()\n",
    "    for raw in df.columns:\n",
    "        col = str(raw).strip()\n",
    "        col = col.replace(\" \", \"_\").lower()\n",
    "        if col in seen:\n",
    "            warnings.warn(\n",
    "                f\"Duplicate column found and ignored: '{col}'. Only the first occurrence will be used.\",\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "            continue\n",
    "        new_cols.append(col)\n",
    "        seen.add(col)\n",
    "\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")] # Remove duplicate columns keeping the first one\n",
    "    df.columns = new_cols # Assign new normalized column names\n",
    "    return df\n",
    "\n",
    "\n",
    "def winsorize(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Applies p5‚Äìp95 winsorization inplace for the given columns.\"\"\"\n",
    "    for col in tqdm(cols, desc=\"Winsorizing\", leave=False):\n",
    "        # Calculate lower (5th) and upper (95th) quantiles for winsorization\n",
    "        lo, hi = df[col].quantile([QUANT_LOW, QUANT_HIGH], interpolation=\"linear\")\n",
    "        # Clip values outside the [lo, hi] range to lo or hi\n",
    "        df[col] = df[col].clip(lo, hi)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. Main pipeline\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    # Exit if the input data file does not exist\n",
    "    if not DATA_PATH.exists():\n",
    "        sys.exit(f\"‚ùå Data file not found at {DATA_PATH.resolve()}\")\n",
    "\n",
    "    print(\"üì•  Reading spreadsheet‚Ä¶\")\n",
    "    df = read_excel_unique(DATA_PATH)\n",
    "\n",
    "    # ----- Essential column validation -----\n",
    "    # Find any required columns that are missing from the DataFrame\n",
    "    missing = sorted(REQUIRED_COLS - set(df.columns))\n",
    "    if missing:\n",
    "        # Exit if mandatory columns are missing\n",
    "        sys.exit(f\"‚ùå Missing mandatory columns: {missing}\")\n",
    "\n",
    "    # ----- Type normalization & impossible values -----\n",
    "    print(\"üßπ  Adjusting types and values‚Ä¶\")\n",
    "    # Convert 'data' column to datetime, coercing errors to NaT (Not a Time)\n",
    "    df[\"data\"] = pd.to_datetime(df[\"data\"], errors=\"coerce\")\n",
    "\n",
    "    # For columns that should not have negative values, replace negatives with NaN\n",
    "    for col in NON_NEGATIVE & set(df.columns):\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "    # ----- Convert weight to grams and remove peso_kg -----\n",
    "    # Convert 'peso_kg' (weight in kg) to 'peso_g' (weight in grams)\n",
    "    df[\"peso_g\"] = df[\"peso_kg\"] * 1000.0  # Precision in grams\n",
    "    # Drop the original 'peso_kg' column\n",
    "    df.drop(columns=\"peso_kg\", inplace=True)\n",
    "\n",
    "    # ----- p5‚Äìp95 winsorization -----\n",
    "    # Select all numeric columns for winsorization\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Apply winsorization to numeric columns\n",
    "    winsorize(df, numeric_cols)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 4. Indicator calculation\n",
    "    # -----------------------------------------------------------------\n",
    "    print(\"üßÆ  Calculating indicators‚Ä¶\")\n",
    "\n",
    "    # Initialize an empty DataFrame for indicators\n",
    "    df_ind = pd.DataFrame()\n",
    "    # Copy 'data' column to the indicators DataFrame\n",
    "    df_ind[\"data\"] = df[\"data\"]\n",
    "\n",
    "    # Direct copies of columns as indicators\n",
    "    df_ind[\"Fat_pct_mean\"]          = df[\"gordura_pct\"]\n",
    "    df_ind[\"Circumference_cm_med\"] = df[\"circunferencia_cm\"]\n",
    "    df_ind[\"Total_kcal_mean\"]       = df[\"energia_total\"]\n",
    "    df_ind[\"Steps_mean\"]            = df[\"passos\"]\n",
    "    df_ind[\"RestingHR_mean\"]        = df[\"fc_repouso_bpm\"]\n",
    "    df_ind[\"HRV_mean\"]              = df[\"hrv_medio\"]\n",
    "    df_ind[\"SleepDur_mean\"]         = df[\"duracao_sono_min\"]\n",
    "    df_ind[\"Water_ml_mean\"]         = df[\"agua_consumida_ml\"]\n",
    "\n",
    "    # Derived indicators / ratios\n",
    "    df_ind[\"Lean_to_Fat_ratio\"]       = df[\"massa_magra_pct\"] / df[\"gordura_pct\"]\n",
    "    df_ind[\"Activity_kcal_fraction\"]  = df[\"energia_atividade\"] / df[\"energia_total\"]\n",
    "    df_ind[\"Standing_minutes_raw\"]    = df[\"minutos_em_pe\"]  # For future standard deviation calculation\n",
    "    df_ind[\"DeepFrac_mean\"]           = df[\"minutos_deep\"] / df[\"duracao_sono_min\"]\n",
    "\n",
    "    # Ratios per kg ‚Äì dividing weight in grams by 1000\n",
    "    kg_denominator = df[\"peso_g\"] / 1000.0 # Convert grams back to kg for ratios\n",
    "    df_ind[\"Carb_per_kg_mean\"]        = df[\"carboidratos_g\"] / kg_denominator\n",
    "    df_ind[\"Protein_per_kg_mean\"]     = df[\"proteinas_g\"] / kg_denominator\n",
    "\n",
    "    # Caloric deficit remains the same calculation\n",
    "    df_ind[\"Cal_deficit_mean\"]        = df[\"calorias_consumidas\"] - df[\"energia_total\"]\n",
    "\n",
    "    # ----- Save parquet -----\n",
    "    print(\"üíæ  Saving Parquet‚Ä¶\")\n",
    "    # Create parent directories if they don't exist\n",
    "    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Sort data by 'data' column and save to a Parquet file\n",
    "    df_ind.sort_values(\"data\").to_parquet(OUTPUT_PATH, index=False, engine=\"pyarrow\")\n",
    "\n",
    "    # Quick summary\n",
    "    print(\"\\n‚úÖ  Preprocessing complete!\")\n",
    "    print(f\"File saved to: {OUTPUT_PATH.resolve()}\")\n",
    "    print(\"\\nNumeric summary (post-winsorization):\")\n",
    "    # Print descriptive statistics for the indicators DataFrame\n",
    "    print(df_ind.describe().loc[[\"mean\", \"std\", \"min\", \"max\"]].round(2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b112ed5f03b68c65"
   },
   "cell_type": "markdown",
   "source": [
    "## 02. Window Construction Pipeline\n",
    "-----\n",
    "This cell constructs 91-day sliding windows from daily preprocessed health indicators and raw weight data. For each window, it aggregates various metrics (mean, median, standard deviation) and calculates a key target: the weight change slope in kilograms per week. The resulting dataset, with one row per window, is saved for subsequent modeling."
   ],
   "id": "b112ed5f03b68c65"
  },
  {
   "metadata": {
    "id": "268e71663dc587d4",
    "outputId": "7f6a44fd-de41-4490-e76e-bdb455d3dc9e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. Paths and parameters\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "DAILY_PARQUET = Path(\"dados_diarios_preproc.parquet\") # Path to the preprocessed daily data Parquet file\n",
    "EXCEL_SOURCE = Path(\"dados.xlsx\") # Path to the original Excel source file for raw weight data\n",
    "WINDOW_PARQUET = Path(\"janelas_91d.parquet\") # Output path for the aggregated window data Parquet file\n",
    "\n",
    "WINDOW_SIZE = 91   # Number of days in each window\n",
    "STEP_SIZE = 7      # Number of days to slide the window for each step\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. Defensive Excel reading (without duplicate columns)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def read_excel_unique(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Reads Excel, normalizes headers, and discards duplicate columns.\"\"\"\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "\n",
    "    norm_cols, seen = [], set() # Initialize lists for normalized columns and a set for seen column names\n",
    "    for raw in df.columns:\n",
    "        col = str(raw).strip().replace(\" \", \"_\").lower() # Normalize column name: strip spaces, replace spaces with underscores, convert to lowercase\n",
    "        if col in seen:\n",
    "            warnings.warn(f\"Duplicate column ignored: {col}\") # Warn if a duplicate column is found\n",
    "            continue # Skip adding this duplicate column\n",
    "        norm_cols.append(col) # Add normalized column name to list\n",
    "        seen.add(col) # Add to set of seen column names\n",
    "\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")] # Filter out duplicate columns from the DataFrame, keeping the first occurrence\n",
    "    df.columns = norm_cols # Assign the normalized column names to the DataFrame\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. Window aggregation functions\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def slope_kg_per_week(sub: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates the slope of weight (kg/week) within a window.\"\"\"\n",
    "    y = sub[\"peso_g\"].values / 1000.0  # Convert weight from grams to kilograms\n",
    "    if np.isnan(y).any():\n",
    "        return np.nan # Return NaN if any weight value is missing\n",
    "    x = np.arange(len(sub))            # Create an array representing days (0 to 90 for a 91-day window)\n",
    "    slope_day = np.polyfit(x, y, 1)[0] # Calculate the slope of the linear regression (kg/day)\n",
    "    return slope_day * 7               # Convert slope from kg/day to kg/week\n",
    "\n",
    "\n",
    "def aggregate_window(sub: pd.DataFrame) -> dict:\n",
    "    \"\"\"Calculates window metrics (mean, median, std) + target.\"\"\"\n",
    "    out: dict[str, float] = {}\n",
    "\n",
    "    # --- Means (11) ---\n",
    "    mean_cols = {\n",
    "        \"Fat_pct_mean\", \"Total_kcal_mean\", \"Steps_mean\", \"RestingHR_mean\",\n",
    "        \"HRV_mean\", \"SleepDur_mean\", \"Water_ml_mean\", \"Lean_to_Fat_ratio\",\n",
    "        \"Activity_kcal_fraction\", \"DeepFrac_mean\", \"Cal_deficit_mean\",\n",
    "    }\n",
    "    for c in mean_cols:\n",
    "        out[c] = sub[c].mean() # Calculate the mean for each specified column\n",
    "\n",
    "    # --- Median (1) ---\n",
    "    out[\"Circumference_cm_med\"] = sub[\"Circumference_cm_med\"].median() # Calculate the median for circumference\n",
    "\n",
    "    # --- Standard deviation (1) ---\n",
    "    out[\"Standing_variability\"] = sub[\"Standing_minutes_raw\"].std() # Calculate the standard deviation for standing minutes\n",
    "\n",
    "    # --- Ratios per kg (2) ---\n",
    "    out[\"Carb_per_kg_mean\"]    = sub[\"Carb_per_kg_mean\"].mean() # Calculate the mean of carbohydrates per kg\n",
    "    out[\"Protein_per_kg_mean\"] = sub[\"Protein_per_kg_mean\"].mean() # Calculate the mean of protein per kg\n",
    "\n",
    "    # --- Target ---\n",
    "    out[\"slope_kg_per_week\"] = slope_kg_per_week(sub) # Calculate the target slope (weight change per week)\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_windows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generates the final DataFrame: 1 row per sliding window.\"\"\"\n",
    "    records = [] # List to store aggregated window records\n",
    "    # Determine the starting indices for each window\n",
    "    starts = range(0, len(df) - WINDOW_SIZE + 1, STEP_SIZE)\n",
    "    for i in tqdm(starts, desc=\"Generating windows\"): # Iterate through each window start index with a progress bar\n",
    "        sub = df.iloc[i : i + WINDOW_SIZE] # Extract the sub-DataFrame for the current window\n",
    "        rec = aggregate_window(sub) # Aggregate metrics for the current window\n",
    "        rec[\"window_start\"] = sub[\"data\"].iloc[0] # Record the start date of the window\n",
    "        records.append(rec) # Add the aggregated record to the list\n",
    "    return pd.DataFrame(records) # Convert the list of records into a DataFrame\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. Main pipeline\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    # 4.1 Load daily indicators\n",
    "    # Check if the preprocessed daily data file exists\n",
    "    if not DAILY_PARQUET.exists():\n",
    "        sys.exit(\"‚ùå Daily parquet not found. Run 01_preprocess.py first.\") # Exit if not found\n",
    "    df_ind = pd.read_parquet(DAILY_PARQUET) # Load the daily indicators DataFrame\n",
    "\n",
    "    # 4.2 Append weight (g)\n",
    "    # Read relevant columns from the original Excel source\n",
    "    df_weight = read_excel_unique(EXCEL_SOURCE)[[\"data\", \"peso_kg\"]]\n",
    "    df_weight[\"data\"] = pd.to_datetime(df_weight[\"data\"], errors=\"coerce\") # Convert 'data' column to datetime\n",
    "    df_weight[\"peso_g\"] = df_weight[\"peso_kg\"] * 1000.0 # Convert weight from kg to grams\n",
    "\n",
    "    # Merge (inner) ‚Äì removes dates without indicator or without weight\n",
    "    df_full = (\n",
    "        df_ind.merge(df_weight[[\"data\", \"peso_g\"]], on=\"data\", how=\"inner\") # Inner merge indicators with weight data on 'data'\n",
    "               .sort_values(\"data\") # Sort the merged DataFrame by date\n",
    "               .reset_index(drop=True) # Reset index after sorting\n",
    "    )\n",
    "\n",
    "    # 4.3 Generate windows\n",
    "    df_windows = generate_windows(df_full) # Call function to generate sliding windows\n",
    "\n",
    "    # 4.4 Save\n",
    "    WINDOW_PARQUET.parent.mkdir(parents=True, exist_ok=True) # Create parent directories for the output file if they don't exist\n",
    "    df_windows.to_parquet(WINDOW_PARQUET, index=False, engine=\"pyarrow\") # Save the aggregated window data to a Parquet file\n",
    "\n",
    "    # 4.5 Summary\n",
    "    print(\"‚úÖ  Windows saved to:\", WINDOW_PARQUET.resolve()) # Print confirmation message and file path\n",
    "    print(\"Total windows:\", len(df_windows)) # Print the total number of generated windows\n",
    "    print(df_windows.head()) # Print the first few rows of the generated windows DataFrame\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "268e71663dc587d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9e26ed8707bd80f4"
   },
   "cell_type": "markdown",
   "source": [
    "## 03. Feature Correlation Analysis\n",
    "----\n",
    "\n",
    "This cell performs an initial analysis of the relationships between the features generated in the previous step. It calculates and visualizes the Pearson correlation matrix for the 91-day window data, identifying highly correlated feature pairs.\n",
    "\n",
    "| Name               | Description                                   |\n",
    "| :----------------- |:----------------------------------------------|\n",
    "| `corr_matrix`      | Pearson correlation matrix of all features    |\n",
    "| `high_corr_df`     | DataFrame of feature pairs with  corr >= 0.80 |\n",
    "\n",
    "---"
   ],
   "id": "9e26ed8707bd80f4"
  },
  {
   "metadata": {
    "id": "823b509b20dc6e9a",
    "outputId": "53a4d3db-881a-436f-dca4-74f7c435fb66",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. Load the windows data ---\n",
    "# Loads the preprocessed 91-day window data from a Parquet file into a Pandas DataFrame.\n",
    "df_windows = pd.read_parquet(\"janelas_91d.parquet\")\n",
    "\n",
    "# --- 2. Select feature columns ---\n",
    "# Defines a list of feature column names to be used for correlation analysis.\n",
    "feature_cols = [\n",
    "    \"Fat_pct_mean\", \"Total_kcal_mean\", \"Steps_mean\", \"RestingHR_mean\",\n",
    "    \"HRV_mean\", \"SleepDur_mean\", \"Water_ml_mean\", \"Lean_to_Fat_ratio\",\n",
    "    \"Activity_kcal_fraction\", \"DeepFrac_mean\", \"Cal_deficit_mean\",\n",
    "    \"Circumference_cm_med\", \"Standing_variability\",\n",
    "    \"Carb_per_kg_mean\", \"Protein_per_kg_mean\"\n",
    "]\n",
    "# Selects only the specified feature columns from the DataFrame for analysis.\n",
    "X = df_windows[feature_cols]\n",
    "\n",
    "# --- 3. Correlation matrix ---\n",
    "# Calculates the pairwise Pearson correlation coefficient for all selected feature columns.\n",
    "corr_matrix = X.corr()\n",
    "\n",
    "# --- 4. Heatmap (now with viridis) ---\n",
    "# Creates a figure and a set of subplots for the heatmap visualization.\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# Generates a heatmap of the correlation matrix, with values ranging from -1 to 1, using the 'viridis' colormap.\n",
    "im = ax.imshow(corr_matrix.values, vmin=-1, vmax=1, cmap=\"viridis\")\n",
    "# Sets the x-axis ticks to correspond to the number of feature columns.\n",
    "ax.set_xticks(range(len(feature_cols)))\n",
    "# Sets the y-axis ticks to correspond to the number of feature columns.\n",
    "ax.set_yticks(range(len(feature_cols)))\n",
    "# Sets the x-axis tick labels to the feature names, rotated for readability.\n",
    "ax.set_xticklabels(feature_cols, rotation=90)\n",
    "# Sets the y-axis tick labels to the feature names.\n",
    "ax.set_yticklabels(feature_cols)\n",
    "# Sets the title of the heatmap.\n",
    "ax.set_title(\"Feature Correlation Matrix (Pearson)\")\n",
    "# Adds a color bar next to the heatmap to indicate correlation values.\n",
    "plt.colorbar(im, ax=ax, label=\"Correlation coefficient\")\n",
    "# Adjusts plot parameters for a tight layout, preventing labels from overlapping.\n",
    "plt.tight_layout()\n",
    "# Displays the generated heatmap.\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Pairs with |corr| ‚â• 0.80 ---\n",
    "threshold = 0.80 # Defines the correlation threshold for identifying highly correlated pairs.\n",
    "rows = [\n",
    "    # Iterates through all unique pairs of feature columns.\n",
    "    {\"var1\": feature_cols[i], \"var2\": feature_cols[j], \"corr\": corr_matrix.iat[i, j]}\n",
    "    for i, j in combinations(range(len(feature_cols)), 2)\n",
    "    # Filters pairs where the absolute correlation coefficient is greater than or equal to the threshold.\n",
    "    if abs(corr_matrix.iat[i, j]) >= threshold\n",
    "]\n",
    "# Creates a DataFrame from the identified highly correlated pairs.\n",
    "high_corr_df = (pd.DataFrame(rows)\n",
    "                # Sorts the DataFrame by the absolute correlation value in descending order.\n",
    "                .sort_values(\"corr\", key=np.abs, ascending=False)\n",
    "                # Resets the DataFrame index.\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "# Displays the DataFrame containing highly correlated feature pairs.\n",
    "display(high_corr_df)"
   ],
   "id": "823b509b20dc6e9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "425aa47fa68614df"
   },
   "cell_type": "markdown",
   "source": [
    "## 04. Collinearity Filtering and Feature Statistics\n",
    "\n",
    "This cell addresses multicollinearity among features and provides statistical insights into the selected and removed variables. It groups highly correlated features, selects the best representative from each group, and then reports the internal group correlation and the correlation with the target variable (`slope_kg_per_week`) for all features.\n",
    "\n",
    "| Name                 | Description                                                                                                                                                                                            |\n",
    "| :------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `FILTERED_PARQUET`   | Parquet file containing the filtered features (after collinearity resolution) along with the target and date columns.                                                                                      |\n",
    "| `REMOVED_TXT`        | Text file listing the features that were removed due to high collinearity, along with their average absolute internal group correlation and absolute correlation with the target.                       |\n",
    "\n",
    "---"
   ],
   "id": "425aa47fa68614df"
  },
  {
   "metadata": {
    "id": "e3f89a878d6ab57d",
    "outputId": "7100f6b9-df17-4f7b-b678-f27ee7164c14",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------------------------------------------\n",
    "WINDOW_PARQUET = Path(\"janelas_91d.parquet\") # Path to the input 91-day window data Parquet file\n",
    "FILTERED_PARQUET = Path(\"janelas_91d_filtered.parquet\") # Output path for the filtered Parquet file\n",
    "REMOVED_TXT = Path(\"janelas_91d_removed_cols.txt\") # Output path for the text file listing removed columns\n",
    "\n",
    "THRESHOLD = 0.80  # Absolute Pearson correlation threshold for collinearity grouping\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Fat_pct_mean\", \"Total_kcal_mean\", \"Steps_mean\", \"RestingHR_mean\",\n",
    "    \"HRV_mean\", \"SleepDur_mean\", \"Water_ml_mean\", \"Lean_to_Fat_ratio\",\n",
    "    \"Activity_kcal_fraction\", \"DeepFrac_mean\", \"Cal_deficit_mean\",\n",
    "    \"Circumference_cm_med\", \"Standing_variability\",\n",
    "    \"Carb_per_kg_mean\", \"Protein_per_kg_mean\",\n",
    "] # List of feature columns to be analyzed for collinearity\n",
    "\n",
    "TARGET_COL = \"slope_kg_per_week\" # Name of the target variable column\n",
    "DATE_COL = \"window_start\" # Name of the date column\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Utilities\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def build_graph(corr: pd.DataFrame, cols: list[str], thr: float) -> nx.Graph:\n",
    "    \"\"\"Builds a graph where nodes are columns and edges connect highly correlated columns.\"\"\"\n",
    "    g = nx.Graph() # Initialize an empty undirected graph\n",
    "    g.add_nodes_from(cols) # Add all feature columns as nodes to the graph\n",
    "    for i, c1 in enumerate(cols):\n",
    "        for c2 in cols[i + 1 :]: # Iterate through unique pairs of columns\n",
    "            if abs(corr.loc[c1, c2]) >= thr: # If the absolute correlation between two columns is above the threshold\n",
    "                g.add_edge(c1, c2) # Add an edge between these two columns\n",
    "    return g\n",
    "\n",
    "\n",
    "def pick_keep_column(group: list[str], corr: pd.DataFrame, y_corr: pd.Series) -> str:\n",
    "    \"\"\"Selects the column to keep from a highly correlated group.\n",
    "\n",
    "    Selection criteria:\n",
    "    1. Smallest average absolute internal correlation within the group.\n",
    "    2. Tie-breaker: Highest absolute correlation with the target variable (y).\n",
    "    \"\"\"\n",
    "    mean_r = {\n",
    "        col: np.mean([abs(corr.loc[col, other]) for other in group if other != col])\n",
    "        for col in group\n",
    "    } # Calculate the average absolute internal correlation for each column in the group\n",
    "    min_mean = min(mean_r.values()) # Find the minimum average internal correlation\n",
    "    candidates = [c for c, m in mean_r.items() if np.isclose(m, min_mean)] # Identify columns with the minimum average internal correlation\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0] # If only one candidate, return it\n",
    "    # If there's a tie, choose the candidate with the highest absolute correlation with the target (y)\n",
    "    return max(candidates, key=lambda c: abs(y_corr[c]))\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Main pipeline\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    # Check if the input window data file exists\n",
    "    if not WINDOW_PARQUET.exists():\n",
    "        sys.exit(\"‚ùå Window file not found.\")\n",
    "\n",
    "    df = pd.read_parquet(WINDOW_PARQUET) # Load the window data DataFrame\n",
    "    X = df[FEATURE_COLS] # Extract feature columns\n",
    "    y = df[TARGET_COL] # Extract the target column\n",
    "\n",
    "    corr = X.corr() # Calculate the correlation matrix for features\n",
    "    y_corr = X.apply(lambda col: col.corr(y))  # Calculate the correlation of each feature with the target variable (y)\n",
    "\n",
    "    # Build a graph to identify highly correlated groups (connected components)\n",
    "    graph = build_graph(corr, FEATURE_COLS, THRESHOLD)\n",
    "    groups = list(nx.connected_components(graph)) # Get list of connected components (groups of highly correlated features)\n",
    "\n",
    "    kept, removed_info = [], [] # Initialize lists for kept columns and information about removed columns\n",
    "    for grp in groups:\n",
    "        grp_list = sorted(grp, key=lambda c: FEATURE_COLS.index(c)) # Sort group members by their original index in FEATURE_COLS\n",
    "        if len(grp_list) == 1:\n",
    "            kept.append(grp_list[0]) # If a group has only one member, keep it\n",
    "            continue\n",
    "        keep_col = pick_keep_column(grp_list, corr, y_corr) # Select the best column to keep from the group\n",
    "        kept.append(keep_col) # Add the selected column to the 'kept' list\n",
    "        for col in grp_list:\n",
    "            if col == keep_col:\n",
    "                continue # Skip the kept column\n",
    "            # Calculate the average absolute internal correlation for the removed column\n",
    "            mean_internal = np.mean([abs(corr.loc[col, other]) for other in grp_list if other != col])\n",
    "            # Store the removed column's name, its average internal correlation, and its absolute correlation with y\n",
    "            removed_info.append((col, mean_internal, abs(y_corr[col])))\n",
    "\n",
    "    # Sort the kept columns by their original order\n",
    "    kept.sort(key=lambda c: FEATURE_COLS.index(c))\n",
    "    # Sort the removed columns information by their original order\n",
    "    removed_info.sort(key=lambda t: FEATURE_COLS.index(t[0]))\n",
    "\n",
    "    # --- Report ---\n",
    "    print(\"Kept (\", len(kept), \") ‚Äì statistics:\")\n",
    "    for col in kept:\n",
    "        # Calculate the average absolute internal correlation for the kept column within its original group\n",
    "        internal_vals = [abs(corr.loc[col, other]) for other in FEATURE_COLS if other != col and abs(corr.loc[col, other]) >= THRESHOLD]\n",
    "        mean_int = np.mean(internal_vals) if internal_vals else np.nan # If no internal correlations, set to NaN\n",
    "        mean_text = f\"{mean_int:.3f}\" if not np.isnan(mean_int) else \"‚Äî\" # Format for display\n",
    "        # Print statistics for the kept column\n",
    "        print(f\"  {col:<25} avg|œÅ_group|={mean_text:<5}  |œÅ(col,y)|={abs(y_corr[col]):.3f}\")\n",
    "\n",
    "    print(\"\\nRemoved (\", len(removed_info), \") ‚Äì avg|œÅ| and |œÅ(col,y)|:\")\n",
    "    for col, m_int, rho_y in removed_info:\n",
    "        # Print statistics for the removed column\n",
    "        print(f\"  {col:<25} {m_int:.3f} {rho_y:.3f}\")\n",
    "\n",
    "    # Save filtered Parquet file\n",
    "    # Select kept features, target, and date columns and save to Parquet\n",
    "    df[kept + [TARGET_COL, DATE_COL]].to_parquet(FILTERED_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    # Save removed columns information to a text file\n",
    "    with REMOVED_TXT.open(\"w\") as f:\n",
    "        for col, m_int, rho_y in removed_info:\n",
    "            f.write(f\"{col}\\t{m_int:.3f}\\t{rho_y:.3f}\\n\")\n",
    "\n",
    "    print(\"\\n‚úÖ Filtered Parquet saved to:\", FILTERED_PARQUET.resolve())\n",
    "    print(\"Detailed list saved to:\", REMOVED_TXT.resolve())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "e3f89a878d6ab57d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5693f557a409bfcd"
   },
   "cell_type": "markdown",
   "source": [
    "## 05. Correlation Heatmaps: Before and After Collinearity Filtering\n",
    "----\n",
    "This cell visualizes the correlation matrices of features *before* and *after* the collinearity filtering process. It presents two heatmaps side-by-side, maintaining identical square cell sizes and aligning their bases to clearly show the impact of the filtering on feature relationships.\n",
    "\n",
    "| Name               | Description                                    |\n",
    "| :----------------- | :--------------------------------------------- |\n",
    "| `df_before`        | DataFrame containing features before filtering. |\n",
    "| `df_after`         | DataFrame containing features after filtering.  |\n",
    "| `corr_before`      | Correlation matrix of features before filtering. |\n",
    "| `corr_after`       | Correlation matrix of features after filtering. |\n",
    "\n",
    "---\n"
   ],
   "id": "5693f557a409bfcd"
  },
  {
   "metadata": {
    "id": "6ca084e885b48dab",
    "outputId": "1c6aa86a-db3a-45f7-8c9b-b62b12bae594",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Define paths for input and target/date columns\n",
    "BEFORE_PATH = \"janelas_91d.parquet\" # Path to the Parquet file with data before filtering\n",
    "AFTER_PATH  = \"janelas_91d_filtered.parquet\" # Path to the Parquet file with data after filtering\n",
    "TARGET = \"slope_kg_per_week\" # Name of the target column\n",
    "DATE   = \"window_start\" # Name of the date column\n",
    "\n",
    "# ---- Load data ----\n",
    "# Load the dataframes from the specified Parquet files\n",
    "df_before = pd.read_parquet(BEFORE_PATH)\n",
    "df_after  = pd.read_parquet(AFTER_PATH)\n",
    "\n",
    "# Identify feature columns by excluding target and date columns\n",
    "feat_before = [c for c in df_before.columns if c not in {TARGET, DATE}]\n",
    "feat_after  = [c for c in df_after.columns  if c not in {TARGET, DATE}]\n",
    "\n",
    "# Calculate correlation matrices for features before and after filtering\n",
    "corr_before = df_before[feat_before].corr()\n",
    "corr_after  = df_after[feat_after].corr()\n",
    "\n",
    "# Get the number of features in each set\n",
    "n_before = len(feat_before)\n",
    "n_after  = len(feat_after)\n",
    "\n",
    "# ---- Figure layout ----\n",
    "cell = 0.4  # Base size in inches per cell\n",
    "# Calculate figure width based on the number of features and additional space for colorbar and margins\n",
    "fig_width = cell * (n_before + n_after) + 2\n",
    "# Calculate figure height based on the number of features in the 'before' plot (which is typically larger)\n",
    "fig_height = cell * n_before\n",
    "fig = plt.figure(figsize=(fig_width, fig_height)) # Create the figure with calculated dimensions\n",
    "\n",
    "# Define a GridSpec for layout: 1 row, 3 columns with specified width ratios for before, after, and colorbar\n",
    "gs = GridSpec(1, 3, width_ratios=[n_before, n_after, 0.6], wspace=0.3)\n",
    "ax_before = fig.add_subplot(gs[0]) # Add subplot for 'before' heatmap\n",
    "ax_after  = fig.add_subplot(gs[1]) # Add subplot for 'after' heatmap\n",
    "cb_ax     = fig.add_subplot(gs[2]) # Add subplot for the colorbar\n",
    "\n",
    "# Get a copy of the 'viridis' colormap\n",
    "cmap = mpl.colormaps[\"viridis\"].copy()\n",
    "\n",
    "# ---- Helper: clean spines/anchor and set aspect ----\n",
    "# Iterate through both axes to apply common formatting\n",
    "for ax in (ax_before, ax_after):\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False) # Hide the spines (borders) of the plot\n",
    "    ax.set_anchor(\"SW\") # Anchor the subplot content to the south-west (bottom-left) corner\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\") # Set aspect ratio to 'equal' so cells are square\n",
    "\n",
    "ax_after.set_facecolor(\"white\") # Set the background color of the 'after' plot to white\n",
    "\n",
    "# ---- Plot BEFORE ----\n",
    "# Display the 'before' correlation matrix as an image (heatmap)\n",
    "ax_before.imshow(corr_before.values, vmin=-1, vmax=1, cmap=cmap, origin=\"lower\")\n",
    "ax_before.set_xticks(range(n_before)) # Set x-axis ticks\n",
    "ax_before.set_yticks(range(n_before)) # Set y-axis ticks\n",
    "ax_before.set_xticklabels(feat_before, rotation=90, fontsize=7) # Set x-axis labels with rotation\n",
    "ax_before.set_yticklabels(feat_before, fontsize=7) # Set y-axis labels\n",
    "ax_before.set_title(\"Antes do filtro\") # Set title for the 'before' plot\n",
    "\n",
    "# ---- Plot AFTER ----\n",
    "# Display the 'after' correlation matrix as an image (heatmap)\n",
    "ax_after.imshow(corr_after.values, vmin=-1, vmax=1, cmap=cmap, origin=\"lower\")\n",
    "ax_after.set_xticks(range(n_after)) # Set x-axis ticks\n",
    "ax_after.set_yticks(range(n_after)) # Set y-axis ticks\n",
    "ax_after.set_xticklabels(feat_after, rotation=90, fontsize=7) # Set x-axis labels with rotation\n",
    "ax_after.set_yticklabels(feat_after, fontsize=7) # Set y-axis labels\n",
    "ax_after.set_title(\"Depois do filtro\") # Set title for the 'after' plot\n",
    "\n",
    "# Align bases ‚Äì ensured by anchor SW and equal aspect; no extra code\n",
    "\n",
    "# ---- Colorbar ----\n",
    "# Add a color bar to the figure, linked to the 'before' heatmap, with a specified label\n",
    "fig.colorbar(ax_before.images[0], cax=cb_ax, label=\"Correla√ß√£o de Pearson\")\n",
    "cb_ax.yaxis.set_ticks_position(\"right\") # Position the color bar ticks on the right\n",
    "\n",
    "# ---- Layout ----\n",
    "# Adjust plot parameters for a tight layout, leaving space for the colorbar (rect)\n",
    "fig.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "plt.show() # Display the plots"
   ],
   "id": "6ca084e885b48dab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1de1735458fc39a2"
   },
   "cell_type": "markdown",
   "source": [
    "## 06. Collinearity Network Visualization\n",
    "---\n",
    "This cell visualizes the collinearity network of features, providing a graphical representation of highly correlated variables. Nodes represent features, and edges connect features with a Pearson correlation coefficient (absolute value) greater than or equal to 0.80. The visualization uses the `viridis` color palette to distinguish between features kept after filtering (darker tone) and those eliminated (lighter tone), with a clear legend for easy interpretation."
   ],
   "id": "1de1735458fc39a2"
  },
  {
   "metadata": {
    "id": "f34ac875497dcfe9",
    "outputId": "1fb1d6a7-f39a-4f5e-a458-f1a8971828ce",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Define paths for input Parquet files and key column names\n",
    "PARQUET_BEFORE = \"janelas_91d.parquet\" # Path to the Parquet file containing all features before filtering\n",
    "PARQUET_AFTER  = \"janelas_91d_filtered.parquet\" # Path to the Parquet file containing features after collinearity filtering\n",
    "TARGET = \"slope_kg_per_week\" # Name of the target column\n",
    "DATE   = \"window_start\" # Name of the date column\n",
    "THRESHOLD = 0.80 # Absolute Pearson correlation threshold for drawing edges in the graph\n",
    "\n",
    "# ---- Load datasets ----\n",
    "# Load the DataFrame containing all original features\n",
    "df_before = pd.read_parquet(PARQUET_BEFORE)\n",
    "# Load the DataFrame containing features after collinearity filtering\n",
    "df_after  = pd.read_parquet(PARQUET_AFTER)\n",
    "\n",
    "# Identify all feature columns from the 'before' DataFrame by excluding target and date columns\n",
    "features_all  = [c for c in df_before.columns if c not in {TARGET, DATE}]\n",
    "# Identify features that were kept after filtering from the 'after' DataFrame\n",
    "features_keep = [c for c in df_after.columns  if c not in {TARGET, DATE}]\n",
    "\n",
    "# Calculate the correlation matrix for all original features\n",
    "corr = df_before[features_all].corr()\n",
    "\n",
    "# ---- Build graph ----\n",
    "G = nx.Graph() # Initialize an empty undirected graph\n",
    "G.add_nodes_from(features_all) # Add all feature names as nodes to the graph\n",
    "for i, c1 in enumerate(features_all):\n",
    "    for c2 in features_all[i + 1 :]: # Iterate through unique pairs of features\n",
    "        if abs(corr.loc[c1, c2]) >= THRESHOLD: # If the absolute correlation between features is above the threshold\n",
    "            # Add an edge between the features, with the absolute correlation as its weight\n",
    "            G.add_edge(c1, c2, weight=abs(corr.loc[c1, c2]))\n",
    "\n",
    "# ---- Node styling (viridis) ----\n",
    "cmap = mpl.colormaps[\"viridis\"] # Get the 'viridis' colormap\n",
    "color_keep = cmap(0.9)   # Define a darker color from viridis for kept nodes\n",
    "color_drop = cmap(0.4)   # Define a lighter color from viridis for dropped nodes\n",
    "# Assign colors to nodes based on whether they were kept or dropped\n",
    "node_colors = [color_keep if n in features_keep else color_drop for n in G.nodes]\n",
    "# Assign sizes to nodes, larger for kept nodes, smaller for dropped nodes\n",
    "node_sizes  = [900 if n in features_keep else 500 for n in G.nodes]\n",
    "\n",
    "# Compute node positions using the spring layout algorithm for visualization, with a fixed seed for reproducibility\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "plt.figure(figsize=(10, 8)) # Create a matplotlib figure with a specified size\n",
    "\n",
    "# Draw edges with alpha proportional to their weight (correlation strength)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in G.edges] # Get all edge weights\n",
    "max_w = max(edge_weights) if edge_weights else 1 # Find the maximum weight for normalization, handle empty list\n",
    "for (u, v), w in zip(G.edges, edge_weights):\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, edgelist=[(u, v)], width=2, # Draw individual edges with a fixed width\n",
    "        alpha=w / max_w, edge_color=\"#555555\" # Set alpha (transparency) based on normalized weight, and color\n",
    "    )\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes) # Draw nodes with specified colors and sizes\n",
    "nx.draw_networkx_labels(G, pos, font_size=8) # Draw node labels (feature names)\n",
    "\n",
    "# ---- Legend ----\n",
    "# Create Patch objects for the legend to represent kept and eliminated nodes\n",
    "legend_elements = [\n",
    "    Patch(facecolor=color_keep, edgecolor='none', label='Mantida'), # Label for kept features\n",
    "    Patch(facecolor=color_drop, edgecolor='none', label='Eliminada'), # Label for eliminated features\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc=\"upper left\") # Add the legend to the plot\n",
    "\n",
    "plt.title(\"Rede de Colinearidade (|œÅ| ‚â• 0.80)\") # Set the title of the plot\n",
    "plt.axis(\"off\") # Turn off the axis (no frame, ticks, or labels)\n",
    "plt.tight_layout() # Adjust layout to prevent elements from overlapping\n",
    "plt.show() # Display the plot"
   ],
   "id": "f34ac875497dcfe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "dd9653475c3561cd"
   },
   "cell_type": "markdown",
   "source": [
    "## 07. Model Data Preparation: Normalization, Split, and Statistics\n",
    "---\n",
    "This cell prepares the preprocessed and filtered window data for model training and evaluation. It normalizes features using z-score scaling based on the training set's statistics, performs a chronological split into initial training and walk-forward sets, and saves the prepared datasets along with the calculated scalers.\n",
    "\n",
    "| Name             | Description                                          |\n",
    "| :--------------- | :--------------------------------------------------- |\n",
    "| `TRAIN_PARQUET`  | Parquet file for the initial training dataset.       |\n",
    "| `WALK_PARQUET`   | Parquet file for the walk-forward dataset.           |\n",
    "| `SCALER_JSON`    | JSON file containing mean and standard deviation for feature normalization. |\n",
    "| `df_train`       | DataFrame for the initial training data.             |\n",
    "| `df_walk`        | DataFrame for the walk-forward data.                 |"
   ],
   "id": "dd9653475c3561cd"
  },
  {
   "metadata": {
    "id": "cd466694ca2dd446",
    "outputId": "cb9a04d4-0260-4a53-b1f1-43ad93d0757d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths for input and output files\n",
    "FILTERED_PARQUET = Path(\"janelas_91d_filtered.parquet\") # Path to the input filtered window data\n",
    "TRAIN_PARQUET    = Path(\"train_initial.parquet\")    # Output path for the initial training dataset\n",
    "WALK_PARQUET     = Path(\"walk_forward.parquet\")     # Output path for the walk-forward validation dataset\n",
    "SCALER_JSON      = Path(\"feature_scalers.json\")      # Output path for the JSON file containing feature scalers\n",
    "\n",
    "# Define column names and split fraction\n",
    "TARGET_COL = \"slope_kg_per_week\" # Name of the target variable column\n",
    "DATE_COL   = \"window_start\"   # Name of the date column\n",
    "TRAIN_FRAC = 0.70  # Fraction of oldest windows to use for initial training (70%)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Pipeline\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    # Check if the filtered Parquet file exists; exit if not\n",
    "    if not FILTERED_PARQUET.exists():\n",
    "        sys.exit(\"‚ùå Filtered Parquet not found. Run the collinearity filter first.\")\n",
    "\n",
    "    # Load the filtered data, sort by date, and reset index\n",
    "    df = pd.read_parquet(FILTERED_PARQUET).sort_values(DATE_COL).reset_index(drop=True)\n",
    "    # Identify feature columns by excluding target and date columns\n",
    "    feature_cols = [c for c in df.columns if c not in {TARGET_COL, DATE_COL}]\n",
    "\n",
    "    # ---- Temporal Split ----\n",
    "    # Calculate the index for splitting the DataFrame based on TRAIN_FRAC\n",
    "    split_idx = int(len(df) * TRAIN_FRAC)\n",
    "    # Create the initial training DataFrame (oldest 70% of data)\n",
    "    df_train = df.iloc[:split_idx].copy()\n",
    "    # Create the walk-forward validation DataFrame (remaining 30% of data)\n",
    "    df_walk  = df.iloc[split_idx:].copy()\n",
    "\n",
    "    # ---- Compute scalers on train ----\n",
    "    # Calculate the mean of each feature column from the training set\n",
    "    means = df_train[feature_cols].mean()\n",
    "    # Calculate the standard deviation of each feature column from the training set (ddof=0 for population std)\n",
    "    stds  = df_train[feature_cols].std(ddof=0)\n",
    "\n",
    "    # Save scalers (means and standard deviations) to a JSON file\n",
    "    # The JSON file will store these values for later use (e.g., for applying to new data)\n",
    "    SCALER_JSON.write_text(json.dumps({\"mean\": means.to_dict(), \"std\": stds.to_dict()}, indent=2))\n",
    "\n",
    "    # ---- Apply z-score normalization ----\n",
    "    # Apply Z-score normalization to feature columns in both training and walk-forward datasets\n",
    "    # (value - mean) / standard_deviation\n",
    "    for col in feature_cols:\n",
    "        df_train[col] = (df_train[col] - means[col]) / stds[col]\n",
    "        df_walk[col]  = (df_walk[col]  - means[col]) / stds[col]\n",
    "\n",
    "    # ---- Save Parquets ----\n",
    "    # Ensure the parent directory for output Parquet files exists\n",
    "    TRAIN_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Save the normalized training DataFrame to a Parquet file\n",
    "    df_train.to_parquet(TRAIN_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    # Save the normalized walk-forward DataFrame to a Parquet file\n",
    "    df_walk.to_parquet(WALK_PARQUET,  index=False, engine=\"pyarrow\")\n",
    "\n",
    "    # ---- Report ----\n",
    "    print(\"‚úÖ Data prepared:\")\n",
    "    print(f\"  Initial Training : {len(df_train)} windows ‚Üí {TRAIN_PARQUET}\")\n",
    "    print(f\"  Walk-forward     : {len(df_walk)} windows ‚Üí {WALK_PARQUET}\")\n",
    "    print(\"  Scalers saved to:\", SCALER_JSON)\n",
    "\n",
    "    # ---- Extra stats ----\n",
    "    print(\"\\n### Statistics of normalized features (training set)\\n\")\n",
    "    # Print descriptive statistics for the normalized features in the training set\n",
    "    print(df_train[feature_cols].describe().T[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]].round(3))\n",
    "\n",
    "    print(\"\\n### Summary of target slope_kg_per_week\\n\")\n",
    "    # Print summary statistics for the target variable in both training and walk-forward sets\n",
    "    for name, dset in {\"Training\": df_train, \"Walk\": df_walk}.items():\n",
    "        s = dset[TARGET_COL]\n",
    "        print(f\"{name:<7} ‚Üí mean: {s.mean():.3f}  std: {s.std():.3f}  min: {s.min():.3f}  max: {s.max():.3f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "cd466694ca2dd446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5bf92b5bf449675b"
   },
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "id": "5bf92b5bf449675b"
  },
  {
   "metadata": {
    "id": "a1a142cdee705329"
   },
   "cell_type": "markdown",
   "source": [
    "## 08. Defini√ß√£o do Modelo BSTS: Estrutura e Priors Hier√°rquicos\n",
    "---\n",
    "This cell defines the structure of a Bayesian Structural Time Series (BSTS) model using TensorFlow Probability. It loads the normalized training data, specifies a local linear trend component and an exogenous regression component with the pre-selected features. Additionally, it maps feature indices to predefined groups, which will be used for setting up hierarchical priors in a subsequent step.\n",
    "\n",
    "| Name               | Description                                    |\n",
    "| :----------------- | :--------------------------------------------- |\n",
    "| `GROUPS_JSON`      | JSON file mapping feature group names to their respective column indices. |"
   ],
   "id": "a1a142cdee705329"
  },
  {
   "metadata": {
    "id": "50c80cc9b5442d2c",
    "outputId": "995b6999-9c76-4598-8fed-62b27aa9090a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm  # Progress bar library\n",
    "\n",
    "# --- Config paths ---\n",
    "TRAIN_PARQUET = Path(\"train_initial.parquet\") # Path to the initial training data Parquet file\n",
    "TARGET_COL = \"slope_kg_per_week\" # Name of the target variable column\n",
    "DATE_COL   = \"window_start\"   # Name of the date column\n",
    "\n",
    "# --- Load data ---\n",
    "# Load the training data from the Parquet file and sort it by date\n",
    "df_train = pd.read_parquet(TRAIN_PARQUET).sort_values(DATE_COL)\n",
    "# Identify feature columns by excluding the target and date columns\n",
    "feature_cols = [c for c in df_train.columns if c not in {TARGET_COL, DATE_COL}]\n",
    "\n",
    "# Convert the observed time series (target variable) to a TensorFlow tensor\n",
    "observed_time_series = tf.convert_to_tensor(df_train[TARGET_COL].values, dtype=tf.float32)\n",
    "# Convert the exogenous regressors (feature matrix) to a TensorFlow tensor\n",
    "exog_regressors = tf.convert_to_tensor(df_train[feature_cols].values, dtype=tf.float32)\n",
    "\n",
    "# --- Component 1: Local Linear Trend ---\n",
    "# Define a Local Linear Trend component for the BSTS model.\n",
    "# This component models the underlying trend in the time series.\n",
    "trend = tfp.sts.LocalLinearTrend(observed_time_series=observed_time_series)\n",
    "\n",
    "# --- Component 2: Exogenous Regression ---\n",
    "# Define a Linear Regression component for the BSTS model.\n",
    "# This component models the effect of external (exogenous) variables on the time series.\n",
    "regression = tfp.sts.LinearRegression(design_matrix=exog_regressors, name=\"ExogenousRegs\")\n",
    "\n",
    "# --- Build model ---\n",
    "# Combine the trend and regression components into a single Sum model.\n",
    "# The Sum model represents the observed time series as a sum of its components.\n",
    "model = tfp.sts.Sum(components=[trend, regression], observed_time_series=observed_time_series)\n",
    "\n",
    "print(\"BSTS model defined with:\")\n",
    "print(\" ‚Ä¢ Local trend component\")\n",
    "print(f\" ‚Ä¢ Exogenous regression with {len(feature_cols)} features\")\n",
    "print(\"Components ready to receive hierarchical priors in Step 2.\")\n",
    "\n",
    "# --- Save feature groups for priors ---\n",
    "# Define a dictionary mapping conceptual groups to their respective feature names.\n",
    "feature_groups = {\n",
    "    \"comp_corporal\": [\"Fat_pct_mean\", \"Lean_to_Fat_ratio\", \"Circumference_cm_med\"],\n",
    "    \"gasto_energetico\": [\"Total_kcal_mean\", \"Activity_kcal_fraction\"],\n",
    "    \"atividade\": [\"Steps_mean\", \"Standing_variability\"],\n",
    "    \"cardio\": [\"RestingHR_mean\", \"HRV_mean\"],\n",
    "    \"sono\": [\"SleepDur_mean\", \"DeepFrac_mean\"],\n",
    "    \"nutrientes\": [\"Carb_per_kg_mean\", \"Protein_per_kg_mean\"],\n",
    "    \"ingestao_hidratacao\": [\"Cal_deficit_mean\", \"Water_ml_mean\"],\n",
    "}\n",
    "\n",
    "print(\"Mapping feature indices by group‚Ä¶\")\n",
    "# Initialize a dictionary to store the indices of features for each group.\n",
    "group_indices = {}\n",
    "# Iterate through each feature group with a progress bar.\n",
    "for g, cols in tqdm(list(feature_groups.items()), desc=\"Groups\"):\n",
    "    # For each group, find the index of each feature column within the 'feature_cols' list.\n",
    "    # This mapping is crucial for applying hierarchical priors to specific groups of features.\n",
    "    group_indices[g] = [feature_cols.index(col) for col in cols if col in feature_cols]\n",
    "\n",
    "# Define the path for saving the feature groups JSON file.\n",
    "GROUPS_JSON = Path(\"feature_groups.json\")\n",
    "# Convert the 'group_indices' dictionary to a Pandas Series and then save it as a JSON file.\n",
    "# The 'indent=2' makes the JSON output human-readable.\n",
    "GROUPS_JSON.write_text(pd.Series(group_indices).to_json(indent=2))\n",
    "print(\"Group map saved to\", GROUPS_JSON)"
   ],
   "id": "50c80cc9b5442d2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d72c9710d2c6ca97"
   },
   "cell_type": "markdown",
   "source": [
    "## 09. Defini√ß√£o do Modelo BSTS: Estrutura com Priors Hier√°rquicos Ajustados\n",
    "---\n",
    "This cell defines the Bayesian Structural Time Series (BSTS) model structure with carefully adjusted hierarchical priors, which aim to reduce parametric uncertainty. It loads the normalized training data, sets up a local linear trend component, and an exogenous regression component. For the regression weights, hierarchical priors are applied, with specific, reduced scale values for each feature group. Additionally, an explicit `HalfNormal(scale=0.5)` prior is introduced for the observation noise scale to recalibrate `œÉ_obs`.\n",
    "\n",
    "| Name              | Description                                                                                             |\n",
    "| :---------------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `GROUP_SIGMAS`    | Dictionary mapping feature group names to adjusted hyper-prior scale values for hierarchical priors.    |\n",
    "| `group_indices`   | Dictionary loaded from `feature_groups.json`, mapping group names to their corresponding feature indices. |\n",
    "| `model`           | The constructed `tfp.sts.Sum` model with defined components and adjusted priors.                       |"
   ],
   "id": "d72c9710d2c6ca97"
  },
  {
   "metadata": {
    "id": "9805b13e6d55bf68",
    "outputId": "29eb7d1b-8e0b-44cc-aa1d-249e18659739",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths & constants ---\n",
    "TRAIN_PARQUET = Path(\"train_initial.parquet\") # Path to the initial training data Parquet file\n",
    "GROUPS_JSON   = Path(\"feature_groups.json\")   # Path to the JSON file containing feature group indices\n",
    "TARGET_COL    = \"slope_kg_per_week\" # Name of the target variable column\n",
    "DATE_COL      = \"window_start\"      # Name of the date column\n",
    "\n",
    "# --- Adjusted hyper-priors œÑ (smaller values) ---\n",
    "# Dictionary defining the scale (sigma) for the HalfNormal hyper-priors for each feature group.\n",
    "# These smaller values reduce parametric uncertainty.\n",
    "GROUP_SIGMAS = {\n",
    "    \"comp_corporal\":        0.07,\n",
    "    \"gasto_energetico\":     0.07,\n",
    "    \"atividade\":            0.03,\n",
    "    \"cardio\":               0.03,\n",
    "    \"sono\":                 0.03,\n",
    "    \"nutrientes\":           0.05,\n",
    "    \"ingestao_hidratacao\":  0.05,\n",
    "}\n",
    "\n",
    "# --- Load data ---\n",
    "print(\"üì•  Loading training data‚Ä¶\")\n",
    "# Load the training DataFrame and sort it by date\n",
    "df_train = pd.read_parquet(TRAIN_PARQUET).sort_values(DATE_COL)\n",
    "# Identify feature columns by excluding target and date columns\n",
    "feature_cols = [c for c in df_train.columns if c not in {TARGET_COL, DATE_COL}]\n",
    "# Convert feature matrix (X) to a TensorFlow tensor\n",
    "X = tf.convert_to_tensor(df_train[feature_cols].values, dtype=tf.float32)\n",
    "# Convert observed time series (Y, target variable) to a TensorFlow tensor\n",
    "Y = tf.convert_to_tensor(df_train[TARGET_COL].values, dtype=tf.float32)\n",
    "\n",
    "# --- STS Components ---\n",
    "# Define the Local Linear Trend component for the BSTS model\n",
    "trend = tfp.sts.LocalLinearTrend(observed_time_series=Y)\n",
    "\n",
    "# --- Hierarchical priors for regression ---\n",
    "# Load the feature group indices from the JSON file\n",
    "with GROUPS_JSON.open() as f:\n",
    "    group_indices = json.load(f)\n",
    "\n",
    "prior_scales = [] # List to store the prior scale variables for each feature weight\n",
    "for idx in range(len(feature_cols)):\n",
    "    # Find the group to which the current feature index belongs\n",
    "    grp = None\n",
    "    for g, lst in group_indices.items():\n",
    "        if idx in lst:\n",
    "            grp = g\n",
    "            break\n",
    "    if grp is None:\n",
    "        # Handle case where a feature index is not found in any group (safety check)\n",
    "        print(f\"Warning: Feature index {idx} not found in any group. Using default sigma.\")\n",
    "        sigma = 0.05 # Default sigma if group not found\n",
    "        tau_name = f\"tau_feature_{idx}\" # Unique name for ungrouped feature's prior\n",
    "    else:\n",
    "        # Get the predefined sigma for the found group, or use a default if group not in GROUP_SIGMAS\n",
    "        sigma = GROUP_SIGMAS.get(grp, 0.05)\n",
    "        tau_name = f\"tau_{grp}\" # Name for the prior scale variable based on the group name\n",
    "\n",
    "    # Create a TransformedVariable for the prior scale (tau) for the current feature/group.\n",
    "    # Softplus bijector ensures the scale is positive.\n",
    "    tau = tfp.util.TransformedVariable(\n",
    "        initial_value=sigma,\n",
    "        bijector=tfp.bijectors.Softplus(), # Ensures the scale parameter is positive\n",
    "        name=tau_name # Name for the variable, useful for debugging\n",
    "    )\n",
    "    prior_scales.append(tau) # Add the created prior scale variable to the list\n",
    "# Stack the list of individual prior scale variables into a single tensor\n",
    "prior_scale_values = tf.stack(prior_scales)\n",
    "\n",
    "# Define the Linear Regression component with hierarchical priors for its weights.\n",
    "# The `weights_prior` is a Normal distribution whose scale is determined by the `prior_scale_values` tensor,\n",
    "# allowing different prior strengths for different feature groups.\n",
    "regression = tfp.sts.LinearRegression(\n",
    "    design_matrix=X,\n",
    "    weights_prior=tfp.distributions.Normal(loc=0., scale=prior_scale_values),\n",
    "    name=\"HierLinearRegs\"\n",
    ")\n",
    "\n",
    "# --- Prior for observation noise (œÉ_obs) ‚Äì HalfNormal scale 0.5 ---\n",
    "# Define a HalfNormal prior distribution for the observation noise scale.\n",
    "# This explicitly sets an expectation for the magnitude of observational noise.\n",
    "obs_noise_prior = tfp.distributions.HalfNormal(scale=0.5)\n",
    "\n",
    "# --- Summary model with adjusted hierarchical priors and recalibrated œÉ_obs ---\n",
    "# Build the final BSTS model as a sum of the trend and regression components.\n",
    "# The `observation_noise_scale_prior` is explicitly set to recalibrate sigma_obs.\n",
    "model = tfp.sts.Sum(\n",
    "    components=[trend, regression],\n",
    "    observed_time_series=Y,\n",
    "    observation_noise_scale_prior=obs_noise_prior # Explicit prior for observation noise scale\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model with adjusted hierarchical priors and recalibrated œÉ_obs constructed.\")"
   ],
   "id": "9805b13e6d55bf68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "45d8b66546532e5c"
   },
   "cell_type": "markdown",
   "source": [
    "## 10. Variational Inference Setup (Step 3-a)\n",
    "---\n",
    "This cell sets up the necessary components for performing Variational Inference (VI) on the Bayesian Structural Time Series (BSTS) model. It ensures environment variables are configured, loads the training data, reconstructs the BSTS model with the defined hierarchical priors, and initializes the surrogate posterior distribution and the Adam optimizer. This setup prepares the environment and model for the optimization process in the next step of Variational Inference.\n",
    "\n",
    "| Name                  | Description                                                                 |\n",
    "| :-------------------- | :-------------------------------------------------------------------------- |\n",
    "| `GROUP_SIGMAS`        | Dictionary mapping feature group names to the adjusted hyper-prior scale values for hierarchical priors. |\n",
    "| `model`               | The reconstructed `tfp.sts.Sum` model with hierarchical priors.             |\n",
    "| `surrogate_posterior` | The `tfp.sts.VariationalPosterior` object used to approximate the true posterior. |\n",
    "| `optimizer`           | The `tf.keras.optimizers.Adam` instance configured for optimization.        |"
   ],
   "id": "45d8b66546532e5c"
  },
  {
   "metadata": {
    "id": "3cddb0d16f65cf13",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "83288edb-32d2-4355-fc37-188fd8399a4b"
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# 07_vi_step3a_setup.py ‚Äì Variational Inference (Setup) ‚Äì FIXED\n",
    "# ----------------------------------------------------------------\n",
    "# ‚Ä¢ Sets environment variables *before* the first TF import to\n",
    "# ‚Ä¢ Maintains original logic for random seed, model reconstruction\n",
    "#   with hierarchical priors, and surrogate posterior creation.\n",
    "# ================================================================\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 0. Environment variables ‚Äì must come BEFORE any TF import\n",
    "# -----------------------------------------------------------------\n",
    "import os, random, json, logging\n",
    "import random, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. Imports after environment variables\n",
    "# -----------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import absl.logging as absl_logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress Python/absl loggers\n",
    "tf.get_logger().setLevel(\"ERROR\") # Set TensorFlow logger level to ERROR\n",
    "absl_logging.set_verbosity(absl_logging.ERROR) # Set absl logging verbosity to ERROR\n",
    "absl_logging.set_stderrthreshold(\"error\") # Set absl stderr threshold to ERROR\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) # Set standard logging level for TensorFlow to ERROR\n",
    "\n",
    "# ----- Force CPU-only ----- # Removed/Commented to allow GPU\n",
    "# try:\n",
    "#     tf.config.set_visible_devices([], \"GPU\") # Attempt to set visible devices to only CPU (empty list for GPU)\n",
    "# except Exception:\n",
    "#     pass # Catch any exceptions if setting devices fails\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. Constants and hyperparameters\n",
    "# -----------------------------------------------------------------\n",
    "TRAIN_PARQUET = Path(\"train_initial.parquet\") # Path to the initial training data Parquet file\n",
    "GROUPS_JSON   = Path(\"feature_groups.json\")   # Path to the JSON file containing feature group indices\n",
    "TARGET_COL    = \"slope_kg_per_week\" # Name of the target variable column\n",
    "DATE_COL      = \"window_start\"      # Name of the date column\n",
    "\n",
    "NUM_STEPS     = 2000 # Number of optimization steps for variational inference\n",
    "LEARNING_RATE = 1e-2 # Learning rate for the Adam optimizer\n",
    "\n",
    "# Fixed seed for reproducibility\n",
    "TF_SEED = 42 # Define a fixed seed value\n",
    "random.seed(TF_SEED) # Set Python's random seed\n",
    "np.random.seed(TF_SEED) # Set NumPy's random seed\n",
    "print(\"Seed used in this execution:\", TF_SEED) # Print the seed being used\n",
    "tf.random.set_seed(TF_SEED) # Set TensorFlow's global random seed\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. Load training data\n",
    "# -----------------------------------------------------------------\n",
    "# Load the training data from the Parquet file and sort it by date\n",
    "df_train = pd.read_parquet(TRAIN_PARQUET).sort_values(DATE_COL)\n",
    "# Identify feature columns by excluding target and date columns\n",
    "feature_cols = [c for c in df_train.columns if c not in {TARGET_COL, DATE_COL}]\n",
    "# Convert observed time series (Y, target variable) to a TensorFlow tensor\n",
    "Y = tf.convert_to_tensor(df_train[TARGET_COL].values, dtype=tf.float32)\n",
    "# Convert feature matrix (X, exogenous regressors) to a TensorFlow tensor\n",
    "X = tf.convert_to_tensor(df_train[feature_cols].values, dtype=tf.float32)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. Reconstruct model with hierarchical priors (Step 2 logic)\n",
    "# -----------------------------------------------------------------\n",
    "# Load the feature group indices from the JSON file\n",
    "with GROUPS_JSON.open() as f:\n",
    "    group_indices = json.load(f)\n",
    "\n",
    "# Define adjusted hyper-priors (smaller values) for feature groups\n",
    "GROUP_SIGMAS = {\n",
    "    \"comp_corporal\": 0.10,\n",
    "    \"gasto_energetico\": 0.10,\n",
    "    \"atividade\": 0.05,\n",
    "    \"cardio\": 0.05,\n",
    "    \"sono\": 0.05,\n",
    "    \"nutrientes\": 0.07,\n",
    "    \"ingestao_hidratacao\": 0.07,\n",
    "}\n",
    "\n",
    "# Define the Local Linear Trend component for the BSTS model\n",
    "trend = tfp.sts.LocalLinearTrend(observed_time_series=Y)\n",
    "prior_scales = [] # List to store prior scale variables for each feature weight\n",
    "for idx in range(len(feature_cols)):\n",
    "    # Check if the current feature index belongs to any defined group\n",
    "    grp = None\n",
    "    for g, lst in group_indices.items():\n",
    "        if idx in lst:\n",
    "            grp = g\n",
    "            break\n",
    "    if grp is None:\n",
    "        # Warning if a feature index is not found in any group; use a default sigma\n",
    "        print(f\"Warning: Feature index {idx} not found in any group. Using default sigma.\")\n",
    "        sigma = 0.05\n",
    "        tau_name = f\"tau_feature_{idx}\" # Unique name for ungrouped feature's prior\n",
    "    else:\n",
    "        # Get the sigma value for the identified group, or a default if group is not in GROUP_SIGMAS\n",
    "        sigma = GROUP_SIGMAS.get(grp, 0.05)\n",
    "        tau_name = f\"tau_{grp}\" # Name for the prior scale variable based on the group name\n",
    "\n",
    "    # Create a TransformedVariable for the prior scale (tau) for the current feature/group.\n",
    "    # Softplus bijector ensures the scale is positive.\n",
    "    tau = tfp.util.TransformedVariable(sigma, tfp.bijectors.Softplus(), name=tau_name)\n",
    "    prior_scales.append(tau) # Add the created prior scale variable to the list\n",
    "# Stack the list of individual prior scale variables into a single tensor\n",
    "prior_scale_values = tf.stack(prior_scales)\n",
    "\n",
    "# Define the Linear Regression component with hierarchical priors for its weights\n",
    "regression = tfp.sts.LinearRegression(\n",
    "    design_matrix=X,\n",
    "    weights_prior=tfp.distributions.Normal(loc=0., scale=prior_scale_values), # Hierarchical prior on weights\n",
    "    name=\"HierLinearRegs\")\n",
    "\n",
    "# Build the final BSTS model as a sum of the trend and regression components\n",
    "model = tfp.sts.Sum([trend, regression], observed_time_series=Y)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. Surrogate posterior & optimizer\n",
    "# -----------------------------------------------------------------\n",
    "# Build the factored surrogate posterior distribution for the model parameters\n",
    "surrogate_posterior = tfp.sts.build_factored_surrogate_posterior(model)\n",
    "# Initialize the Adam optimizer with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "print(\"‚úÖ VI setup ready ‚Äì surrogate posterior and optimizer defined.\")\n",
    "print(\"Num steps:\", NUM_STEPS)\n",
    "\n",
    "# Expose key variables to the global scope for use in subsequent cells (e.g., for optimization and analysis)\n",
    "globals().update({\n",
    "    \"model\": model, # The constructed BSTS model\n",
    "    \"surrogate_posterior\": surrogate_posterior, # The surrogate posterior distribution\n",
    "    \"optimizer\": optimizer, # The optimizer\n",
    "    \"Y\": Y, # The observed time series (target variable)\n",
    "    \"NUM_STEPS\": NUM_STEPS, # Number of optimization steps\n",
    "    \"feature_cols\": feature_cols # List of feature column names\n",
    "})"
   ],
   "id": "3cddb0d16f65cf13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "780606d33d1ca605"
   },
   "cell_type": "markdown",
   "source": [
    "## 11. Bayesian Structural Time Series (BSTS) Model Variational Inference Training\n",
    "---\n",
    "This cell executes the Variational Inference (VI) training process for the BSTS model. It leverages TensorFlow's automatic GPU detection, suppresses verbose logging, and optimizes the surrogate posterior by maximizing the Evidence Lower Bound (ELBO) over a specified number of steps. The training progress is displayed via a progress bar, and the ELBO values are saved to a file for later analysis.\n",
    "\n",
    "| Name      | Description                                                    |\n",
    "| :-------- | :------------------------------------------------------------- |\n",
    "| `ELBO_PATH` | NumPy file containing the history of ELBO values during training. |"
   ],
   "id": "780606d33d1ca605"
  },
  {
   "metadata": {
    "id": "6df026ef55217752",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "d9ce497a-de04-400b-80c2-01b022659594"
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# 08_vi_step3b_train_fixed.py ‚Äì Variational Inference (Training)\n",
    "# ----------------------------------------------------------------\n",
    "# GPU-friendly version that:\n",
    "# ‚Ä¢ Suppresses TensorFlow INFO/WARN logs.\n",
    "# ‚Ä¢ Automatically uses GPU if available.\n",
    "# ‚Ä¢ Uses trace_fn with tf.reduce_mean to convert ELBO tensor ‚Üí scalar.\n",
    "# ‚Ä¢ Saves ELBO vector.\n",
    "# ================================================================\n",
    "\n",
    "# ----- Suppress logs -----\n",
    "import os\n",
    "# Set TensorFlow's C++ logging level to suppress INFO and WARNING messages, showing only fatal errors.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm # Import tqdm for progress bars\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# ----- Check global objects from Step 3-a -----\n",
    "# Define a tuple of required global variables that should have been set in the previous cell (Step 3-a).\n",
    "required = (\"model\", \"surrogate_posterior\", \"optimizer\", \"Y\", \"NUM_STEPS\")\n",
    "# Check for any missing required global variables.\n",
    "missing = [n for n in required if n not in globals()]\n",
    "if missing:\n",
    "    # If any required variables are missing, raise a RuntimeError instructing the user to run Step 3-a first.\n",
    "    raise RuntimeError(f\"Execute Step 3-a first; missing: {missing}\")\n",
    "\n",
    "# Define the path where the ELBO (Evidence Lower Bound) values will be saved.\n",
    "ELBO_PATH = Path(\"vi_elbo.npy\")\n",
    "# Create parent directories for the ELBO path if they don't already exist.\n",
    "ELBO_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üèÉ  Starting VI ({NUM_STEPS} steps)‚Ä¶\")\n",
    "# Initialize a tqdm progress bar for visualizing the training steps.\n",
    "bar = tqdm(total=NUM_STEPS, desc=\"VI\", unit=\"step\")\n",
    "# Initialize an empty list to store the ELBO loss values during training.\n",
    "loss_list: list[float] = []\n",
    "\n",
    "# ----- Safe trace_fn -----\n",
    "# Define a trace function to be called at each optimization step.\n",
    "def trace_fn(tr):\n",
    "    # Convert the ELBO tensor (which might have a sample_size dimension) to a scalar by taking its mean.\n",
    "    # The negative of the loss is taken because tfp.vi.fit_surrogate_posterior minimizes loss,\n",
    "    # but ELBO is maximized.\n",
    "    elbo_scalar = float(tf.reduce_mean(-tr.loss).numpy())\n",
    "    loss_list.append(elbo_scalar) # Append the scalar ELBO value to the loss list\n",
    "    bar.update(1) # Update the progress bar by one step\n",
    "    # Update the progress bar's postfix message with the current ELBO value every 100 steps.\n",
    "    if len(loss_list) % 100 == 0:\n",
    "        bar.set_postfix({\"ELBO\": f\"{elbo_scalar:.3f}\"})\n",
    "    return tr.loss  # Required by the tfp.vi.fit_surrogate_posterior API\n",
    "\n",
    "# ----- Fit VI -----\n",
    "# Perform variational inference to fit the surrogate posterior to the target log probability.\n",
    "tfp.vi.fit_surrogate_posterior(\n",
    "    # The target log probability function is derived from the joint distribution of the model\n",
    "    # given the observed time series.\n",
    "    target_log_prob_fn=model.joint_distribution(observed_time_series=Y).log_prob,\n",
    "    surrogate_posterior=surrogate_posterior, # The surrogate posterior distribution to optimize\n",
    "    optimizer=optimizer, # The optimizer used for training\n",
    "    num_steps=NUM_STEPS, # The total number of optimization steps\n",
    "    trace_fn=trace_fn, # The callback function to execute at each step (for progress and logging)\n",
    "    sample_size=1, # Number of samples to draw from the surrogate posterior at each step\n",
    ")\n",
    "\n",
    "bar.close() # Close the progress bar after training is complete\n",
    "np.save(ELBO_PATH, np.array(loss_list)) # Save the list of ELBO values to a NumPy file\n",
    "print(\"‚úÖ VI completed. Final ELBO =\", loss_list[-1]) # Print the final ELBO value\n",
    "print(\"ELBO vector saved to\", ELBO_PATH) # Print the path where the ELBO vector was saved"
   ],
   "id": "6df026ef55217752",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a955cf45367aeefc"
   },
   "cell_type": "markdown",
   "source": [
    "## ELBO and Posterior Diagnostics\n",
    "---\n",
    "This cell visualizes the convergence of the Variational Inference (VI) training by plotting the Evidence Lower Bound (ELBO) over iterations. Following this, it samples from the trained surrogate posterior distribution to obtain draws for model parameters, specifically the regression coefficients and the trend level scale. These sampled posteriors are then saved to Parquet files for further analysis and model interpretation.\n",
    "\n",
    "| Name          | Description                                                    |\n",
    "| :------------ | :------------------------------------------------------------- |\n",
    "| `ELBO_PATH`   | NumPy file containing the history of ELBO values during training. |\n",
    "| `COEF_PATH`   | Parquet file containing samples of the posterior regression coefficients. |\n",
    "| `TREND_PATH`  | Parquet file containing samples of the posterior trend level scale. |\n",
    "| `coef_df`     | DataFrame holding the sampled posterior distributions of the regression coefficients. |\n",
    "| `trend_scale` | NumPy array holding the sampled posterior distribution of the trend level scale. |"
   ],
   "id": "a955cf45367aeefc"
  },
  {
   "metadata": {
    "id": "ece609d52151cf21",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "outputId": "d49f5c2b-09bc-4e15-9094-d56e8e3bc0ce"
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# ----- Check prerequisites -----\n",
    "# Assert that 'surrogate_posterior' is available in the global scope; if not, raise an error.\n",
    "assert \"surrogate_posterior\" in globals(), \"Surrogate posterior missing. Run 3-a and 3-b.\"\n",
    "ELBO_PATH = Path(\"vi_elbo.npy\") # Define the path to the saved ELBO values\n",
    "# Assert that the ELBO file exists; if not, raise an error.\n",
    "assert ELBO_PATH.exists(), \"vi_elbo.npy not found. Run 3-b.\"\n",
    "\n",
    "# Define paths for saving the posterior samples of coefficients and trend\n",
    "COEF_PATH  = Path(\"coef_posterior.parquet\")\n",
    "TREND_PATH = Path(\"trend_posterior.parquet\")\n",
    "\n",
    "# ----- 1. Plot ELBO -----\n",
    "losses = np.load(ELBO_PATH) # Load the ELBO values from the .npy file\n",
    "plt.figure(figsize=(6, 3)) # Create a new figure with specified size\n",
    "# Plot the negative of the ELBO values (since ELBO is maximized, loss is minimized)\n",
    "# Use a specific color from the 'viridis' colormap\n",
    "plt.plot(-losses, color=mpl.colormaps.get_cmap(\"viridis\")(0.6), lw=1.2)\n",
    "plt.xlabel(\"Iteration\") # Set x-axis label\n",
    "plt.ylabel(\"ELBO\") # Set y-axis label\n",
    "plt.title(\"ELBO Convergence\") # Set plot title\n",
    "plt.tight_layout() # Adjust plot layout to prevent elements from overlapping\n",
    "plt.show() # Display the plot\n",
    "\n",
    "# ----- 2. Sample posterior -----\n",
    "print(\"üì¶  Sampling 1,000 draws from the posterior‚Ä¶\")\n",
    "# Sample 1000 draws from the fitted surrogate posterior distribution\n",
    "samples = surrogate_posterior.sample(1000)\n",
    "\n",
    "# --- Coefficients ---\n",
    "# Extract the weights (coefficients) from the sampled posterior.\n",
    "# The shape might be (num_samples, num_chains, num_features) or similar.\n",
    "weights = samples[\"HierLinearRegs/_weights\"].numpy()\n",
    "# Flatten the weights array to (num_samples * num_chains, num_features) if multiple chains/dimensions exist.\n",
    "weights = weights.reshape(-1, weights.shape[-1])\n",
    "\n",
    "# Attempt to retrieve 'feature_cols' from global scope.\n",
    "feature_cols = globals().get(\"feature_cols\")\n",
    "if feature_cols is None:\n",
    "    # Fallback: if 'feature_cols' is not found in globals (e.g., if this cell is run independently),\n",
    "    # load 'train_initial.parquet' to infer feature column names.\n",
    "    df_tmp = pd.read_parquet(Path(\"train_initial.parquet\"))\n",
    "    feature_cols = [c for c in df_tmp.columns if c not in {\"slope_kg_per_week\", \"window_start\"}]\n",
    "\n",
    "# Create a Pandas DataFrame from the sampled weights, using 'feature_cols' as column names.\n",
    "coef_df = pd.DataFrame(weights, columns=feature_cols)\n",
    "# Save the coefficients DataFrame to a Parquet file.\n",
    "coef_df.to_parquet(COEF_PATH, index=False)\n",
    "\n",
    "# --- Trend ---\n",
    "# Extract the trend level scale samples from the posterior and ensure it's a 1D array.\n",
    "trend_scale = samples[\"LocalLinearTrend/_level_scale\"].numpy().reshape(-1)\n",
    "# Create a DataFrame from the trend scale samples and save it to a Parquet file.\n",
    "pd.DataFrame({\"trend_level_scale\": trend_scale}).to_parquet(TREND_PATH, index=False)\n",
    "\n",
    "# ----- 3. Quick summary -----\n",
    "print(\"\\n‚îÄ‚îÄ Coefficients summary (\", len(coef_df), \"samples) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "# Print descriptive statistics for the sampled coefficients (mean, std, min, quartiles, max).\n",
    "print(coef_df.describe().T[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]].round(3))\n",
    "\n",
    "print(f\"\\nCoefficient samples saved to: {COEF_PATH}\")\n",
    "print(f\"Trend posterior saved to: {TREND_PATH}\")"
   ],
   "id": "ece609d52151cf21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8276fae90e5af399"
   },
   "cell_type": "markdown",
   "source": [
    "## Z-Score Calibration for 95% Confidence Interval\n",
    "---\n",
    "This cell performs a crucial calibration step for the prediction confidence intervals. It executes a walk-forward forecasting simulation to collect mean (`Œº_i`) and standard deviation (`œÉ_i`) for each prediction. Then, it searches for an optimal `z`-score value on a grid that yields a 95% confidence interval coverage between 90% and 95%. Finally, it regenerates the predictions and calculates final metrics using this calibrated `z`-score.\n",
    "\n",
    "| Name         | Description                                                            |\n",
    "| :----------- | :--------------------------------------------------------------------- |\n",
    "| `PRED_PARQ`  | Parquet file containing walk-forward predictions with calibrated 95% confidence intervals. |\n",
    "| `METRICS_JS` | JSON file containing the final MAE, calibrated 95% CI coverage, and the calibrated `z`-score. |\n",
    "| `pred_df`    | DataFrame containing the final walk-forward predictions and confidence intervals. |"
   ],
   "id": "8276fae90e5af399"
  },
  {
   "metadata": {
    "id": "d49797756dad0d90",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "230a2660-ab98-47c6-a12f-6d8582d5fc46"
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# ----- Configura√ß√µes -----\n",
    "TRAIN_PARQ = Path('train_initial.parquet') # Path to the initial training data Parquet file\n",
    "WALK_PARQ  = Path('walk_forward.parquet')  # Path to the walk-forward validation data Parquet file\n",
    "PRED_PARQ  = Path('walk_preds_ic95_calibrated.parquet') # Output path for calibrated 95% CI predictions\n",
    "METRICS_JS = Path('walk_metrics_ic95_calibrated.json') # Output path for final calibration metrics\n",
    "\n",
    "TARGET = 'slope_kg_per_week' # Name of the target column\n",
    "DATE   = 'window_start' # Name of the date column\n",
    "NUM_DRAWS = 500 # Number of samples to draw from the posterior for each forecast\n",
    "\n",
    "# ----- Carrega dados -----\n",
    "df_train0 = pd.read_parquet(TRAIN_PARQ) # Loads the initial training DataFrame\n",
    "df_walk   = pd.read_parquet(WALK_PARQ)  # Loads the walk-forward DataFrame\n",
    "# Identifies feature columns by excluding the target and date columns\n",
    "feature_cols = [c for c in df_train0.columns if c not in {TARGET, DATE}]\n",
    "\n",
    "# ----- Template de tend√™ncia e offset -----\n",
    "# Defines a Local Linear Trend component template without observed time series (to be provided at runtime)\n",
    "trend_template = tfp.sts.LocalLinearTrend(observed_time_series=None)\n",
    "# Defines a constant offset tensor, initialized to 0.0\n",
    "constant_offset = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "# ----- Fun√ß√£o de forecast de 1 passo (retorna Œº e œÉ) -----\n",
    "@tf.function # Decorator to compile the function into a TensorFlow graph for better performance\n",
    "def forecast_stats(y_hist: tf.Tensor, x_hist: tf.Tensor, x_future: tf.Tensor):\n",
    "    # Concatenates historical exogenous regressors with the future regressor (for the prediction step)\n",
    "    X_full = tf.concat([x_hist, x_future], axis=0)\n",
    "    # Concatenates the historical observed time series with a NaN value for the future point\n",
    "    y_ext  = tf.concat([y_hist, [tf.constant(np.nan, dtype=tf.float32)]], axis=0)\n",
    "    # Defines a Linear Regression component with the full design matrix\n",
    "    reg = tfp.sts.LinearRegression(design_matrix=X_full, name='HierLinearRegs')\n",
    "    # Constructs a temporary STS model for this specific forecast step\n",
    "    model_step = tfp.sts.Sum(\n",
    "        components=[trend_template, reg], # Uses the trend template and the current regression component\n",
    "        observed_time_series=y_ext, # The extended observed time series\n",
    "        constant_offset=constant_offset\n",
    "    )\n",
    "    # Performs the 1-step forecast using samples from the surrogate posterior\n",
    "    fc = tfp.sts.forecast(\n",
    "        model_step,\n",
    "        observed_time_series=y_hist, # Only historical observations are known\n",
    "        parameter_samples=surrogate_posterior.sample(NUM_DRAWS), # Uses sampled parameters from VI\n",
    "        num_steps_forecast=1, # Forecasts one step ahead\n",
    "        include_observation_noise=True # Includes observation noise in the forecast\n",
    "    )\n",
    "    mu    = tf.reduce_mean(fc.mean()) # Calculates the mean of the forecast means (Œº)\n",
    "    sigma = tf.reduce_mean(fc.stddev()) # Calculates the mean of the forecast standard deviations (œÉ)\n",
    "    return mu, sigma # Returns the aggregated mean and standard deviation\n",
    "\n",
    "# ----- 1) Coleta Œº_i, œÉ_i e real -----\n",
    "mus, sigs, reals = [], [], [] # Lists to store means, stds, and actual values\n",
    "df_cum = df_train0.copy() # Initializes a cumulative DataFrame with the initial training data\n",
    "\n",
    "# Iterates over each row of the walk-forward DataFrame to simulate sequential forecasting\n",
    "for _, row in tqdm(df_walk.iterrows(), total=len(df_walk), desc='Collect stats'):\n",
    "    y_hist = df_cum[TARGET].values # Gets historical target values\n",
    "    x_hist = df_cum[feature_cols].values # Gets historical feature values\n",
    "    # Gets future feature values (for the current row) and reshapes for TensorFlow\n",
    "    x_future = row[feature_cols].values[np.newaxis, :]\n",
    "\n",
    "    # Asserts that 'surrogate_posterior' is available in the global scope\n",
    "    assert \"surrogate_posterior\" in globals(), \"Surrogate posterior not found in scope.\"\n",
    "\n",
    "    # Converts NumPy arrays to TensorFlow tensors\n",
    "    y_hist_tf    = tf.constant(y_hist, dtype=tf.float32)\n",
    "    x_hist_tf    = tf.constant(x_hist, dtype=tf.float32)\n",
    "    x_future_tf  = tf.constant(x_future, dtype=tf.float32)\n",
    "\n",
    "    # Calls the forecast function to get the mean (mu) and standard deviation (sigma) for the current step\n",
    "    mu_tensor, sigma_tensor = forecast_stats(y_hist_tf, x_hist_tf, x_future_tf)\n",
    "    mus.append(float(mu_tensor.numpy())) # Appends the numerical mean\n",
    "    sigs.append(float(sigma_tensor.numpy())) # Appends the numerical standard deviation\n",
    "    reals.append(float(row[TARGET])) # Appends the actual observed target value\n",
    "    # Appends the current row to the cumulative DataFrame for the next iteration\n",
    "    df_cum = pd.concat([df_cum, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Converts lists to NumPy arrays for easier numerical operations\n",
    "mus = np.array(mus)\n",
    "sigs = np.array(sigs)\n",
    "reals = np.array(reals)\n",
    "\n",
    "# ----- 2) Calibra z em grid para coverage entre 90% e 95% -----\n",
    "zs = np.linspace(0.0, 2.5, 251) # Creates a grid of z-score values from 0.0 to 2.5\n",
    "# Calculates coverage for each z-score: proportion of actual values falling within (mu - z*sigma, mu + z*sigma)\n",
    "coverages = [(z, np.mean((reals >= mus - z*sigs) & (reals <= mus + z*sigs))) for z in zs]\n",
    "\n",
    "# Filters z-score candidates that result in coverage within the acceptable range (90% to 95%)\n",
    "valid = [(z, cov) for z, cov in coverages if 0.90 <= cov <= 0.95]\n",
    "\n",
    "if valid:\n",
    "    # If valid candidates exist, chooses the z-score whose coverage is closest to 0.95\n",
    "    z_cal, cov_cal = min(valid, key=lambda x: abs(x[1] - 0.95))\n",
    "else:\n",
    "    # If no z-score falls within the range, chooses the z-score closest to 0.925 (the midpoint of the desired range)\n",
    "    z_cal, cov_cal = min(coverages, key=lambda x: abs(x[1] - 0.925))\n",
    "\n",
    "print(f'üîç z_calibrado = {z_cal:.2f} ‚Üí coverage = {cov_cal:.3f}')\n",
    "\n",
    "# ----- 3) Regera predi√ß√µes com z_calibrado -----\n",
    "rows = [] # List to store prediction results\n",
    "df_cum = df_train0.copy() # Re-initializes the cumulative DataFrame for final predictions\n",
    "\n",
    "# Iterates over each row of the walk-forward DataFrame again to generate final predictions\n",
    "for _, row in tqdm(df_walk.iterrows(), total=len(df_walk), desc='Final preds'):\n",
    "    y_hist = df_cum[TARGET].values # Gets historical target values\n",
    "    x_hist = df_cum[feature_cols].values # Gets historical feature values\n",
    "    x_future = row[feature_cols].values[np.newaxis, :] # Gets future feature values\n",
    "\n",
    "    # Converts NumPy arrays to TensorFlow tensors\n",
    "    y_hist_tf    = tf.constant(y_hist, dtype=tf.float32)\n",
    "    x_hist_tf    = tf.constant(x_hist, dtype=tf.float32)\n",
    "    x_future_tf  = tf.constant(x_future, dtype=tf.float32)\n",
    "\n",
    "    # Calls the forecast function to get the mean (mu) and standard deviation (sigma)\n",
    "    mu_tensor, sigma_tensor = forecast_stats(y_hist_tf, x_hist_tf, x_future_tf)\n",
    "    mu    = float(mu_tensor.numpy())\n",
    "    sigma = float(sigma_tensor.numpy())\n",
    "\n",
    "    # Calculates the lower and upper bounds of the 95% confidence interval using the calibrated z-score\n",
    "    lo = mu - z_cal * sigma\n",
    "    hi = mu + z_cal * sigma\n",
    "\n",
    "    # Appends the results (date, mean prediction, CI bounds, actual value) to the rows list\n",
    "    rows.append({\n",
    "        DATE:         row[DATE],\n",
    "        'pred_mean':  mu,\n",
    "        'pred_lo95':  lo,\n",
    "        'pred_hi95':  hi,\n",
    "        'slope_real': float(row[TARGET])\n",
    "    })\n",
    "    # Appends the current row to the cumulative DataFrame for the next iteration\n",
    "    df_cum = pd.concat([df_cum, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Creates a DataFrame from the collected prediction results\n",
    "pred_df = pd.DataFrame(rows)\n",
    "# Saves the prediction DataFrame to a Parquet file\n",
    "pred_df.to_parquet(PRED_PARQ, index=False)\n",
    "\n",
    "# ----- 4) Salva m√©tricas finais -----\n",
    "# Calculates Mean Absolute Error (MAE)\n",
    "mae_val     = float(np.mean(np.abs(pred_df['pred_mean'] - pred_df['slope_real'])))\n",
    "# Calculates the final coverage of the 95% confidence interval\n",
    "coverage_val= float(np.mean((pred_df['slope_real'] >= pred_df['pred_lo95']) & (pred_df['slope_real'] <= pred_df['pred_hi95'])))\n",
    "# Saves the calculated metrics (MAE, coverage, calibrated z-score) to a JSON file\n",
    "METRICS_JS.write_text(json.dumps({\n",
    "    'MAE': mae_val,\n",
    "    'IC95_coverage': coverage_val,\n",
    "    'z_calibrated': z_cal\n",
    "}, indent=2))\n",
    "\n",
    "print(f\"‚úÖ Walk-forward calibrado. MAE = {mae_val:.3f} / coverage = {coverage_val*100:.1f}%\")"
   ],
   "id": "d49797756dad0d90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "7b1e2d8ec1f05e5d"
   },
   "cell_type": "markdown",
   "source": [
    "# Diagn√≥stico"
   ],
   "id": "7b1e2d8ec1f05e5d"
  },
  {
   "metadata": {
    "id": "3df194ca5bea3a23"
   },
   "cell_type": "markdown",
   "source": [
    "## Predictive Standard Deviations ($\\sigma_i$) Diagnostics\n",
    "---\n",
    "This cell performs a diagnostic analysis of the predictive standard deviations ($\\sigma_i$) generated during the walk-forward forecasting. It presents summary statistics for these deviations and visualizes their distribution through a histogram, offering insights into the uncertainty associated with each window's prediction."
   ],
   "id": "3df194ca5bea3a23"
  },
  {
   "metadata": {
    "id": "fc0bb64a5d0c484b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "outputId": "118f711a-dbdb-4fb4-ffa6-21a905531732"
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'sigs' has already been generated in the previous cell as a numpy array\n",
    "# If not yet in scope, recalculate by calling forecast_stats for each window\n",
    "\n",
    "# 1. Summary statistics\n",
    "# Calculate descriptive statistics for the 'sigs' array, including min, quartiles, and max.\n",
    "desc = pd.Series(sigs).describe(percentiles=[0.25, 0.5, 0.75])\n",
    "print(\"üîç Estat√≠sticas de œÉ·µ¢ por janela:\")\n",
    "# Print a subset of the descriptive statistics: min, 25th percentile, median (50th), 75th percentile, and max.\n",
    "print(desc[[\"min\", \"25%\", \"50%\", \"75%\", \"max\"]])\n",
    "\n",
    "# 2. Histogram of œÉ·µ¢ distribution\n",
    "plt.figure(figsize=(8, 4)) # Create a new figure for the histogram with specified dimensions.\n",
    "plt.hist(sigs, bins=20) # Plot a histogram of the 'sigs' array with 20 bins.\n",
    "plt.title(\"Distribui√ß√£o de œÉ·µ¢ (desvio preditivo por janela)\") # Set the title of the histogram.\n",
    "plt.xlabel(\"œÉ·µ¢\") # Set the x-axis label.\n",
    "plt.ylabel(\"Frequ√™ncia\") # Set the y-axis label.\n",
    "plt.tight_layout() # Adjust plot parameters for a tight layout.\n",
    "plt.show() # Display the histogram."
   ],
   "id": "fc0bb64a5d0c484b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Step-Ahead Residuals Diagnostics\n",
    "---\n",
    "This cell performs diagnostic analysis on the one-step-ahead residuals from the walk-forward predictions. It calculates the residuals (actual minus predicted values), visualizes their time series to check for patterns, and then plots the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) to identify any remaining serial correlation in the errors.\n",
    "\n",
    "| Name      | Description                                                                  |\n",
    "| :-------- | :--------------------------------------------------------------------------- |\n",
    "| `PRED_PARQ` | Path to the Parquet file containing walk-forward predictions and confidence intervals. |\n",
    "| `pred_df` | DataFrame containing walk-forward predictions, actual values, and calculated residuals. |"
   ],
   "metadata": {
    "id": "B6k8b2Ltivcm"
   },
   "id": "B6k8b2Ltivcm"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Load walk-forward generated predictions\n",
    "PRED_PARQ = Path(\"walk_preds_ic95_calibrated.parquet\") # Path to the Parquet file containing walk-forward predictions\n",
    "pred_df = pd.read_parquet(PRED_PARQ) # Load the predictions DataFrame\n",
    "\n",
    "# 2) Calculate residuals\n",
    "pred_df[\"residual\"] = pred_df[\"slope_real\"] - pred_df[\"pred_mean\"] # Calculate residuals (actual - predicted)\n",
    "\n",
    "# 3) Plot residual series\n",
    "plt.figure(figsize=(10, 3)) # Create a new figure for the time series plot of residuals\n",
    "plt.plot(pred_df[\"window_start\"], pred_df[\"residual\"], marker=\"o\", ls=\"-\") # Plot residuals against window start date\n",
    "plt.axhline(0, color=\"k\", lw=0.8) # Add a horizontal line at y=0 for reference\n",
    "plt.title(\"Res√≠duos One-Step-Ahead ao Longo do Tempo\") # Set the plot title\n",
    "plt.xlabel(\"Janela (in√≠cio)\") # Set the x-axis label\n",
    "plt.ylabel(\"Res√≠duo (real ‚Äì previsto)\") # Set the y-axis label\n",
    "plt.tight_layout() # Adjust plot parameters for a tight layout\n",
    "plt.show() # Display the plot\n",
    "\n",
    "# 4) Plot ACF and PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3)) # Create a figure and a grid of 1 row, 2 columns for subplots\n",
    "plot_acf(pred_df[\"residual\"], lags=20, ax=axes[0], title=\"ACF dos Res√≠duos\") # Plot Autocorrelation Function (ACF) of residuals\n",
    "plot_pacf(pred_df[\"residual\"], lags=18, ax=axes[1], title=\"PACF dos Res√≠duos\") # Plot Partial Autocorrelation Function (PACF) of residuals\n",
    "plt.tight_layout() # Adjust plot parameters for a tight layout\n",
    "plt.show() # Display the plots"
   ],
   "metadata": {
    "id": "pobNxnKOiwTr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "outputId": "40b06659-f453-4866-a5f0-4be63373a394"
   },
   "id": "pobNxnKOiwTr",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Histograms and Normality Tests *\n",
    "---\n",
    "This cell, as part of the \"Feature Histograms and Normality Tests\" Canvas, loads the filtered feature data and performs two key diagnostic steps. It first generates and displays histograms for each feature to visually assess their distributions. Following this, it conducts D'Agostino's K^2 normality tests for each feature, providing statistical insights into whether their distributions deviate significantly from a normal distribution.\n",
    "\n",
    "This cell does not create any new DataFrames, files, or Parquet files as outputs."
   ],
   "metadata": {
    "id": "iqZseSChkiNy"
   },
   "id": "iqZseSChkiNy"
  },
  {
   "metadata": {
    "id": "a3d3b6a271b71a0f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "a9389c81-a7ad-4892-e846-c0b71617fa35"
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the filtered window data, which contains the features after collinearity filtering.\n",
    "FILTERED_PARQUET = Path(\"janelas_91d_filtered.parquet\")\n",
    "TARGET_COL = \"slope_kg_per_week\"\n",
    "DATE_COL   = \"window_start\"\n",
    "\n",
    "# --- Load Data ---\n",
    "# Load the filtered window data.\n",
    "try:\n",
    "    df_filtered = pd.read_parquet(FILTERED_PARQUET)\n",
    "except FileNotFoundError:\n",
    "    # Print an error message if the Parquet file is not found.\n",
    "    print(f\"Error: {FILTERED_PARQUET} not found. Please ensure the previous steps (collinearity filtering) have been run.\")\n",
    "    exit() # Exit the script if the file is missing.\n",
    "\n",
    "# Identify feature columns by excluding the target and date columns.\n",
    "feature_cols = [c for c in df_filtered.columns if c not in {TARGET_COL, DATE_COL}]\n",
    "\n",
    "# --- Generate Histograms ---\n",
    "print(\"Generating histograms for all features...\")\n",
    "# Determine the number of rows and columns for the subplot grid based on the number of features.\n",
    "n_features = len(feature_cols)\n",
    "n_cols = 3  # Number of columns in the subplot grid\n",
    "n_rows = (n_features + n_cols - 1) // n_cols # Calculate the number of rows needed to fit all features.\n",
    "\n",
    "# Create a figure and a grid of subplots for the histograms.\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "axes = axes.flatten() # Flatten the 2D array of axes into a 1D array for easier iteration.\n",
    "\n",
    "# Iterate through each feature column to generate and plot its histogram.\n",
    "for i, col in enumerate(feature_cols):\n",
    "    sns.histplot(df_filtered[col], kde=True, ax=axes[i], bins=30) # Plot histogram with Kernel Density Estimate.\n",
    "    axes[i].set_title(f'Distribution of {col}') # Set the title for the current subplot.\n",
    "    axes[i].set_xlabel(col) # Set the x-axis label.\n",
    "    axes[i].set_ylabel('Frequency') # Set the y-axis label.\n",
    "\n",
    "# Hide any unused subplots if the number of features is not a perfect multiple of n_cols.\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout() # Adjust subplot parameters for a tight layout, preventing labels from overlapping.\n",
    "plt.show() # Display the generated histograms.\n",
    "print(\"Histograms generated.\")\n",
    "\n",
    "# --- Perform Normality Tests (D'Agostino's K^2 Test) ---\n",
    "print(\"\\nPerforming D'Agostino's K^2 Normality Test for all features:\")\n",
    "print(\"-\" * 60)\n",
    "alpha = 0.05 # Define the significance level for the normality test.\n",
    "\n",
    "results = {} # Dictionary to store the normality test results for each feature.\n",
    "for col in feature_cols:\n",
    "    data = df_filtered[col].dropna() # Drop NaN values from the feature data before performing the test.\n",
    "    if len(data) < 20: # Check if there are enough data points; D'Agostino's K^2 test typically requires at least 20 samples.\n",
    "        print(f\"Warning: Not enough data points for {col} ({len(data)} samples). Skipping normality test.\")\n",
    "        results[col] = {\"statistic\": np.nan, \"p_value\": np.nan, \"normal\": \"N/A (Insufficient Data)\"}\n",
    "        continue\n",
    "\n",
    "    # Perform D'Agostino's K^2 test for normality.\n",
    "    # Returns: statistic (float), pvalue (float).\n",
    "    k2, p_value = stats.normaltest(data)\n",
    "\n",
    "    # Determine if the data is considered normal based on the p-value and alpha.\n",
    "    is_normal = \"Yes (Fail to reject H0)\" if p_value > alpha else \"No (Reject H0)\"\n",
    "\n",
    "    # Store the test results in the dictionary.\n",
    "    results[col] = {\n",
    "        \"statistic\": k2,\n",
    "        \"p_value\": p_value,\n",
    "        \"normal\": is_normal\n",
    "    }\n",
    "    # Print the results for the current feature.\n",
    "    print(f\"Feature: {col:<30} | Statistic: {k2:.3f} | P-value: {p_value:.3f} | Normal: {is_normal}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"For each feature, the D'Agostino's K^2 test checks if the data comes from a normal distribution.\")\n",
    "print(f\"The null hypothesis (H0) is that the data is normally distributed.\")\n",
    "print(f\"If P-value > {alpha} (our significance level), we fail to reject H0, suggesting the data is likely normal.\")\n",
    "print(f\"If P-value <= {alpha}, we reject H0, suggesting the data is not normally distributed.\")\n"
   ],
   "id": "a3d3b6a271b71a0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approximate PIT Histogram Calculation and Plot (Normal CDF Approximation)\n",
    "---\n",
    "This cell calculates the approximate Probability Integral Transform (PIT) values for each one-step-ahead forecast in the walk-forward validation. It uses the Cumulative Distribution Function (CDF) of a Normal distribution, parameterized by the predicted mean and standard deviation for each forecast. The cell then visualizes the distribution of these approximate PIT values using a histogram, which helps assess the calibration of the predictive distributions."
   ],
   "metadata": {
    "id": "6O6pvK1Ml_1C"
   },
   "id": "6O6pvK1Ml_1C"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0ba3ddb9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "outputId": "b61cc5fd-7f2f-4bc0-f643-ca01d1f11913"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats # Import scipy.stats for Normal CDF\n",
    "\n",
    "# Path configurations (using the calibrated predictions file)\n",
    "PRED_PARQ = Path(\"walk_preds_ic95_calibrated.parquet\") # Path to the calibrated predictions Parquet file\n",
    "\n",
    "# Load predictions and actual values data\n",
    "try:\n",
    "    pred_df = pd.read_parquet(PRED_PARQ) # Loads the predictions DataFrame\n",
    "    if pred_df.empty:\n",
    "        print(f\"Warning: {PRED_PARQ} is empty. Cannot compute PIT histogram.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {PRED_PARQ} not found. Please ensure the walk-forward with calibration step was run successfully.\")\n",
    "    pred_df = None # Set to None to prevent further errors\n",
    "\n",
    "if pred_df is not None and not pred_df.empty:\n",
    "    # Extract predictions (mean, IC95) and actual values\n",
    "    pred_mean = pred_df[\"pred_mean\"].values # Extracts predicted mean values\n",
    "    pred_lo95 = pred_df[\"pred_lo95\"].values # Extracts lower bound of 95% confidence interval\n",
    "    pred_hi95 = pred_df[\"pred_hi95\"].values # Extracts upper bound of 95% confidence interval\n",
    "    real_values = pred_df[\"slope_real\"].values # Extracts actual (real) target values\n",
    "\n",
    "    # Calculate predictive standard deviation from IC95 (assuming Normality)\n",
    "    # For a 95% CI of a Normal, the distance between the upper limit and the mean\n",
    "    # is approximately 1.96 * stddev.\n",
    "    # stddev = (pred_hi95 - pred_mean) / 1.96  or (pred_hi95 - pred_lo95) / (2 * 1.96)\n",
    "    # We will use the second form for greater robustness\n",
    "    # Calculates the predictive standard deviation from the 95% confidence interval, assuming normality.\n",
    "    # For a 95% CI of a Normal distribution, the distance from the mean to the upper bound is approx. 1.96 * stddev.\n",
    "    # Using the full width of the CI (hi - lo) divided by (2 * 1.96) for robustness.\n",
    "    pred_stddev = (pred_hi95 - pred_lo95) / (2 * stats.norm.ppf(0.975)) # Uses ppf(0.975) for the 97.5th percentile of a standard normal distribution\n",
    "\n",
    "    # Calculate approximate PIT using Normal CDF\n",
    "    # PIT = P(X <= real_value | X ~ N(pred_mean, pred_stddev^2))\n",
    "    # Equivalent to stats.norm.cdf(real_value, loc=pred_mean, scale=pred_stddev)\n",
    "    # Calculates the approximate Probability Integral Transform (PIT) value for each observation.\n",
    "    # This is done by evaluating the CDF of a Normal distribution (with the predicted mean and stddev) at the actual observed value.\n",
    "    approx_pits = stats.norm.cdf(real_values, loc=pred_mean, scale=pred_stddev)\n",
    "\n",
    "    # Plot PIT histogram\n",
    "    plt.figure(figsize=(8,4)) # Creates a new figure for the PIT histogram\n",
    "    plt.hist(approx_pits, bins=10, range=(0,1), edgecolor='black') # Plots the histogram of approximate PIT values\n",
    "    plt.title(\"Approximate PIT Histogram (One-Step-Ahead)\") # Sets the title of the plot\n",
    "    plt.xlabel(\"Approximate PIT value (Normal CDF)\") # Sets the x-axis label\n",
    "    plt.ylabel(\"Frequ√™ncia\") # Sets the y-axis label\n",
    "    plt.axhline(len(approx_pits)/10, color='red', linestyle='--', label=\"Uniform reference\") # Adds a horizontal line representing the expected frequency for a uniform distribution\n",
    "    plt.legend() # Displays the legend\n",
    "    plt.tight_layout() # Adjusts plot parameters for a tight layout\n",
    "    plt.show() # Displays the plot\n",
    "\n",
    "    print(\"\\n‚úÖ Approximate PIT histogram generated.\")\n",
    "else:\n",
    "    print(\"\\nSkipping PIT histogram generation due to missing or empty prediction data.\")"
   ],
   "id": "0ba3ddb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reliability Diagram in Fine Bins\n",
    "---\n",
    "This cell generates a Reliability Diagram to assess the calibration of the predictive confidence intervals. It recalculates the actual coverage for various nominal coverage probabilities (in 5% bins) using the calibrated 95% confidence interval results from the walk-forward validation. The diagram plots the actual coverage against the nominal coverage, with an ideal calibration line for comparison.\n",
    "\n",
    "| Name      | Description                                                                  |\n",
    "| :-------- | :--------------------------------------------------------------------------- |\n",
    "| `pred_df` | DataFrame containing walk-forward predictions, actual values, and confidence intervals. |"
   ],
   "metadata": {
    "id": "_ACmD8shp0IW"
   },
   "id": "_ACmD8shp0IW"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fa62e4ed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "outputId": "b3818926-285f-41a4-aa0f-2ac20999f9d6"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Configura√ß√µes de caminho (usando o arquivo de previs√µes calibradas)\n",
    "PRED_PARQ = Path(\"walk_preds_ic95_calibrated.parquet\") # Path to the calibrated predictions Parquet file\n",
    "\n",
    "# Carrega dados de previs√µes e valores reais\n",
    "try:\n",
    "    pred_df = pd.read_parquet(PRED_PARQ) # Loads the predictions DataFrame\n",
    "    if pred_df.empty:\n",
    "        print(f\"Warning: {PRED_PARQ} is empty. Cannot compute Reliability Diagram.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {PRED_PARQ} not found. Please ensure the walk-forward with calibration step was run successfully.\")\n",
    "    pred_df = None # Set to None to prevent further errors\n",
    "\n",
    "if pred_df is not None and not pred_df.empty:\n",
    "    # Extrai previs√µes (m√©dia, IC95) e valores reais\n",
    "    pred_mean = pred_df[\"pred_mean\"].values # Extracts predicted mean values\n",
    "    pred_lo95 = pred_df[\"pred_lo95\"].values # Extracts lower bound of 95% confidence interval\n",
    "    pred_hi95 = pred_df[\"pred_hi95\"].values # Extracts upper bound of 95% confidence interval\n",
    "    real_values = pred_df[\"slope_real\"].values # Extracts actual (real) target values\n",
    "\n",
    "    # Recalcula o desvio-padr√£o preditivo a partir do IC95 calibrado (assumindo Normalidade)\n",
    "    # Usando o z_calibrado obtido anteriormente\n",
    "    # Recalculates the predictive standard deviation from the calibrated 95% CI (assuming Normality).\n",
    "    # This assumes z_calibrated is available in the global scope from the calibration cell.\n",
    "    if 'z_cal' not in globals():\n",
    "         # Fallback: Recalculate z_calibrated if not in globals (less ideal, assumes same data).\n",
    "         # This part is just a fallback; ideally, z_cal should be available.\n",
    "         print(\"Warning: z_calibrated not found in global scope. Recalculating...\")\n",
    "         mus = pred_df[\"pred_mean\"].values\n",
    "         sigs_from_95 = (pred_df[\"pred_hi95\"].values - pred_df[\"pred_lo95\"].values) / (2 * stats.norm.ppf(0.975))\n",
    "         reals = pred_df[\"slope_real\"].values\n",
    "\n",
    "         zs = np.linspace(1.0, 2.5, 151)\n",
    "         coverages = []\n",
    "         for z in zs:\n",
    "             lo = mus - z * sigs_from_95\n",
    "             hi = mus + z * sigs_from_95\n",
    "             cov = np.mean((reals >= lo) & (reals <= hi))\n",
    "             coverages.append(cov)\n",
    "         coverages = np.array(coverages)\n",
    "         target = 0.95\n",
    "         idx = np.argmin(np.abs(coverages - target))\n",
    "         z_cal = zs[idx]\n",
    "         print(f'Recalculated z_calibrated = {z_cal:.2f}')\n",
    "\n",
    "    # Use the calibrated standard deviation for all intervals\n",
    "    # Calculates the calibrated predictive standard deviation using the 95% CI width and the corresponding Z-score for 95% (1.96).\n",
    "    pred_stddev_calibrated = (pred_hi95 - pred_lo95) / (2 * stats.norm.ppf(0.975)) # Using the original 95% z for stddev base\n",
    "\n",
    "    # Define bins of nominal coverage (e.g., 5% to 100% in 5% increments)\n",
    "    nominal_coverages = np.linspace(0.05, 1.0, 20) # Creates an array of nominal coverage probabilities from 5% to 100%\n",
    "    actual_coverages = [] # List to store the actual coverage probabilities for each bin\n",
    "\n",
    "    print(\"üßÆ  Calculando cobertura real por bin‚Ä¶\")\n",
    "    # Calculate actual coverage for each nominal coverage bin\n",
    "    for nominal_cov in tqdm(nominal_coverages, desc=\"Bins\"): # Iterate through each nominal coverage probability with a progress bar\n",
    "        # Calculate the corresponding z-score for this nominal coverage, assuming a Normal distribution.\n",
    "        # For a two-sided interval, the tail probability is (1 - nominal_cov) / 2.\n",
    "        # The quantile to use for ppf is 1 - (1 - nominal_cov) / 2 = (1 + nominal_cov) / 2.\n",
    "        z_score = stats.norm.ppf((1 + nominal_cov) / 2)\n",
    "\n",
    "        # Calculate the interval bounds for this nominal coverage using the calibrated standard deviation.\n",
    "        lower_bound = pred_mean - z_score * pred_stddev_calibrated\n",
    "        upper_bound = pred_mean + z_score * pred_stddev_calibrated\n",
    "\n",
    "        # Check how many real values fall within this calculated interval.\n",
    "        coverage = np.mean((real_values >= lower_bound) & (real_values <= upper_bound))\n",
    "        actual_coverages.append(coverage) # Append the calculated actual coverage\n",
    "\n",
    "    # --- Plot Reliability Diagram ---\n",
    "    plt.figure(figsize=(6, 6)) # Creates a new figure for the Reliability Diagram\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Ideal Calibration') # Plots the ideal calibration line (y=x)\n",
    "    plt.plot(nominal_coverages, actual_coverages, marker='o', linestyle='-', label='Actual Coverage') # Plots the actual coverage against nominal coverage\n",
    "\n",
    "    plt.title(\"Reliability Diagram\") # Sets the title of the plot\n",
    "    plt.xlabel(\"Nominal Coverage Probability\") # Sets the x-axis label\n",
    "    plt.ylabel(\"Actual Coverage Probability\") # Sets the y-axis label\n",
    "    plt.grid(True, alpha=0.3) # Adds a grid to the plot for better readability\n",
    "    plt.legend() # Displays the legend\n",
    "    plt.xlim([0, 1]) # Sets the x-axis limits from 0 to 1\n",
    "    plt.ylim([0, 1]) # Sets the y-axis limits from 0 to 1\n",
    "    plt.tight_layout() # Adjusts plot parameters for a tight layout\n",
    "    plt.show() # Displays the plot\n",
    "\n",
    "    print(\"\\n‚úÖ Reliability Diagram generated.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Reliability Diagram generation due to missing or empty prediction data.\")"
   ],
   "id": "fa62e4ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Coverage vs. History Size Analysis\n",
    "---\n",
    "This cell analyzes the performance of the calibrated 95% confidence intervals by examining their coverage in relation to the amount of historical data used for each forecast. It plots a binary representation of whether the actual value fell within the predicted interval against the increasing size of the historical window, providing insights into how prediction uncertainty might change with more available data.\n",
    "\n",
    "| Name      | Description                                                                  |\n",
    "| :-------- | :--------------------------------------------------------------------------- |\n",
    "| `df_train0` | DataFrame containing the initial training data.                               |\n",
    "| `df_walk` | DataFrame containing the walk-forward data.                                   |\n",
    "| `pred_df` | DataFrame containing walk-forward predictions with calibrated intervals.      |"
   ],
   "metadata": {
    "id": "Qbghxh01qczR"
   },
   "id": "Qbghxh01qczR"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "411b9373",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "outputId": "b935854b-885c-41ce-e896-34382e5b540a"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path configurations\n",
    "TRAIN_PARQ = Path(\"train_initial.parquet\") # Path to the initial training data Parquet file. Needed to know initial history size.\n",
    "WALK_PARQ  = Path(\"walk_forward.parquet\")   # Path to the walk-forward data Parquet file. Needed to know walk forward steps.\n",
    "PRED_PARQ  = Path(\"walk_preds_ic95_calibrated.parquet\") # Path to the predictions with calibrated intervals.\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df_train0 = pd.read_parquet(TRAIN_PARQ) # Loads the initial training DataFrame.\n",
    "    df_walk   = pd.read_parquet(WALK_PARQ)   # Loads the walk-forward DataFrame.\n",
    "    pred_df   = pd.read_parquet(PRED_PARQ)   # Loads the predictions DataFrame.\n",
    "\n",
    "    if df_train0.empty or df_walk.empty or pred_df.empty:\n",
    "         print(\"Warning: One or more input files are empty. Cannot compute Coverage vs History Size.\")\n",
    "         df_train0, df_walk, pred_df = None, None, None # Set to None to prevent further errors.\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}. Please ensure {TRAIN_PARQ}, {WALK_PARQ}, and {PRED_PARQ} exist.\")\n",
    "    df_train0, df_walk, pred_df = None, None, None # Set to None to prevent further errors.\n",
    "\n",
    "\n",
    "if all([df_train0 is not None, df_walk is not None, pred_df is not None]):\n",
    "    # Determine the size of the initial training set\n",
    "    initial_train_size = len(df_train0)\n",
    "\n",
    "    # Calculate history size for each walk-forward step\n",
    "    # The history size for the i-th step of the walk-forward is initial_train_size + i\n",
    "    history_sizes = initial_train_size + np.arange(len(df_walk))\n",
    "\n",
    "    # Extract real values and prediction intervals from the calibrated predictions\n",
    "    real_values = pred_df[\"slope_real\"].values\n",
    "    pred_lo95   = pred_df[\"pred_lo95\"].values\n",
    "    pred_hi95   = pred_df[\"pred_hi95\"].values\n",
    "\n",
    "    # Determine if the real value falls within the predicted interval for each step (coverage)\n",
    "    coverage = ((real_values >= pred_lo95) & (real_values <= pred_hi95)).astype(int) # 1 if covered, 0 if not\n",
    "\n",
    "    # --- Plot Coverage vs History Size ---\n",
    "    plt.figure(figsize=(10, 4)) # Create a new figure for the plot.\n",
    "    # Scatter plot: History size on x-axis, Coverage (0 or 1) on y-axis\n",
    "    plt.scatter(history_sizes, coverage, alpha=0.6, label=\"Coverage (1=Yes, 0=No)\")\n",
    "\n",
    "    # Optional: Add a line for overall coverage or a moving average\n",
    "    overall_coverage = np.mean(coverage) # Calculate the overall coverage.\n",
    "    plt.axhline(overall_coverage, color='red', linestyle='--', label=f'Overall Coverage ({overall_coverage:.3f})') # Add a horizontal line for overall coverage.\n",
    "\n",
    "    plt.title(\"Coverage of Calibrated 95% CI vs. History Size\") # Set the title of the plot.\n",
    "    plt.xlabel(\"Number of historical windows used for forecast\") # Set the x-axis label.\n",
    "    plt.ylabel(\"Coverage\") # Set the y-axis label.\n",
    "    plt.yticks([0, 1], [\"No (0)\", \"Yes (1)\"]) # Make y-axis labels clearer.\n",
    "    plt.grid(True, alpha=0.3) # Add a grid to the plot.\n",
    "    plt.legend() # Display the legend.\n",
    "    plt.tight_layout() # Adjust plot parameters for a tight layout.\n",
    "    plt.show() # Display the plot.\n",
    "\n",
    "    print(\"\\n‚úÖ Coverage vs. History Size plot generated.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Coverage vs. History Size plot generation due to missing or empty data.\")"
   ],
   "id": "411b9373",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prior Sensitivity Analysis\n",
    "---\n",
    "This cell performs a sensitivity analysis to evaluate the impact of different hyper-prior scale configurations on the Bayesian Structural Time Series (BSTS) model's performance. It iteratively re-runs the Variational Inference training and walk-forward calibration process for each defined prior configuration, tabulating and displaying the resulting Mean Absolute Error (MAE) and 95% Confidence Interval (IC95%) coverage.\n",
    "\n",
    "| Name       | Description                                                               |\n",
    "| :--------- | :------------------------------------------------------------------------ |\n",
    "| `results_df` | DataFrame tabulating MAE, IC95% Coverage, and calibrated Z-score for each prior configuration. |"
   ],
   "metadata": {
    "id": "ca3c8thYq_SE"
   },
   "id": "ca3c8thYq_SE"
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm # Progress bar library\n",
    "import json # For loading JSON data\n",
    "import sys # For system-specific parameters and functions (e.g., sys.exit)\n",
    "from IPython.display import display # For displaying DataFrames in notebooks\n",
    "\n",
    "# --- Path Configurations (reused from previous cells) ---\n",
    "TRAIN_PARQ = Path(\"train_initial.parquet\") # Path to the initial training data Parquet file\n",
    "WALK_PARQ  = Path(\"walk_forward.parquet\")  # Path to the walk-forward validation data Parquet file\n",
    "GROUPS_JSON   = Path(\"feature_groups.json\") # Path to the JSON file containing feature group indices. Assuming this was generated in a previous step.\n",
    "TARGET = 'slope_kg_per_week' # Name of the target variable column\n",
    "DATE   = 'window_start' # Name of the date column\n",
    "NUM_DRAWS = 500 # Number of samples to draw from the surrogate posterior for each forecast\n",
    "\n",
    "# --- Load data (once) ---\n",
    "try:\n",
    "    # Load the initial training DataFrame\n",
    "    df_train0 = pd.read_parquet(TRAIN_PARQ)\n",
    "    # Load the walk-forward DataFrame\n",
    "    df_walk   = pd.read_parquet(WALK_PARQ)\n",
    "    # Identify feature columns by excluding the target and date columns from the training DataFrame\n",
    "    feature_cols = [c for c in df_train0.columns if c not in {TARGET, DATE}]\n",
    "    # Convert the observed time series (target variable) to a TensorFlow tensor\n",
    "    Y = tf.convert_to_tensor(df_train0[TARGET].values, dtype=tf.float32)\n",
    "    # Convert the exogenous regressors (feature matrix) to a TensorFlow tensor\n",
    "    X = tf.convert_to_tensor(df_train0[feature_cols].values, dtype=tf.float32)\n",
    "\n",
    "    # Load feature group indices from the JSON file\n",
    "    with GROUPS_JSON.open() as f:\n",
    "        group_indices = json.load(f)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    # Print an error message if any required data file is not found\n",
    "    print(f\"Error loading data files: {e}. Ensure {TRAIN_PARQ}, {WALK_PARQ}, and {GROUPS_JSON} exist.\")\n",
    "    sys.exit(1) # Exit the script if a file is missing\n",
    "\n",
    "# --- Trend and offset template (once) ---\n",
    "# Define a Local Linear Trend component template without observed time series (to be provided at runtime)\n",
    "trend_template = tfp.sts.LocalLinearTrend(observed_time_series=None)\n",
    "# Define a constant offset tensor, initialized to 0.0\n",
    "constant_offset = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "# --- 1-step forecast function (reused) ---\n",
    "@tf.function # Decorator to compile the function into a TensorFlow graph for performance\n",
    "def forecast_stats(y_hist: tf.Tensor, x_hist: tf.Tensor, x_future: tf.Tensor, surrogate_posterior_tfp):\n",
    "    # Concatenate historical exogenous regressors with the future regressor (for the prediction step)\n",
    "    X_full = tf.concat([x_hist, x_future], axis=0)\n",
    "    # Concatenate the historical observed time series with a NaN value for the future point\n",
    "    y_ext = tf.concat([y_hist, [tf.constant(np.nan, dtype=tf.float32)]], axis=0)\n",
    "    # Define a Linear Regression component with the full design matrix\n",
    "    reg = tfp.sts.LinearRegression(design_matrix=X_full, name='HierLinearRegs')\n",
    "    # Constructs a temporary STS model for this specific forecast step\n",
    "    model_step = tfp.sts.Sum(\n",
    "        components=[trend_template, reg],\n",
    "        observed_time_series=y_ext,\n",
    "        constant_offset=constant_offset\n",
    "    )\n",
    "    # Performs the 1-step forecast using samples from the provided surrogate posterior\n",
    "    fc = tfp.sts.forecast(\n",
    "        model_step,\n",
    "        observed_time_series=y_hist,\n",
    "        parameter_samples=surrogate_posterior_tfp.sample(NUM_DRAWS), # Uses sampled parameters from VI\n",
    "        num_steps_forecast=1, # Forecasts one step ahead\n",
    "        include_observation_noise=True # Includes observation noise in the forecast\n",
    "    )\n",
    "    mu = tf.reduce_mean(fc.mean()) # Calculates the mean of the forecast means (Œº)\n",
    "    sigma = tf.reduce_mean(fc.stddev()) # Calculates the mean of the forecast standard deviations (œÉ)\n",
    "    return mu, sigma # Returns the aggregated mean and standard deviation\n",
    "\n",
    "# --- Function to build the model with priors ---\n",
    "def build_model_with_priors(y_train, x_train, feature_cols, group_indices, group_sigmas):\n",
    "    # Define the Local Linear Trend component\n",
    "    trend = tfp.sts.LocalLinearTrend(observed_time_series=y_train)\n",
    "\n",
    "    prior_scales = [] # List to store the prior scale variables for each feature weight\n",
    "    for idx in range(len(feature_cols)):\n",
    "        grp = None\n",
    "        # Find the group to which the current feature index belongs\n",
    "        for g, lst in group_indices.items():\n",
    "            if idx in lst:\n",
    "                grp = g\n",
    "                break\n",
    "        # Get the sigma value for the identified group, or use a default if group not in config\n",
    "        sigma = group_sigmas.get(grp, 0.05)\n",
    "        # Name for the prior scale variable based on the group name or feature index\n",
    "        tau_name = f\"tau_{grp}\" if grp else f\"tau_feature_{idx}\"\n",
    "        # Create a TransformedVariable for the prior scale (tau) for the current feature/group.\n",
    "        # Softplus bijector ensures the scale is positive.\n",
    "        tau = tfp.util.TransformedVariable(sigma, tfp.bijectors.Softplus(), name=tau_name)\n",
    "        prior_scales.append(tau) # Add the created prior scale variable to the list\n",
    "    # Stack the list of individual prior scale variables into a single tensor\n",
    "    prior_scale_values = tf.stack(prior_scales)\n",
    "\n",
    "    # Define the Linear Regression component with hierarchical priors for its weights\n",
    "    regression = tfp.sts.LinearRegression(\n",
    "        design_matrix=x_train,\n",
    "        weights_prior=tfp.distributions.Normal(loc=0., scale=prior_scale_values), # Hierarchical prior on weights\n",
    "        name=\"HierLinearRegs\"\n",
    "    )\n",
    "    # Keep the recalibrated observation noise prior consistent\n",
    "    obs_noise_prior = tfp.distributions.HalfNormal(scale=0.5)\n",
    "\n",
    "    # Build the final BSTS model as a sum of the trend and regression components\n",
    "    model = tfp.sts.Sum(\n",
    "        components=[trend, regression],\n",
    "        observed_time_series=y_train,\n",
    "        observation_noise_scale_prior=obs_noise_prior # Explicit prior for observation noise scale\n",
    "    )\n",
    "    return model # Return the constructed model\n",
    "\n",
    "# --- Function to train VI ---\n",
    "def train_vi(model_tfp, y_train, num_steps=2000, learning_rate=1e-2, tf_seed=None):\n",
    "    # Set TensorFlow's random seed for reproducibility if provided\n",
    "    if tf_seed is not None:\n",
    "         tf.random.set_seed(tf_seed)\n",
    "    # Build the factored surrogate posterior distribution for the model parameters\n",
    "    surrogate_posterior_tfp = tfp.sts.build_factored_surrogate_posterior(model_tfp)\n",
    "    # Initialize the Adam optimizer with the specified learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Simple trace_fn for loss only\n",
    "    @tf.function # Decorator to compile the function into a TensorFlow graph for performance\n",
    "    def train_step():\n",
    "         # Perform one step of variational inference optimization\n",
    "         return tfp.vi.fit_surrogate_posterior(\n",
    "            target_log_prob_fn=model_tfp.joint_distribution(observed_time_series=y_train).log_prob,\n",
    "            surrogate_posterior=surrogate_posterior_tfp,\n",
    "            optimizer=optimizer,\n",
    "            num_steps=1, # Train step by step\n",
    "            sample_size=1, # Number of samples to draw from the surrogate posterior at each step\n",
    "         )\n",
    "\n",
    "    # Run training steps with tqdm progress bar\n",
    "    print(f\"\\nüèãÔ∏è‚Äç  Training VI for {num_steps} steps‚Ä¶\")\n",
    "    for i in tqdm(range(num_steps), desc=\"VI Training\"):\n",
    "        loss = train_step() # Execute one training step\n",
    "\n",
    "    return surrogate_posterior_tfp # Return the trained surrogate posterior\n",
    "\n",
    "# --- Function to run walk-forward and calibrate Z ---\n",
    "def run_walk_forward_and_calibrate(df_train0, df_walk, feature_cols, target_col, date_col, surrogate_posterior_tfp):\n",
    "    mus, sigs, reals = [], [], [] # Lists to store means, stds, and actual values\n",
    "    df_cum = df_train0.copy() # Initialize a cumulative DataFrame with the initial training data\n",
    "\n",
    "    print(\"üèÉ‚Äç‚ôÄÔ∏è  Running walk-forward to collect stats‚Ä¶\")\n",
    "    # Iterate through each row of the walk-forward DataFrame to simulate sequential forecasting\n",
    "    for _, row in tqdm(df_walk.iterrows(), total=len(df_walk), desc='Walk-Forward Stats'):\n",
    "        y_hist = tf.constant(df_cum[target_col].values, dtype=tf.float32) # Get historical target values as a TensorFlow tensor\n",
    "        x_hist = tf.constant(df_cum[feature_cols].values, dtype=tf.float32) # Get historical feature values as a TensorFlow tensor\n",
    "        x_future = tf.constant(row[feature_cols].values[np.newaxis, :], dtype=tf.float32) # Get future feature values as a TensorFlow tensor\n",
    "\n",
    "        # Call the forecast function to get mean (mu) and standard deviation (sigma) for the current step\n",
    "        mu_tensor, sigma_tensor = forecast_stats(y_hist, x_hist, x_future, surrogate_posterior_tfp)\n",
    "        mus.append(float(mu_tensor.numpy())) # Append the numerical mean\n",
    "        sigs.append(float(sigma_tensor.numpy())) # Append the numerical standard deviation\n",
    "        reals.append(float(row[target_col])) # Append the actual observed target value\n",
    "        # Append the current row to the cumulative DataFrame for the next iteration\n",
    "        df_cum = pd.concat([df_cum, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "    # Convert lists to NumPy arrays for easier numerical operations\n",
    "    mus = np.array(mus)\n",
    "    sigs = np.array(sigs)\n",
    "    reals = np.array(reals)\n",
    "\n",
    "    # Calibrate z-score\n",
    "    zs = np.linspace(1.0, 3.0, 201) # Create a grid of z-score values (expanded range)\n",
    "    coverages = [] # List to store coverage for each z-score\n",
    "    for z in zs:\n",
    "        lo = mus - z * sigs # Calculate lower bound of CI\n",
    "        hi = mus + z * sigs # Calculate upper bound of CI\n",
    "        cov = np.mean((reals >= lo) & (reals <= hi)) # Calculate actual coverage\n",
    "        coverages.append(cov) # Store coverage\n",
    "    coverages = np.array(coverages) # Convert coverages to NumPy array\n",
    "    target_cov = 0.95 # Desired target coverage\n",
    "    idx = np.argmin(np.abs(coverages - target_cov)) # Find the index of the z-score closest to target coverage\n",
    "    z_cal = zs[idx] # Get the calibrated z-score\n",
    "    calibrated_coverage = coverages[idx] # Get the actual coverage for the calibrated z-score\n",
    "\n",
    "    # Regather final predictions with calibrated z\n",
    "    final_mus, final_sigs, final_reals = [], [], [] # Re-collect for MAE and final coverage check\n",
    "    df_cum = df_train0.copy() # Re-initialize the cumulative DataFrame for final predictions\n",
    "\n",
    "    print(\"‚ú®  Generating final predictions with calibrated Z‚Ä¶\")\n",
    "    # Iterate through each row of the walk-forward DataFrame again to generate final predictions\n",
    "    for _, row in tqdm(df_walk.iterrows(), total=len(df_walk), desc='Final Predictions'):\n",
    "        y_hist = tf.constant(df_cum[target_col].values, dtype=tf.float32) # Get historical target values as a TensorFlow tensor\n",
    "        x_hist = tf.constant(df_cum[feature_cols].values, dtype=tf.float32) # Get historical feature values as a TensorFlow tensor\n",
    "        x_future = tf.constant(row[feature_cols].values[np.newaxis, :], dtype=tf.float32) # Get future feature values as a TensorFlow tensor\n",
    "\n",
    "        # Call the forecast function to get mean (mu) and standard deviation (sigma)\n",
    "        mu_tensor, sigma_tensor = forecast_stats(y_hist, x_hist, x_future, surrogate_posterior_tfp)\n",
    "        final_mus.append(float(mu_tensor.numpy())) # Append the numerical mean\n",
    "        final_sigs.append(float(sigma_tensor.numpy())) # Append the numerical standard deviation\n",
    "        final_reals.append(float(row[target_col])) # Append the actual observed target value\n",
    "        # Append the current row to the cumulative DataFrame for the next iteration\n",
    "        df_cum = pd.concat([df_cum, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "    # Convert lists to NumPy arrays for final metric calculations\n",
    "    final_mus = np.array(final_mus)\n",
    "    final_sigs = np.array(final_sigs)\n",
    "    final_reals = np.array(final_reals)\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(final_mus - final_reals))\n",
    "    # Calculate final 95% CI bounds using the calibrated z-score\n",
    "    final_lo95 = final_mus - z_cal * final_sigs\n",
    "    final_hi95 = final_mus + z_cal * final_sigs\n",
    "    # Calculate final coverage of the calibrated 95% CI\n",
    "    final_coverage = np.mean((final_reals >= final_lo95) & (final_reals <= final_hi95))\n",
    "\n",
    "    return float(mae), float(final_coverage), float(z_cal) # Return MAE, coverage, and calibrated z-score\n",
    "\n",
    "# --- Define different prior configurations to test ---\n",
    "# These are dictionaries mapping group name to the scale (sigma) for the hierarchical priors.\n",
    "PRIOR_CONFIGS = {\n",
    "    \"Config_A\": { # Smaller priors (more shrinkage towards zero for weights)\n",
    "        \"comp_corporal\":        0.05,\n",
    "        \"gasto_energetico\":     0.05,\n",
    "        \"atividade\":            0.02,\n",
    "        \"cardio\":               0.02,\n",
    "        \"sono\":                 0.02,\n",
    "        \"nutrientes\":           0.03,\n",
    "        \"ingestao_hidratacao\":  0.03,\n",
    "    },\n",
    "     \"Config_B\": { # Default priors (from previous cells, balanced shrinkage)\n",
    "        \"comp_corporal\":        0.10,\n",
    "        \"gasto_energetico\":     0.10,\n",
    "        \"atividade\":            0.05,\n",
    "        \"cardio\":               0.05,\n",
    "        \"sono\":                 0.05,\n",
    "        \"nutrientes\":           0.07,\n",
    "        \"ingestao_hidratacao\":  0.07,\n",
    "    },\n",
    "    \"Config_C\": { # Larger priors (less shrinkage, allowing larger weights)\n",
    "        \"comp_corporal\":        0.20,\n",
    "        \"gasto_energetico\":     0.20,\n",
    "        \"atividade\":            0.10,\n",
    "        \"cardio\":               0.10,\n",
    "        \"sono\":                 0.10,\n",
    "        \"nutrientes\":           0.15,\n",
    "        \"ingestao_hidratacao\":  0.15,\n",
    "    },\n",
    "    # Add more configurations as needed for further sensitivity analysis\n",
    "}\n",
    "\n",
    "# --- Loop through configurations ---\n",
    "results = [] # List to store results for each prior configuration\n",
    "print(\"üî¨  Starting Prior Sensitivity Analysis‚Ä¶\")\n",
    "\n",
    "# Ensure Determinism for VI training for comparison (Optional but Recommended)\n",
    "# Use a fixed seed for each run, or a separate seed for each run\n",
    "VI_TRAINING_SEED = 42 # Fixed seed for reproducible VI training across configurations\n",
    "\n",
    "for config_name, group_sigmas in PRIOR_CONFIGS.items():\n",
    "    print(f\"\\n--- Running config: {config_name} ---\")\n",
    "\n",
    "    # 1. Build model with current priors\n",
    "    model = build_model_with_priors(Y, X, feature_cols, group_indices, group_sigmas)\n",
    "\n",
    "    # 2. Train VI\n",
    "    surrogate_posterior = train_vi(model, Y, tf_seed=VI_TRAINING_SEED)\n",
    "\n",
    "    # 3. Run Walk-Forward and Calibrate Z\n",
    "    mae, coverage, z_cal = run_walk_forward_and_calibrate(\n",
    "        df_train0=df_train0,\n",
    "        df_walk=df_walk,\n",
    "        feature_cols=feature_cols,\n",
    "        target_col=TARGET,\n",
    "        date_col=DATE,\n",
    "        surrogate_posterior_tfp=surrogate_posterior\n",
    "    )\n",
    "\n",
    "    # 4. Store results\n",
    "    results.append({\n",
    "        \"Config\": config_name,\n",
    "        \"MAE\": mae,\n",
    "        \"IC95_Coverage\": coverage,\n",
    "        \"Z_Calibrated\": z_cal,\n",
    "        \"Prior_Scales\": group_sigmas # Store scales for reference\n",
    "    })\n",
    "\n",
    "    print(f\"--- Config {config_name} Results: MAE={mae:.3f}, Coverage={coverage:.3f}, Z={z_cal:.2f} ---\")\n",
    "\n",
    "\n",
    "# --- Tabulate and display results ---\n",
    "results_df = pd.DataFrame(results) # Convert results list to a DataFrame\n",
    "\n",
    "print(\"\\nüìä  Prior Sensitivity Analysis Results:\")\n",
    "# Format the 'Prior_Scales' dictionary into a readable string for display in the DataFrame\n",
    "results_df['Prior_Scales_Str'] = results_df['Prior_Scales'].apply(\n",
    "    lambda d: ', '.join([f'{k}:{v:.2f}' for k, v in d.items()])\n",
    ")\n",
    "# Display the results DataFrame, rounding specific columns for clarity\n",
    "display(results_df[['Config', 'MAE', 'IC95_Coverage', 'Z_Calibrated', 'Prior_Scales_Str']].round({'MAE': 3, 'IC95_Coverage': 3, 'Z_Calibrated': 2}))\n",
    "\n",
    "print(\"\\n‚úÖ Prior Sensitivity Analysis Complete.\")"
   ],
   "metadata": {
    "id": "Lbi4xH9frBy1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "outputId": "e140a18c-f2c0-48d7-d436-0a19b974c9e0"
   },
   "id": "Lbi4xH9frBy1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MAE and Coverage Over Time\n",
    "---\n",
    "This cell visualizes the model's performance metrics (Mean Absolute Error and 95% Confidence Interval Coverage) as they evolve over the walk-forward validation period. It calculates the cumulative MAE and cumulative coverage at each step and plots these metrics against the window start date, providing insights into the stability and consistency of the model's predictions and uncertainty quantification over time.\n",
    "\n",
    "| Name      | Description                                                                  |\n",
    "| :-------- | :--------------------------------------------------------------------------- |\n",
    "| `pred_df` | DataFrame containing walk-forward predictions, actual values, and calculated metrics for temporal analysis. |"
   ],
   "metadata": {
    "id": "roc5O5vmvNQB"
   },
   "id": "roc5O5vmvNQB"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl # Import matplotlib for colormaps\n",
    "from pathlib import Path\n",
    "\n",
    "# Calculate and plot MAE and Coverage over time\n",
    "print(\"üìä  Calculating and plotting MAE and Coverage over time‚Ä¶\")\n",
    "\n",
    "# Make sure pred_df from the walk-forward step is available\n",
    "# If not, you need to re-run the walk-forward steps to get it.\n",
    "# Assuming pred_df exists and contains:\n",
    "# \"window_start\", \"pred_mean\", \"pred_lo95\", \"pred_hi95\", \"slope_real\"\n",
    "\n",
    "if 'pred_df' not in globals():\n",
    "    # Attempt to load from the calibrated predictions file if pred_df is not already in global scope\n",
    "    PRED_PARQ = Path(\"walk_preds_ic95_calibrated.parquet\")\n",
    "    try:\n",
    "        pred_df = pd.read_parquet(PRED_PARQ)\n",
    "        print(f\"Loaded predictions from {PRED_PARQ}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {PRED_PARQ} not found. Cannot plot metrics over time.\")\n",
    "        pred_df = None # Set to None to prevent further execution if file is missing\n",
    "\n",
    "if pred_df is not None and not pred_df.empty:\n",
    "    # Sort by date just in case to ensure correct cumulative calculations\n",
    "    pred_df = pred_df.sort_values(\"window_start\").reset_index(drop=True)\n",
    "\n",
    "    # Calculate absolute error for each step\n",
    "    pred_df['abs_error'] = np.abs(pred_df['pred_mean'] - pred_df['slope_real'])\n",
    "\n",
    "    # Calculate coverage for each step (1 if the real value is within the CI, 0 otherwise)\n",
    "    pred_df['is_covered'] = ((pred_df['slope_real'] >= pred_df['pred_lo95']) &\n",
    "                             (pred_df['slope_real'] <= pred_df['pred_hi95'])).astype(int)\n",
    "\n",
    "    # Calculate cumulative MAE (Mean Absolute Error) using expanding window mean\n",
    "    pred_df['cumulative_mae'] = pred_df['abs_error'].expanding().mean()\n",
    "    # Calculate cumulative Coverage using expanding window mean\n",
    "    pred_df['cumulative_coverage'] = pred_df['is_covered'].expanding().mean()\n",
    "\n",
    "    # Plotting\n",
    "    # Create a figure with two subplots arranged vertically, sharing the x-axis\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "    # Plot Cumulative MAE on the first subplot\n",
    "    axes[0].plot(pred_df['window_start'], pred_df['cumulative_mae'], marker='o', linestyle='-', markersize=3, color=mpl.colormaps.get_cmap(\"viridis\")(0.7))\n",
    "    axes[0].set_ylabel('MAE Acumulado') # Set y-axis label for MAE plot\n",
    "    axes[0].set_title('MAE Acumulado ao Longo do Tempo') # Set title for MAE plot\n",
    "    axes[0].grid(True, alpha=0.3) # Add grid to MAE plot\n",
    "\n",
    "    # Plot Cumulative Coverage on the second subplot\n",
    "    axes[1].plot(pred_df['window_start'], pred_df['cumulative_coverage'], marker='o', linestyle='-', markersize=3, color=mpl.colormaps.get_cmap(\"viridis\")(0.4))\n",
    "    axes[1].set_ylabel('Cobertura Acumulada') # Set y-axis label for Coverage plot\n",
    "    axes[1].set_title('Cobertura Acumulada (IC95%) ao Longo do Tempo') # Set title for Coverage plot\n",
    "    axes[1].set_xlabel('Janela (in√≠cio)') # Set x-axis label for Coverage plot\n",
    "    axes[1].axhline(0.95, color='red', linestyle='--', label='Alvo 95%') # Add a horizontal line at 95% target coverage\n",
    "    axes[1].legend() # Display legend for Coverage plot\n",
    "    axes[1].grid(True, alpha=0.3) # Add grid to Coverage plot\n",
    "    axes[1].set_ylim([0.5, 1.05]) # Focus the Y-axis around typical coverage values (0.5 to 1.05)\n",
    "\n",
    "    plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
    "    plt.show() # Display the plots\n",
    "\n",
    "    print(\"\\n‚úÖ Plots de MAE e Cobertura Acumulados gerados.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping MAE and Coverage over time plots due to missing or empty prediction data.\")"
   ],
   "metadata": {
    "id": "U2SsCaghvOGm",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "outputId": "1cc9f316-d354-4e20-9d71-cc12490ae15b"
   },
   "id": "U2SsCaghvOGm",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Weight Dynamics\n",
    "---\n",
    "This cell aims to visualize the dynamics of feature weights (coefficients) over time. It loads the posterior samples of coefficients obtained from the initial Variational Inference training. While a true \"over time\" analysis would require saving coefficient posteriors at each step of the walk-forward, this cell provides a simplified view by plotting the distribution (median and interquartile range) of the final coefficients learned from the *initial* training set, giving insight into their relative importance after the first training phase.\n",
    "\n",
    "| Name      | Description                                                                  |\n",
    "| :-------- | :--------------------------------------------------------------------------- |\n",
    "| `coef_df` | DataFrame containing samples of the posterior regression coefficients from the initial training. |"
   ],
   "metadata": {
    "id": "_t9493H5xD9y"
   },
   "id": "_t9493H5xD9y"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl # Import matplotlib for colormaps\n",
    "from pathlib import Path\n",
    "\n",
    "# Extracting and plotting feature weights over time.\n",
    "print(\"üìâ  Extracting and plotting feature weights over time‚Ä¶\")\n",
    "\n",
    "# Make sure coef_posterior.parquet from VI diagnostics is available\n",
    "# If not, you need to re-run VI training and sampling steps (3a, 3b, 3c).\n",
    "COEF_PATH = Path(\"coef_posterior.parquet\") # Path to the Parquet file containing posterior coefficients.\n",
    "\n",
    "if not COEF_PATH.exists():\n",
    "    print(f\"Error: {COEF_PATH} not found. Cannot plot feature importance dynamics.\")\n",
    "else:\n",
    "    try:\n",
    "        coef_df = pd.read_parquet(COEF_PATH) # Load the coefficients DataFrame.\n",
    "        print(f\"Loaded coefficients from {COEF_PATH}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading coefficients from {COEF_PATH}: {e}\")\n",
    "        coef_df = None # Set to None to prevent further errors if loading fails.\n",
    "\n",
    "    # Assuming df_walk is available and sorted chronologically\n",
    "    if 'df_walk' not in globals() or df_walk.empty:\n",
    "        print(\"Warning: df_walk not found or is empty. Cannot associate coefficients with time windows.\")\n",
    "        print(\"Skipping plotting feature importance dynamics.\")\n",
    "        df_walk = None\n",
    "    elif coef_df is not None:\n",
    "        # The coefficients represent the weights learned from the *entire* training set\n",
    "        # up to that point in the walk-forward.\n",
    "        # We need to associate these coefficients with the *start date* of the window\n",
    "        # for which they were used to make a prediction.\n",
    "        # The surrogate_posterior.sample(NUM_DRAWS) gave us 1000 samples of the *final*\n",
    "        # weights learned from the *initial train set* (df_train0).\n",
    "        # To see dynamics *over time*, we would ideally need to save the posterior\n",
    "        # samples at *each step* of the walk-forward. This is computationally intensive\n",
    "        # and was not implemented in the `run_walk_forward_and_calibrate` function.\n",
    "\n",
    "        # A simplified approach is to plot the distribution (e.g., median and IQR)\n",
    "        # of the final posterior weights learned from the initial training set.\n",
    "        # This shows the relative importance *after* the first training phase,\n",
    "        # not how importance changed during the walk-forward.\n",
    "\n",
    "        # Plotting distribution of final coefficients from the initial training set.\n",
    "        print(\"\\nüìä  Plotting distribution of final coefficients from initial training set‚Ä¶\")\n",
    "\n",
    "        if coef_df is not None and not coef_df.empty:\n",
    "            # Calculate median and Interquartile Range (IQR) for each coefficient.\n",
    "            coef_summary = coef_df.describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "            coef_summary['IQR'] = coef_summary['75%'] - coef_summary['25%']\n",
    "            coef_summary = coef_summary.sort_values('50%', ascending=False) # Sort coefficients by their median weight in descending order.\n",
    "\n",
    "            plt.figure(figsize=(8, 6)) # Create a new figure for the plot.\n",
    "            y_pos = np.arange(len(coef_summary)) # Create an array for y-axis positions.\n",
    "            medians = coef_summary['50%'].values # Get the median values of the coefficients.\n",
    "            # Calculate the lower and upper errors for the error bars based on IQR.\n",
    "            errors = [medians - coef_summary['25%'].values, coef_summary['75%'].values - medians]\n",
    "\n",
    "            # Plot median and IQR as error bars.\n",
    "            plt.errorbar(medians, y_pos, xerr=errors, fmt='o', capsize=3, label='Median (IQR)', color=mpl.colormaps.get_cmap(\"viridis\")(0.6))\n",
    "            plt.yticks(y_pos, coef_summary.index) # Set y-axis ticks and labels to feature names.\n",
    "            plt.axvline(0, color='grey', linestyle='--', lw=0.8) # Add a vertical line at 0 for reference.\n",
    "            plt.xlabel(\"Peso (Coeficiente)\") # Set the x-axis label.\n",
    "            plt.title(\"Distribui√ß√£o Posterior dos Coeficientes (do Treino Inicial)\") # Set the plot title.\n",
    "            plt.grid(axis='x', alpha=0.3) # Add a horizontal grid.\n",
    "            plt.legend() # Display the legend.\n",
    "            plt.tight_layout() # Adjust plot parameters for a tight layout.\n",
    "            plt.show() # Display the plot.\n",
    "\n",
    "            print(\"\\n‚úÖ Plot da distribui√ß√£o dos coeficientes do treino inicial gerado.\")\n",
    "\n",
    "\n",
    "        # To genuinely plot \"importance over time\", we would need to re-run\n",
    "        # the walk-forward and sample/save the posterior at each step.\n",
    "        # This is a more advanced analysis and would require modifying the walk-forward logic.\n",
    "\n",
    "        print(\"\\nNote: Plotting feature importance dynamics over time requires saving posterior samples at each walk-forward step, which was not implemented in the current script structure.\")\n",
    "        print(\"The plot above shows the distribution of coefficients learned from the *initial* training data only.\")\n"
   ],
   "metadata": {
    "id": "z-JGRQirxG2V",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "outputId": "4556aaf3-34a5-433b-fe38-846ee1d9b0c7"
   },
   "id": "z-JGRQirxG2V",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Posterior Predictive Check (PPC) with HMC\n",
    "---\n",
    "This cell demonstrates a Posterior Predictive Check (PPC) for a Bayesian Structural Time Series (BSTS) model. It begins by generating synthetic time series data, then defines and fits an STS model using Hamiltonian Monte Carlo (HMC) to sample from the posterior distribution of its parameters. Finally, it performs the PPC by generating replicated data from the fitted model and visualizes how well these replicated data compare to the observed data, assessing the model's fit.\n",
    "\n",
    "This cell does not create any new DataFrames, files, or Parquet files as outputs."
   ],
   "metadata": {
    "id": "dngLCcGXxsSm"
   },
   "id": "dngLCcGXxsSm"
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use seaborn for better plot aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# For reproducibility, set random seeds for TensorFlow and NumPy.\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 1. Generate Synthetic Time Series Data ---\n",
    "# Let's create a time series with a linear trend and weekly seasonality.\n",
    "num_timesteps = 150 # Define the number of time steps for the synthetic series.\n",
    "observed_series = (\n",
    "    tf.linspace(0., 10., num_timesteps) +  # Linear trend component: values increasing linearly over time.\n",
    "    np.sin(2 * np.pi * np.arange(num_timesteps) / 7.) * 2.  # Weekly seasonality component: a sine wave repeating every 7 timesteps.\n",
    "    + np.random.normal(scale=0.5, size=num_timesteps)  # Noise component: random values drawn from a normal distribution.\n",
    ")\n",
    "observed_series = tf.cast(observed_series, dtype=tf.float32) # Cast the series to float32 TensorFlow tensor.\n",
    "\n",
    "# Plot the generated data to visualize its components.\n",
    "plt.figure(figsize=(12, 6)) # Create a figure with a specified size.\n",
    "plt.plot(observed_series, label=\"Observed Data\") # Plot the observed time series.\n",
    "plt.title(\"Synthetic Time Series Data\") # Set the title of the plot.\n",
    "plt.xlabel(\"Timestep\") # Set the x-axis label.\n",
    "plt.ylabel(\"Value\") # Set the y-axis label.\n",
    "plt.legend() # Display the legend.\n",
    "plt.show() # Show the plot.\n",
    "\n",
    "\n",
    "# --- 2. Define the Structural Time Series (STS) Model ---\n",
    "# We'll define a model that matches the data-generating process:\n",
    "# a linear trend component and a seasonal component.\n",
    "def build_sts_model(data):\n",
    "    \"\"\"Builds an STS model.\n",
    "\n",
    "    Args:\n",
    "      data: A `tf.Tensor` representing the observed time series.\n",
    "\n",
    "    Returns:\n",
    "      An instance of `tfp.sts.Sum`.\n",
    "    \"\"\"\n",
    "    # Component for the underlying trend: Local Linear Trend allows for a changing slope and level.\n",
    "    trend = tfp.sts.LocalLinearTrend(observed_time_series=data, name='trend')\n",
    "\n",
    "    # Component for weekly seasonality: Models periodic patterns with a cycle length of 7.\n",
    "    seasonal = tfp.sts.Seasonal(\n",
    "        num_seasons=7, observed_time_series=data, name='seasonal')\n",
    "\n",
    "    # The final model is the sum of the components: combines the trend and seasonal effects.\n",
    "    model = tfp.sts.Sum(components=[trend, seasonal],\n",
    "                        observed_time_series=data)\n",
    "    return model # Return the constructed STS model.\n",
    "\n",
    "sts_model = build_sts_model(observed_series) # Build the STS model using the observed synthetic data.\n",
    "\n",
    "\n",
    "# --- 3. Fit the Model using Hamiltonian Monte Carlo (HMC) ---\n",
    "# We use HMC to sample from the posterior distribution of the model's parameters.\n",
    "# This gives us a set of plausible parameter values given the data.\n",
    "\n",
    "print(\"Fitting STS model with HMC...\")\n",
    "# The `fit_with_hmc` function is a convenient wrapper for this process.\n",
    "# num_results should be large enough for stable estimates of the posterior.\n",
    "# num_warmup_steps are steps to discard while the sampler converges (burn-in period).\n",
    "parameter_samples, kernel_results = tfp.sts.fit_with_hmc(\n",
    "    model=sts_model, # The STS model to fit.\n",
    "    observed_time_series=observed_series, # The data to fit the model to.\n",
    "    num_results=200, # Number of post-warmup samples to collect.\n",
    "    num_warmup_steps=50, # Number of warmup steps to run before collecting samples.\n",
    "    seed=42) # Random seed for reproducibility.\n",
    "\n",
    "print(\"HMC fitting complete.\")\n",
    "# Print the shape of one of the collected parameter samples (e.g., the first parameter's samples).\n",
    "print(f\"Shape of parameter samples: {parameter_samples[0].shape}\")\n",
    "\n",
    "\n",
    "# --- 4. Perform the Posterior Predictive Check (PPC) ---\n",
    "# The core idea of PPC is to generate new data from the model using the\n",
    "# parameters from the posterior distribution. If the model is a good fit,\n",
    "# the generated data should look similar to the observed data.\n",
    "\n",
    "# In TFP v0.25.0, you use `tfp.sts.one_step_predictive` to get the\n",
    "# predictive distribution over the observed data points.\n",
    "print(\"Generating posterior predictive samples...\")\n",
    "# `one_step_predictive` computes the one-step-ahead predictive distribution for each time point.\n",
    "predictive_dist = tfp.sts.one_step_predictive(\n",
    "    model=sts_model, # The fitted STS model.\n",
    "    observed_time_series=observed_series, # The observed data.\n",
    "    parameter_samples=parameter_samples) # The posterior samples of parameters obtained from HMC.\n",
    "\n",
    "# Now, we sample from this predictive distribution to get our \"replicated\" data.\n",
    "# We'll generate a number of replicated datasets equal to the number of HMC samples.\n",
    "num_samples = 100 # Number of replicated datasets to generate.\n",
    "posterior_predictive_samples = predictive_dist.sample(num_samples, seed=42) # Sample from the predictive distribution.\n",
    "\n",
    "# The shape will be [num_samples, num_timesteps] where num_samples is the number of replicated series.\n",
    "print(f\"Shape of posterior predictive samples: {posterior_predictive_samples.shape}\")\n",
    "\n",
    "\n",
    "# --- 5. Visualize the Results ---\n",
    "# A good way to perform a PPC is to plot the observed data along with\n",
    "# the credible interval of the posterior predictive samples.\n",
    "\n",
    "# Calculate the mean of the posterior predictive samples across all replicated datasets.\n",
    "predicted_mean = tf.reduce_mean(posterior_predictive_samples, axis=0)\n",
    "# Calculate the 2.5th percentile (lower bound of 95% credible interval) for each timestep.\n",
    "predicted_lower = tfp.stats.percentile(posterior_predictive_samples, 2.5, axis=0)\n",
    "# Calculate the 97.5th percentile (upper bound of 95% credible interval) for each timestep.\n",
    "predicted_upper = tfp.stats.percentile(posterior_predictive_samples, 97.5, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 7)) # Create a figure for the plot.\n",
    "plt.plot(observed_series, 'k', label='Observed Data') # Plot the original observed data in black.\n",
    "plt.plot(predicted_mean, color='C0', linestyle='--', label='Posterior Predictive Mean') # Plot the mean of the replicated data.\n",
    "plt.fill_between(\n",
    "    range(num_timesteps), # X-axis values (timesteps).\n",
    "    predicted_lower, # Lower bound of the credible interval.\n",
    "    predicted_upper, # Upper bound of the credible interval.\n",
    "    color='C0', # Color of the shaded area.\n",
    "    alpha=0.2, # Transparency of the shaded area.\n",
    "    label='95% Predictive Interval' # Label for the legend.\n",
    ")\n",
    "\n",
    "plt.title(\"Posterior Predictive Check\", fontsize=16) # Set the title of the plot.\n",
    "plt.xlabel(\"Timestep\") # Set the x-axis label.\n",
    "plt.ylabel(\"Value\") # Set the y-axis label.\n",
    "plt.legend() # Display the legend.\n",
    "plt.show() # Show the plot.\n",
    "\n",
    "# Another common PPC visualization is to plot a few of the replicated\n",
    "# datasets directly on top of the observed data to see individual trajectories.\n",
    "plt.figure(figsize=(15, 7)) # Create another figure.\n",
    "plt.plot(observed_series, 'k', linewidth=2, label='Observed Data') # Plot the original observed data.\n",
    "\n",
    "# Plot a few sample trajectories from the posterior predictive samples.\n",
    "for i in range(min(10, num_samples)): # Iterate up to 10 or `num_samples` (whichever is smaller).\n",
    "    plt.plot(posterior_predictive_samples[i, :], color='C1', alpha=0.3) # Plot each sample trajectory with some transparency.\n",
    "\n",
    "# Add a single label for the replicated data samples in the legend.\n",
    "plt.plot([], [], color='C1', alpha=0.5, label='Replicated Data Samples')\n",
    "\n",
    "plt.title(\"Posterior Predictive Samples vs. Observed Data\", fontsize=16) # Set the title.\n",
    "plt.xlabel(\"Timestep\") # Set the x-axis label.\n",
    "plt.ylabel(\"Value\") # Set the y-axis label.\n",
    "plt.legend() # Display the legend.\n",
    "plt.show() # Show the plot."
   ],
   "metadata": {
    "id": "eZu7PxAO90Y6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "02583d30-3122-4ca7-9a12-f112fd02f789"
   },
   "id": "eZu7PxAO90Y6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alternative Cross-Validation (Blocking/Rolling Window)\n",
    "---\n",
    "This cell implements an alternative cross-validation strategy, to robustly evaluate the model's performance. It divides the combined dataset into temporal blocks, iteratively trains the Bayesian Structural Time Series (BSTS) model (using Variational Inference) on historical data, and then evaluates its performance (MAE and coverage) on the subsequent test block. The metrics are summarized for each fold, providing insights into the model's generalization capabilities across different time periods.\n",
    "\n",
    "| Name           | Description                                                                  |\n",
    "| :------------- | :--------------------------------------------------------------------------- |\n",
    "| `df_full_data` | Combined DataFrame of training and walk-forward data, sorted chronologically, used for cross-validation splitting. |\n",
    "| `cv_results`   | List of dictionaries, where each dictionary stores the performance metrics and fold details for a specific cross-validation fold. |\n",
    "| `cv_results_df`| DataFrame summarizing the cross-validation results across all folds.         |"
   ],
   "metadata": {
    "id": "1PoPyHQaCfR1"
   },
   "id": "1PoPyHQaCfR1"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats # Used for normal distribution functions\n",
    "import tensorflow as tf # TensorFlow library for numerical computation\n",
    "import tensorflow_probability as tfp # TensorFlow Probability for STS models\n",
    "import json # For loading JSON files\n",
    "import sys # For system-specific parameters and functions (e.g., sys.exit)\n",
    "from tqdm import tqdm # Progress bar library\n",
    "from IPython.display import display # For displaying DataFrames in notebooks\n",
    "\n",
    "print(\"üß™  Starting Blocking Cross-Validation‚Ä¶\")\n",
    "\n",
    "# --- CV Configurations ---\n",
    "# We will use a Blocking CV, where each test fold is a window of N steps\n",
    "# and the training fold is all the history *before* the test window.\n",
    "# This better simulates the real forecasting scenario.\n",
    "# We could also do a Rolling Window CV, where the training set grows\n",
    "# with each fold, similar to walk-forward, but with multiple folds.\n",
    "# To simplify and demonstrate the concept, we will do a simple Blocking CV.\n",
    "\n",
    "# Number of folds/test blocks for cross-validation.\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Calculate total number of windows by combining initial training and walk-forward data.\n",
    "TOTAL_WINDOWS = len(df_train0) + len(df_walk) if 'df_train0' in globals() and 'df_walk' in globals() else 0\n",
    "if TOTAL_WINDOWS == 0:\n",
    "    print(\"Error: Training and walk-forward data not loaded. Cannot perform CV.\")\n",
    "else:\n",
    "    # Calculate the approximate size of each block.\n",
    "    BLOCK_SIZE = TOTAL_WINDOWS // N_SPLITS\n",
    "    if BLOCK_SIZE == 0:\n",
    "        print(f\"Error: TOTAL_WINDOWS ({TOTAL_WINDOWS}) is less than N_SPLITS ({N_SPLITS}). Cannot create folds.\")\n",
    "    else:\n",
    "\n",
    "        # Combine initial training and walk-forward data for CV splitting.\n",
    "        # Ensure the combined DataFrame is sorted by date for chronological splitting.\n",
    "        df_full_data = pd.concat([df_train0, df_walk]).sort_values(DATE).reset_index(drop=True)\n",
    "\n",
    "        # List to store results for each cross-validation fold.\n",
    "        cv_results = []\n",
    "\n",
    "        # --- Reuse modeling, VI training, and evaluation functions ---\n",
    "        # Assumes that the functions `build_model_with_priors`, `train_vi`,\n",
    "        # and `run_walk_forward_and_calibrate` or parts of them (especially forecast_stats)\n",
    "        # are defined and available in the global scope from previous cells.\n",
    "        # Also assumes that `feature_cols`, `TARGET`, `DATE`, `GROUPS_JSON`,\n",
    "        # `build_model_with_priors`, `train_vi`, `forecast_stats` are available.\n",
    "\n",
    "        # Ensure required objects from previous steps are available for CV.\n",
    "        required_cv = ('df_full_data', 'feature_cols', 'TARGET', 'DATE', 'GROUPS_JSON',\n",
    "                       'build_model_with_priors', 'train_vi', 'forecast_stats')\n",
    "        missing_cv = [n for n in required_cv if n not in globals() or (isinstance(globals()[n], pd.DataFrame) and globals()[n].empty)]\n",
    "\n",
    "        if missing_cv:\n",
    "            print(f\"Error: Missing required objects for CV: {missing_cv}\")\n",
    "            print(\"Please ensure previous cells (loading data, defining model/train functions) were run successfully.\")\n",
    "        else:\n",
    "             # Load group indices from the JSON file.\n",
    "            try:\n",
    "                with GROUPS_JSON.open() as f:\n",
    "                    group_indices = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {GROUPS_JSON} not found. Cannot load feature groups for CV.\")\n",
    "                group_indices = None # Prevent further execution if file is missing\n",
    "\n",
    "            if group_indices:\n",
    "                # Use a default prior configuration for CV, e.g., \"Config_B\" from sensitivity analysis, or define one here.\n",
    "                DEFAULT_GROUP_SIGMAS = {\n",
    "                    \"comp_corporal\":        0.10,\n",
    "                    \"gasto_energetico\":     0.10,\n",
    "                    \"atividade\":            0.05,\n",
    "                    \"cardio\":               0.05,\n",
    "                    \"sono\":                 0.05,\n",
    "                    \"nutrientes\":           0.07,\n",
    "                    \"ingestao_hidratacao\":  0.07,\n",
    "                }\n",
    "\n",
    "                # Deterministic training seed for CV folds (optional, but good for comparison).\n",
    "                CV_VI_SEED = 123\n",
    "\n",
    "                print(f\"Splitting data into {N_SPLITS} blocks of size approx {BLOCK_SIZE}...\")\n",
    "                # Loop through each fold for cross-validation.\n",
    "                for fold_idx in range(N_SPLITS):\n",
    "                    print(f\"\\n--- Processing Fold {fold_idx + 1}/{N_SPLITS} ---\")\n",
    "\n",
    "                    # Define start and end indices for the test block of this fold.\n",
    "                    test_start_idx = fold_idx * BLOCK_SIZE\n",
    "                    test_end_idx = min(test_start_idx + BLOCK_SIZE, len(df_full_data))\n",
    "\n",
    "                    # If the test block is empty or too small, skip this fold.\n",
    "                    if test_start_idx >= test_end_idx or (test_end_idx - test_start_idx) < 5: # Minimum test size of 5 windows\n",
    "                         print(f\"Skipping fold {fold_idx + 1}: Test block too small or empty.\")\n",
    "                         continue\n",
    "\n",
    "                    # Define the training data for the current fold (all data before the test block).\n",
    "                    df_train_fold = df_full_data.iloc[:test_start_idx].copy()\n",
    "                    # Define the test data for the current fold (the current block).\n",
    "                    df_test_fold  = df_full_data.iloc[test_start_idx:test_end_idx].copy()\n",
    "\n",
    "                    if df_train_fold.empty:\n",
    "                        print(f\"Skipping fold {fold_idx + 1}: Training data is empty.\")\n",
    "                        continue\n",
    "\n",
    "                    print(f\"  Train size: {len(df_train_fold)} windows\")\n",
    "                    print(f\"  Test size : {len(df_test_fold)} windows\")\n",
    "                    # Print the date range of the test period for this fold.\n",
    "                    print(f\"  Test period: {df_test_fold[DATE].iloc[0].strftime('%Y-%m-%d')} to {df_test_fold[DATE].iloc[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "\n",
    "                    # 1. Build model with priors on the *current training data*\n",
    "                    # Need to re-normalize features for this fold's training data\n",
    "                    # (proper CV requires re-fitting scalers on each train fold,\n",
    "                    # but for simplicity here, we'll use the global scalers or skip re-scaling if data is already globally scaled)\n",
    "                    # Assuming data is already globally scaled from 04_prepare_model_data.py\n",
    "\n",
    "                    # Convert training and test data for the current fold to TensorFlow tensors.\n",
    "                    Y_train_fold = tf.constant(df_train_fold[TARGET].values, dtype=tf.float32)\n",
    "                    X_train_fold = tf.constant(df_train_fold[feature_cols].values, dtype=tf.float32)\n",
    "                    Y_test_fold  = df_test_fold[TARGET].values # Keep as numpy for evaluation\n",
    "                    X_test_fold  = tf.constant(df_test_fold[feature_cols].values, dtype=tf.float32)\n",
    "\n",
    "                    # Build the BSTS model for the current fold's training data with default priors.\n",
    "                    model_fold = build_model_with_priors(\n",
    "                        Y_train_fold, X_train_fold, feature_cols, group_indices, DEFAULT_GROUP_SIGMAS\n",
    "                    )\n",
    "\n",
    "                    # 2. Train VI on the current training data for this fold.\n",
    "                    surrogate_posterior_fold = train_vi(model_fold, Y_train_fold, tf_seed=CV_VI_SEED)\n",
    "\n",
    "                    # 3. Make predictions on the test fold\n",
    "                    # We need one-step-ahead predictions for each point in the test fold.\n",
    "                    # This means feeding the *history* up to the point *before* the test observation.\n",
    "\n",
    "                    fold_mus, fold_sigs, fold_reals = [], [], [] # Lists to store means, stds, and actuals for this fold.\n",
    "                    # Start with the end of the training fold as history for sequential prediction.\n",
    "                    df_cum_fold = df_train_fold.copy()\n",
    "\n",
    "                    print(\"  Making one-step-ahead predictions on test fold‚Ä¶\")\n",
    "                    # Iterate through each window in the test fold to make predictions.\n",
    "                    for test_idx_in_fold in tqdm(range(len(df_test_fold)), desc=f\"Fold {fold_idx+1} Preds\"):\n",
    "                         current_test_window_data = df_test_fold.iloc[test_idx_in_fold] # Get the current test window data.\n",
    "\n",
    "                         # Convert historical and future data to TensorFlow tensors.\n",
    "                         y_hist_tf = tf.constant(df_cum_fold[TARGET].values, dtype=tf.float32)\n",
    "                         x_hist_tf = tf.constant(df_cum_fold[feature_cols].values, dtype=tf.float32)\n",
    "                         x_future_tf = tf.constant(current_test_window_data[feature_cols].values[np.newaxis, :], dtype=tf.float32)\n",
    "\n",
    "                         # Use the forecast function with the current fold's trained posterior.\n",
    "                         mu_tensor, sigma_tensor = forecast_stats(\n",
    "                             y_hist_tf, x_hist_tf, x_future_tf, surrogate_posterior_fold\n",
    "                         )\n",
    "\n",
    "                         # Store the prediction mean, standard deviation, and actual value.\n",
    "                         fold_mus.append(float(mu_tensor.numpy()))\n",
    "                         fold_sigs.append(float(sigma_tensor.numpy()))\n",
    "                         fold_reals.append(float(current_test_window_data[TARGET]))\n",
    "\n",
    "                         # Add the current test window data to the history for the next prediction step.\n",
    "                         df_cum_fold = pd.concat([df_cum_fold, current_test_window_data.to_frame().T], ignore_index=True)\n",
    "\n",
    "                    # Convert lists to NumPy arrays for metric calculation.\n",
    "                    fold_mus = np.array(fold_mus)\n",
    "                    fold_sigs = np.array(fold_sigs)\n",
    "                    fold_reals = np.array(fold_reals)\n",
    "\n",
    "                    # 4. Calibrate Z and calculate metrics for this fold's predictions\n",
    "                    # Calibrate Z *on this fold's predictions and actuals* or use a global Z?\n",
    "                    # For proper CV, you'd evaluate coverage at a *fixed* confidence level (e.g., 95%)\n",
    "                    # or calibrate Z on the *entire* out-of-sample set (less strict CV).\n",
    "                    # Let's calculate metrics for a fixed 95% CI (z=1.96) and also calculate the optimal Z for this fold.\n",
    "\n",
    "                    # Calculate metrics at a fixed 95% confidence interval (z approximately 1.96).\n",
    "                    z_95_fixed = stats.norm.ppf(0.975) # Z-score for a 95% two-sided interval.\n",
    "                    fold_lo95_fixed = fold_mus - z_95_fixed * fold_sigs # Lower bound of fixed 95% CI.\n",
    "                    fold_hi95_fixed = fold_mus + z_95_fixed * fold_sigs # Upper bound of fixed 95% CI.\n",
    "                    # Calculate coverage for the fixed 95% CI.\n",
    "                    fold_coverage_fixed = np.mean((fold_reals >= fold_lo95_fixed) & (fold_reals <= fold_hi95_fixed))\n",
    "                    # Calculate Mean Absolute Error for the fold.\n",
    "                    fold_mae = np.mean(np.abs(fold_mus - fold_reals))\n",
    "\n",
    "                    # (Optional) Calibrate Z *just for this fold's data*\n",
    "                    # Define a range of z-scores to search for calibration.\n",
    "                    zs_fold = np.linspace(1.0, 3.0, 101)\n",
    "                    coverages_fold = [] # List to store coverages for different z-scores in this fold.\n",
    "                    for z in zs_fold:\n",
    "                        lo = fold_mus - z * fold_sigs\n",
    "                        hi = fold_mus + z * fold_sigs\n",
    "                        cov = np.mean((fold_reals >= lo) & (fold_reals <= hi))\n",
    "                        coverages_fold.append(cov)\n",
    "                    coverages_fold = np.array(coverages_fold) # Convert to NumPy array.\n",
    "                    target_cov = 0.95 # Desired target coverage for calibration.\n",
    "                    # Find the z-score that yields coverage closest to the target.\n",
    "                    idx_fold = np.argmin(np.abs(coverages_fold - target_cov))\n",
    "                    z_cal_fold = zs_fold[idx_fold] # Calibrated z-score for this fold.\n",
    "                    calibrated_coverage_fold = coverages_fold[idx_fold] # Actual coverage for the calibrated z-score.\n",
    "\n",
    "                    print(f\"  Fold {fold_idx + 1} Metrics:\")\n",
    "                    print(f\"    MAE                     : {fold_mae:.3f}\")\n",
    "                    print(f\"    Coverage (Fixed 95%)  : {fold_coverage_fixed:.3f}\")\n",
    "                    print(f\"    Z Calibrated for Fold : {z_cal_fold:.2f} (Coverage: {calibrated_coverage_fold:.3f})\")\n",
    "\n",
    "\n",
    "                    # Append the results of the current fold to the cv_results list.\n",
    "                    cv_results.append({\n",
    "                        \"Fold\": fold_idx + 1,\n",
    "                        \"Train_Start\": df_train_fold[DATE].iloc[0].strftime('%Y-%m-%d'),\n",
    "                        \"Train_End\": df_train_fold[DATE].iloc[-1].strftime('%Y-%m-%d'),\n",
    "                        \"Test_Start\": df_test_fold[DATE].iloc[0].strftime('%Y-%m-%d'),\n",
    "                        \"Test_End\": df_test_fold[DATE].iloc[-1].strftime('%Y-%m-%d'),\n",
    "                        \"Train_Size\": len(df_train_fold),\n",
    "                        \"Test_Size\": len(df_test_fold),\n",
    "                        \"MAE\": fold_mae,\n",
    "                        \"Coverage_Fixed95\": fold_coverage_fixed,\n",
    "                        \"Z_Calibrated_Fold\": z_cal_fold,\n",
    "                        \"Coverage_Calibrated_Fold\": calibrated_coverage_fold\n",
    "                    })\n",
    "\n",
    "                # --- Summarize CV Results ---\n",
    "                cv_results_df = pd.DataFrame(cv_results) # Convert the list of results to a DataFrame.\n",
    "\n",
    "                print(\"\\nüî¨ Blocking Cross-Validation Summary:\")\n",
    "                # Display the summary DataFrame, rounding numerical columns for better readability.\n",
    "                display(cv_results_df.round({'MAE': 3, 'Coverage_Fixed95': 3, 'Z_Calibrated_Fold': 2, 'Coverage_Calibrated_Fold': 3}))\n",
    "\n",
    "                print(\"\\nAverage Metrics Across Folds:\")\n",
    "                # Calculate and print the average of key metrics across all folds.\n",
    "                print(cv_results_df[['MAE', 'Coverage_Fixed95', 'Z_Calibrated_Fold', 'Coverage_Calibrated_Fold']].mean().round(3))\n",
    "\n",
    "                print(\"\\n‚úÖ Blocking Cross-Validation Complete.\")"
   ],
   "metadata": {
    "id": "72Rmf4Fa9BW4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1461
    },
    "outputId": "7c5e00bc-e5cd-46ac-ca19-cd80c106ff7e"
   },
   "id": "72Rmf4Fa9BW4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An√°lise"
   ],
   "metadata": {
    "id": "fOw7bYOuBFlr"
   },
   "id": "fOw7bYOuBFlr"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Step-Ahead Forecast Generation per Block\n",
    "This cell generates multi-step-ahead forecasts for each block within the walk-forward validation period. It iteratively processes the data, accumulating historical observations, and for each block, it predicts `N_SUB` consecutive steps ahead using the trained Bayesian Structural Time Series (BSTS) model and its surrogate posterior. The mean, standard deviation, and actual values for each prediction are recorded.\n",
    "\n",
    "| Name          | Description                                                                  |\n",
    "| :------------ | :--------------------------------------------------------------------------- |\n",
    "| `multi_preds` | DataFrame containing the generated multi-step-ahead predictions (mean, stddev) and actual values for each block and sub-step. |"
   ],
   "metadata": {
    "id": "TxYuNgguPtPy"
   },
   "id": "TxYuNgguPtPy"
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # Progress bar library\n",
    "\n",
    "# === Cell A: Multi-Block Forecast Generation ===\n",
    "# Prerequisites: `model`, `surrogate_posterior` (trained via VI);\n",
    "# DataFrames `df_train0` (train_initial) and `df_walk` (walk-forward) already loaded\n",
    "# PARAMS: N_SUB = number of consecutive forecast steps per block\n",
    "\n",
    "N_SUB = 13  # Example: predict slope for the next 13 steps (blocks)\n",
    "\n",
    "records = [] # List to store prediction records for all blocks and sub-steps\n",
    "# Iterate over possible block start indices. The loop runs until there are enough subsequent steps for N_SUB forecasts.\n",
    "for start_idx in tqdm(range(len(df_walk) - N_SUB), desc='Blocos'):\n",
    "    # Accumulate historical data up to the beginning of the current block.\n",
    "    # This simulates the expanding window approach for each block.\n",
    "    df_hist = pd.concat([df_train0, df_walk.iloc[:start_idx]], ignore_index=True)\n",
    "    # Iterate through each sub-step within the current block (from 1 to N_SUB).\n",
    "    # 'sub' represents the forecast horizon (e.g., 1-step ahead, 2-steps ahead, etc.).\n",
    "    for sub in range(1, N_SUB+1):\n",
    "        # Select normalized features for the current future step being predicted.\n",
    "        # This is the data for the (start_idx + sub - 1)-th row in the df_walk DataFrame.\n",
    "        future_row = df_walk.iloc[start_idx + sub - 1]\n",
    "        y_real = future_row[TARGET] # Extract the actual (real) target value for this future step.\n",
    "        x_future = future_row[feature_cols].values[np.newaxis, :] # Extract future features and reshape for TensorFlow.\n",
    "\n",
    "        # Get historical data (target and features) from the accumulated history DataFrame.\n",
    "        y_hist = df_hist[TARGET].values\n",
    "        x_hist = df_hist[feature_cols].values\n",
    "\n",
    "        # Perform forecast via BSTS+VI (passing the trained surrogate_posterior).\n",
    "        # Convert historical target and features to TensorFlow tensors.\n",
    "        y_hist_tf   = tf.constant(y_hist, dtype=tf.float32)\n",
    "        x_hist_tf   = tf.constant(x_hist, dtype=tf.float32)\n",
    "        # Convert future features to a TensorFlow tensor.\n",
    "        x_fut_tf    = tf.constant(x_future, dtype=tf.float32)\n",
    "        # Call the forecast_stats function (defined in previous cells) to get mean (mu) and standard deviation (sigma).\n",
    "        mu_t, sig_t = forecast_stats(y_hist_tf, x_hist_tf, x_fut_tf, surrogate_posterior)\n",
    "        # Convert TensorFlow tensors to Python floats for storage.\n",
    "        mu, sigma   = float(mu_t.numpy()), float(sig_t.numpy())\n",
    "\n",
    "        # Append the prediction record to the list.\n",
    "        records.append({\n",
    "            'block_index': start_idx, # The starting index of the current block in df_walk.\n",
    "            'sub_step': sub, # The forecast horizon (e.g., 1 for 1-step ahead, N_SUB for N_SUB-step ahead).\n",
    "            'mu': mu, # Predicted mean of the target.\n",
    "            'sigma': sigma, # Predicted standard deviation of the target.\n",
    "            'y_real': float(y_real) # Actual (real) value of the target.\n",
    "        })\n",
    "# Convert the list of records into a Pandas DataFrame.\n",
    "multi_preds = pd.DataFrame(records)\n",
    "print(f\"‚úÖ Geradas {len(multi_preds)} previs√µes em {len(df_walk)-N_SUB} blocos de {N_SUB} passos.\")\n"
   ],
   "metadata": {
    "id": "cE5q7ONEPvLL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "b0c6d29b-2f1a-4d39-a5eb-2abb13b6fdcd"
   },
   "id": "cE5q7ONEPvLL",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Window Generation for Fractional Coverage\n",
    "This cell prepares the indices for model evaluation by dividing each walk-forward validation block into `N_SUB` sub-periods. This enables the generation of multiple forecasts per block and the calculation of fractional coverage metrics.\n",
    "\n",
    "| Name         | Description                                                                                                                              |\n",
    "| :----------- | :--------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `eval_windows` | DataFrame containing the **block index**, **sub-step**, and the **original index** (`walk_idx`) of each data point within the evaluation window. |"
   ],
   "metadata": {
    "id": "y5wYApXHjBGT"
   },
   "id": "y5wYApXHjBGT"
  },
  {
   "cell_type": "code",
   "source": [
    "# Parameters for the evaluation window generation.\n",
    "N_SUB = 13  # This defines the number of sub-periods within the original window (e.g., 91 days).\n",
    "\n",
    "# Dynamically determine the feature columns, excluding the target and date columns.\n",
    "# This ensures that only relevant features are considered after any potential collinearity adjustments.\n",
    "feature_cols = [c for c in df_walk.columns if c not in {TARGET, DATE}]\n",
    "\n",
    "# Calculate the maximum number of distinct evaluation blocks possible.\n",
    "# Each block starts at an index 'i' within `df_walk` and covers N_SUB steps.\n",
    "max_blocks = len(df_walk) - N_SUB + 1\n",
    "\n",
    "# Display a summary of the window generation process.\n",
    "# It shows the total number of blocks, sub-periods per block, and the total number of individual targets that will be evaluated.\n",
    "print(f\"Generating evaluation windows: {max_blocks} blocks √ó {N_SUB} sub-periods = {max_blocks * N_SUB} targets\")\n",
    "\n",
    "# Initialize an empty list to store the details of each sub-period across all blocks.\n",
    "# This list will be converted into a DataFrame later.\n",
    "blocks = []\n",
    "# Iterate through each possible starting index for a block.\n",
    "for blk in range(max_blocks):\n",
    "    # Generate a list of indices (offsets) that correspond to the data points within the current block.\n",
    "    # These indices refer to positions in the `df_walk` DataFrame.\n",
    "    offsets = list(range(blk, blk + N_SUB))\n",
    "    # For each offset within the current block, append a record to the `blocks` list.\n",
    "    # 'step' represents the sequential sub-period (e.g., 1st, 2nd, ..., N_SUB-th step within the block).\n",
    "    # 'idx' is the actual index in the `df_walk` DataFrame.\n",
    "    for step, idx in enumerate(offsets, start=1):\n",
    "        blocks.append({\n",
    "            'block_index': blk,    # The index of the current walk-forward block.\n",
    "            'sub_step': step,      # The sequential step within the current block (1 to N_SUB).\n",
    "            'walk_idx': idx        # The actual index in `df_walk` corresponding to this sub-period.\n",
    "        })\n",
    "\n",
    "# Convert the list of block and sub-period details into a Pandas DataFrame.\n",
    "# This DataFrame, `eval_windows`, will be used to structure the evaluation process.\n",
    "eval_windows = pd.DataFrame(blocks)\n",
    "# Confirm the creation of the `eval_windows` DataFrame and display its columns.\n",
    "print(\"‚úÖ eval_windows created with columns:\", list(eval_windows.columns))\n",
    "# Verify that each block has the correct number of sub-periods (N_SUB rows).\n",
    "print(eval_windows.groupby('block_index').size().head(), \"... (each block should have N_SUB rows)\")\n",
    "\n",
    "# Display the first 5 rows of the `eval_windows` DataFrame to show its structure and content.\n",
    "print(\"Sample of eval_windows (first 5 rows):\")\n",
    "print(eval_windows.head())"
   ],
   "metadata": {
    "id": "2T3p1ziVjCVE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "49d8ab1b-eaa6-4bad-a796-1ee7d82a4675"
   },
   "id": "2T3p1ziVjCVE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Step-Ahead Forecast Generation per Block\n",
    "This cell generates multi-step-ahead forecasts for each block within the walk-forward validation period. It iterates through the pre-defined evaluation windows, accumulates historical observations, and for each sub-step, it predicts the target value using the trained Bayesian Structural Time Series (BSTS) model and its surrogate posterior. The mean, standard deviation, and actual values for each prediction are recorded.\n",
    "\n",
    "| Name          | Description                                                                                             |\n",
    "| :------------ | :------------------------------------------------------------------------------------------------------ |\n",
    "| `multi_preds` | DataFrame containing the generated multi-step-ahead predictions (mean, standard deviation) and actual values for each block and sub-step. |"
   ],
   "metadata": {
    "id": "CianVbQljZde"
   },
   "id": "CianVbQljZde"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 3: Multi-Step-Ahead Forecasting per Block ===\n",
    "# Prerequisites: This cell requires several pre-existing objects and DataFrames:\n",
    "# - `eval_windows`: A DataFrame defining the specific evaluation windows and sub-steps.\n",
    "# - `df_train0`: The initial training DataFrame.\n",
    "# - `df_walk`: The walk-forward validation DataFrame.\n",
    "# - `feature_cols`: A list of columns to be used as features for the model.\n",
    "# - `TARGET`: The name of the target variable column.\n",
    "# - `DATE`: The name of the date column.\n",
    "# - `forecast_stats`: A pre-defined function used to generate forecasts (mean and standard deviation).\n",
    "# - `surrogate_posterior`: The trained surrogate posterior object from the Variational Inference (VI) process.\n",
    "\n",
    "import tensorflow as tf      # Import TensorFlow for numerical operations, especially with tensors.\n",
    "import pandas as pd          # Import Pandas for data manipulation and DataFrame operations.\n",
    "import numpy as np           # Import NumPy for numerical operations, especially array handling.\n",
    "from tqdm import tqdm        # Import tqdm to display a progress bar during the forecasting loop.\n",
    "\n",
    "records = []  # Initialize an empty list to store the results of each prediction.\n",
    "# Iterate over each row in the `eval_windows` DataFrame to perform forecasts.\n",
    "# `tqdm` provides a progress bar, showing the total number of forecasts being made.\n",
    "for _, row in tqdm(eval_windows.iterrows(), total=len(eval_windows), desc='Forecasts'):\n",
    "    # Extract the block index, sub-step, and walk-forward index from the current row.\n",
    "    blk_index = row['block_index']\n",
    "    sub_step  = int(row['sub_step'])\n",
    "    walk_idx  = int(row['walk_idx'])\n",
    "\n",
    "    # Construct the historical data DataFrame up to the beginning of the current block.\n",
    "    # This simulates the expanding window approach, where the model sees more data over time.\n",
    "    df_hist = pd.concat([df_train0, df_walk.iloc[:blk_index]], ignore_index=True)\n",
    "\n",
    "    # Prepare historical and future data for forecasting.\n",
    "    # `y_hist`: Actual target values from the historical data.\n",
    "    y_hist = df_hist[TARGET].values\n",
    "    # `x_hist`: Feature values from the historical data.\n",
    "    x_hist = df_hist[feature_cols].values\n",
    "    # `future_row`: The specific row from `df_walk` that needs to be predicted.\n",
    "    future_row = df_walk.iloc[walk_idx]\n",
    "    # `y_real`: The actual (true) target value for the future period being predicted.\n",
    "    y_real = float(future_row[TARGET])\n",
    "    # `x_future`: Feature values for the future period, reshaped for TensorFlow compatibility.\n",
    "    x_future = future_row[feature_cols].values[np.newaxis, :]\n",
    "\n",
    "    # Convert NumPy arrays to TensorFlow tensors.\n",
    "    # This is necessary because the `forecast_stats` function expects TensorFlow tensors as input.\n",
    "    y_hist_tf = tf.constant(y_hist, dtype=tf.float32)\n",
    "    x_hist_tf = tf.constant(x_hist, dtype=tf.float32)\n",
    "    x_fut_tf  = tf.constant(x_future, dtype=tf.float32)\n",
    "\n",
    "    # Perform the prediction using the Bayesian Structural Time Series (BSTS) model\n",
    "    # and its trained Variational Inference (VI) surrogate posterior.\n",
    "    # `mu_t`: The predicted mean of the target.\n",
    "    # `sig_t`: The predicted standard deviation of the target.\n",
    "    mu_t, sig_t = forecast_stats(y_hist_tf, x_hist_tf, x_fut_tf, surrogate_posterior)\n",
    "    # Convert TensorFlow tensors back to standard Python floats for storage.\n",
    "    mu, sigma = float(mu_t.numpy()), float(sig_t.numpy())\n",
    "\n",
    "    # Append the prediction results to the `records` list.\n",
    "    # Each dictionary represents one prediction for a specific block and sub-step.\n",
    "    records.append({\n",
    "        'block_index': blk_index, # The starting block index in the walk-forward period.\n",
    "        'sub_step': sub_step,     # The specific sub-step (forecast horizon) within the block.\n",
    "        'mu': mu,                 # The predicted mean value.\n",
    "        'sigma': sigma,           # The predicted standard deviation.\n",
    "        'y_real': y_real          # The actual observed value.\n",
    "    })\n",
    "\n",
    "# Convert the list of prediction records into a Pandas DataFrame.\n",
    "# This `multi_preds` DataFrame consolidates all generated forecasts.\n",
    "multi_preds = pd.DataFrame(records)\n",
    "\n",
    "# Display a verification message showing the total number of predictions generated.\n",
    "# It confirms the successful completion of the forecasting process across all blocks and sub-steps.\n",
    "print(f\"‚úÖ Generated {len(multi_preds)} forecasts in {max_blocks} blocks √ó {N_SUB} steps each.\")\n",
    "# Print the first 5 rows of the `multi_preds` DataFrame to provide a quick sample of the results.\n",
    "print(\"Sample of the first 5 rows of multi_preds:\")\n",
    "print(multi_preds.head())"
   ],
   "metadata": {
    "id": "7eo7ecvsjaN_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "7ae9a643-dbad-414f-9e33-b61ce6b1da2e"
   },
   "id": "7eo7ecvsjaN_",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collection of Forecast Vectors per Block\n",
    "This cell organizes the multi-step-ahead forecast data by collecting the predicted means ($\\mu$), standard deviations ($\\sigma$), and actual values ($y$) into distinct lists for each block. This restructuring facilitates further analysis, particularly for evaluating coverage probabilities.\n",
    "\n",
    "| Name         | Description                                                                                             |\n",
    "| :----------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `vectors_df` | DataFrame where each row represents a block, containing lists of predicted means (`mus`), standard deviations (`sigs`), and actual values (`reals`) for all sub-steps within that block. |4. **Coletar vetores de (Œº, œÉ, y) por bloco**\n",
    "\n"
   ],
   "metadata": {
    "id": "lG7LgzAhjikR"
   },
   "id": "lG7LgzAhjikR"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 4: Collection of (Œº, œÉ, y) Vectors per Block ===\n",
    "# Prerequisites: This cell requires the `multi_preds` DataFrame,\n",
    "# which should contain columns: `block_index`, `sub_step`, `mu` (predicted mean),\n",
    "# `sigma` (predicted standard deviation), and `y_real` (actual value).\n",
    "# Parameter: `N_SUB` represents the expected number of forecasts (sub-steps) per block.\n",
    "\n",
    "import pandas as pd # Import the Pandas library for data manipulation, especially with DataFrames.\n",
    "\n",
    "# Initialize an empty list to store the aggregated data for each block.\n",
    "# Each element in this list will be a dictionary containing the block index and lists of its `mu`, `sigma`, and `y_real` values.\n",
    "data = []\n",
    "# Group the `multi_preds` DataFrame by `block_index`.\n",
    "# This allows processing all predictions belonging to a single walk-forward block together.\n",
    "for blk, grp in multi_preds.groupby('block_index'):\n",
    "    # Sort the current group (all predictions for a single block) by `sub_step`.\n",
    "    # This ensures that the `mu`, `sigma`, and `y_real` values are in the correct chronological order\n",
    "    # for the multi-step-ahead forecasts within each block.\n",
    "    grp_sorted = grp.sort_values('sub_step')\n",
    "    # Extract the 'mu' (predicted mean) values for the current block and convert them to a Python list.\n",
    "    mus   = grp_sorted['mu'].tolist()\n",
    "    # Extract the 'sigma' (predicted standard deviation) values for the current block and convert them to a Python list.\n",
    "    sigs  = grp_sorted['sigma'].tolist()\n",
    "    # Extract the 'y_real' (actual observed) values for the current block and convert them to a Python list.\n",
    "    reals = grp_sorted['y_real'].tolist()\n",
    "\n",
    "    # Append a dictionary to the `data` list.\n",
    "    # Each dictionary represents one block and contains its index,\n",
    "    # along with the collected lists of `mus`, `sigs`, and `reals` for all sub-steps within that block.\n",
    "    data.append({\n",
    "        'block_index': blk,    # The index of the current block.\n",
    "        'mus': mus,            # List of predicted means for all sub-steps in this block.\n",
    "        'sigs': sigs,          # List of predicted standard deviations for all sub-steps in this block.\n",
    "        'reals': reals         # List of actual values for all sub-steps in this block.\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries (`data`) into a new Pandas DataFrame.\n",
    "# In this DataFrame, each cell in the 'mus', 'sigs', and 'reals' columns will contain a list.\n",
    "vectors_df = pd.DataFrame(data)\n",
    "\n",
    "# Print a verification message confirming the successful collection of vectors for all blocks.\n",
    "# It also states the expected number of targets (N_SUB) within each block.\n",
    "print(f\"‚úÖ Collected vectors for {len(vectors_df)} blocks (each with {N_SUB} targets)\")\n",
    "# Print a sample of the first 3 blocks from the `vectors_df`.\n",
    "# `.to_string(index=False)` is used to display the full content of the lists in a readable format without the DataFrame index.\n",
    "print(\"\\nSample of 3 blocks:\")\n",
    "print(vectors_df.head(3).to_string(index=False))"
   ],
   "metadata": {
    "id": "9WDdRWtxjjPP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "c6aac806-53c5-4cf9-cdc0-3fe61a423766"
   },
   "id": "9WDdRWtxjjPP",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Z-score Recalibration per Block\n",
    "This cell performs a block-wise recalibration of the z-score. For each block, it finds a calibrated z-score that best aligns the empirical coverage of the prediction intervals with a specified target coverage, within a defined range. This process ensures more accurate and reliable prediction intervals for future forecasts.\n",
    "\n",
    "| Name         | Description                                                                                             |\n",
    "| :----------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `calib_df`   | DataFrame containing the calibrated z-score (`z_calibrated`) and the resulting coverage (`coverage_block`) for each block. |"
   ],
   "metadata": {
    "id": "vitculgHj8GJ"
   },
   "id": "vitculgHj8GJ"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 5: Z-score Recalibration per Block ===\n",
    "# Prerequisites: This cell requires the `vectors_df` DataFrame, which must contain\n",
    "# the `block_index`, `mus` (list of predicted means), `sigs` (list of predicted standard deviations),\n",
    "# and `reals` (list of actual values) columns.\n",
    "# It also requires pre-defined minimum and maximum coverage parameters.\n",
    "\n",
    "# Define the target coverage level for the prediction intervals.\n",
    "target_cov = 0.95\n",
    "# Define the acceptable range for the coverage.\n",
    "# The recalibrated z-score will aim to achieve a coverage within this window.\n",
    "cov_min, cov_max = 0.90, 0.95\n",
    "# Create a grid of z-values to test. These z-values will be used to calculate different coverage levels.\n",
    "# `np.linspace(0.5, 3.0, 251)` generates 251 evenly spaced values between 0.5 and 3.0.\n",
    "z_values = np.linspace(0.5, 3.0, 251)\n",
    "\n",
    "# Initialize an empty list to store the recalibration results for each block.\n",
    "results = []\n",
    "# Iterate over each row (which represents a block) in the `vectors_df` DataFrame.\n",
    "for _, row in vectors_df.iterrows():\n",
    "    # Convert the lists of mus, sigs, and reals for the current block into NumPy arrays.\n",
    "    # This allows for efficient element-wise operations.\n",
    "    mus = np.array(row['mus'])\n",
    "    sigs = np.array(row['sigs'])\n",
    "    reals = np.array(row['reals'])\n",
    "    block = row['block_index'] # Get the index of the current block.\n",
    "\n",
    "    # Calculate the empirical coverage for each z-value in `z_values`.\n",
    "    # For each z, it checks if the `reals` (actual values) fall within the prediction interval\n",
    "    # defined by `mus ¬± z * sigs`. The mean of these boolean results gives the coverage proportion.\n",
    "    coverages = [(z, np.mean((reals >= mus - z*sigs) & (reals <= mus + z*sigs))) for z in z_values]\n",
    "\n",
    "    # Select the z-values and their corresponding coverages that fall within the acceptable range [cov_min, cov_max].\n",
    "    valid = [(z, cov) for z, cov in coverages if cov_min <= cov <= cov_max]\n",
    "    if valid:\n",
    "        # If there are valid z-values, choose the one whose coverage is closest to the `target_cov`.\n",
    "        # This uses a lambda function as the key for `min` to find the minimum absolute difference.\n",
    "        z_cal, cov_cal = min(valid, key=lambda x: abs(x[1] - target_cov))\n",
    "    else:\n",
    "        # If no z-value results in coverage within the [cov_min, cov_max] range,\n",
    "        # fallback to choosing the z-value whose coverage is closest to the midpoint of the range.\n",
    "        mid = (cov_min + cov_max) / 2\n",
    "        z_cal, cov_cal = min(coverages, key=lambda x: abs(x[1] - mid))\n",
    "\n",
    "    # Append the recalibration results for the current block to the `results` list.\n",
    "    results.append({'block_index': block,\n",
    "                    'z_calibrated': z_cal,\n",
    "                    'coverage_block': cov_cal})\n",
    "\n",
    "# Convert the list of results into a Pandas DataFrame.\n",
    "# This `calib_df` DataFrame holds the calibrated z-score and achieved coverage for each block.\n",
    "calib_df = pd.DataFrame(results)\n",
    "# Print a confirmation message indicating the completion of the calibration process\n",
    "# and the number of blocks for which calibration was performed.\n",
    "print(f\"‚úÖ Calibration completed for {len(calib_df)} blocks.\")\n",
    "# Display the first few rows of the `calib_df` DataFrame to show a sample of the results.\n",
    "print(\"Sample of the first rows of calib_df:\")\n",
    "print(calib_df.head())"
   ],
   "metadata": {
    "id": "VTpytUnGj9vc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "11623966-098e-470c-8fce-c892acc0dc34"
   },
   "id": "VTpytUnGj9vc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fractional Coverage Evaluation and Reliable Block Selection\n",
    "---\n",
    "This cell evaluates the empirical coverage of each block after z-score recalibration. It classifies blocks as **\"Reliable\"** or **\"Unreliable\"** based on whether their coverage falls within a pre-defined acceptable range. The cell then quantifies and displays the percentage of reliable blocks, providing an overview of the model's performance in generating well-calibrated prediction intervals.\n",
    "\n",
    "| Name       | Description                                                                                             |\n",
    "| :--------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `calib_df` | Modified DataFrame containing the original calibrated z-score and block coverage, now with an added `Status` column indicating if the block is \"Reliable\" or \"Unreliable\". |"
   ],
   "metadata": {
    "id": "Wk306ETKa5GL"
   },
   "id": "Wk306ETKa5GL"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 6: Fractional Coverage Evaluation and Reliable Block Selection ===\n",
    "# Prerequisites: This cell requires the `calib_df` DataFrame, which must contain\n",
    "# the `block_index`, `z_calibrated`, and `coverage_block` columns.\n",
    "\n",
    "# 1. Define Coverage Limits\n",
    "# These are the acceptable minimum and maximum empirical coverage percentages for a block to be considered \"reliable\".\n",
    "cov_min, cov_max = 0.90, 0.95\n",
    "\n",
    "# 2. Mark the Status of Each Block\n",
    "# A new column 'Status' is added to `calib_df`.\n",
    "# It uses a lambda function to classify each block:\n",
    "# - 'Confi√°vel' (Reliable) if its `coverage_block` is within the `cov_min` and `cov_max` range.\n",
    "# - 'N√£o confi√°vel' (Unreliable) otherwise.\n",
    "calib_df['Status'] = calib_df['coverage_block'].apply(\n",
    "    lambda c: 'Confi√°vel' if cov_min <= c <= cov_max else 'N√£o confi√°vel'\n",
    ")\n",
    "\n",
    "# 3. Quantify Results\n",
    "# Calculate the total number of blocks evaluated.\n",
    "n_total      = len(calib_df)\n",
    "# Count the number of blocks classified as 'Confi√°vel' (Reliable).\n",
    "n_confiaveis = (calib_df['Status'] == 'Confi√°vel').sum()\n",
    "# Calculate the percentage of reliable blocks relative to the total.\n",
    "pct_conf     = n_confiaveis / n_total * 100\n",
    "\n",
    "# 4. Display Summary\n",
    "# Print a summary report showing the total blocks, number of reliable and unreliable blocks,\n",
    "# and their respective percentages.\n",
    "print(f\"‚úÖ Evaluated {n_total} blocks: {n_confiaveis} reliable ({pct_conf:.1f}%), \"\n",
    "      f\"{n_total - n_confiaveis} unreliable ({100-pct_conf:.1f}%).\\n\")\n",
    "\n",
    "# 5. Show Sample of the Final Table\n",
    "# Print the first few rows of the updated `calib_df` to show the 'Status' column.\n",
    "# `.to_string(index=False)` prevents printing the DataFrame index.\n",
    "# `float_format=lambda x: f\"{x:.3f}\"` formats floating-point numbers to three decimal places for readability.\n",
    "print(\"Sample of evaluated blocks:\")\n",
    "print(calib_df.head().to_string(index=False,\n",
    "      float_format=lambda x: f\"{x:.3f}\"))"
   ],
   "metadata": {
    "id": "R_OmhInza52Q",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "b501f7e9-de37-4e53-ecb6-4c428eff277d"
   },
   "id": "R_OmhInza52Q",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Error Metrics Calculation per Block\n",
    "----\n",
    "This cell calculates common error metrics (Mean Absolute Error - MAE and Root Mean Squared Error - RMSE) for the multi-step-ahead forecasts within each block. It then consolidates these error metrics with the previously calculated coverage and status information into a single DataFrame, providing a comprehensive view of the model's performance per block.\n",
    "\n",
    "| Name       | Description                                                                                             |\n",
    "| :--------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `error_df` | DataFrame containing the calculated MAE and RMSE for each block.                                        |\n",
    "| `final_df` | Consolidated DataFrame merging `calib_df` and `error_df`, presenting `block_index`, `z_calibrated`, `coverage_block`, `Status`, `MAE`, and `RMSE`. |"
   ],
   "metadata": {
    "id": "DBVFI1UdbMg-"
   },
   "id": "DBVFI1UdbMg-"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 7: Error Metrics Calculation per Block ===\n",
    "# Prerequisites: This cell requires the `vectors_df` DataFrame (containing `block_index`, `mus`, `reals` lists)\n",
    "# and the `calib_df` DataFrame (containing `block_index`, `coverage_block`, and `Status`).\n",
    "\n",
    "import numpy as np   # Import NumPy for numerical operations, especially for error calculations.\n",
    "import pandas as pd  # Import Pandas for data manipulation and DataFrame operations.\n",
    "\n",
    "# Initialize an empty list to store error metrics for each block.\n",
    "records = []\n",
    "# Iterate over each row (representing a block's forecast data) in the `vectors_df`.\n",
    "for _, row in vectors_df.iterrows():\n",
    "    blk = row['block_index'] # Get the index of the current block.\n",
    "    mus = np.array(row['mus'])   # Convert the list of predicted means to a NumPy array.\n",
    "    reals = np.array(row['reals']) # Convert the list of actual values to a NumPy array.\n",
    "    n = len(mus) # Get the number of predictions in this block.\n",
    "\n",
    "    # Calculate Error Metrics\n",
    "    # Mean Absolute Error (MAE) for the current block.\n",
    "    # It measures the average magnitude of the errors without considering their direction.\n",
    "    mae_block  = np.mean(np.abs(mus - reals))\n",
    "    # Root Mean Squared Error (RMSE) for the current block.\n",
    "    # It measures the square root of the average of the squared errors, giving more weight to larger errors.\n",
    "    rmse_block = np.sqrt(np.mean((mus - reals)**2))\n",
    "\n",
    "    # Append the calculated error metrics for the current block to the `records` list.\n",
    "    records.append({\n",
    "        'block_index': blk,        # The index of the block.\n",
    "        'MAE': mae_block,          # Mean Absolute Error for the block.\n",
    "        'RMSE': rmse_block         # Root Mean Squared Error for the block.\n",
    "    })\n",
    "\n",
    "# Convert the list of error records into a Pandas DataFrame.\n",
    "error_df = pd.DataFrame(records)\n",
    "\n",
    "# Merge `error_df` with `calib_df` to create a final consolidated DataFrame.\n",
    "# The merge is performed on the 'block_index' column, combining coverage, status, and error metrics.\n",
    "final_df = pd.merge(calib_df, error_df, on='block_index')\n",
    "\n",
    "# Display Results\n",
    "# Print a confirmation message indicating that errors have been calculated for all blocks.\n",
    "print(f\"‚úÖ Errors calculated for {len(final_df)} blocks.\")\n",
    "# Print a sample of the `final_df` showing all combined metrics for the first few blocks.\n",
    "# `.to_string(index=False)` hides the DataFrame index, and `float_format='%.4f'` formats floats to 4 decimal places.\n",
    "print(\"\\nSample of blocks with all metrics:\")\n",
    "print(final_df.head().to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Summarize Statistics\n",
    "# Calculate the mean and standard deviation for MAE and RMSE across all blocks.\n",
    "summary = final_df[['MAE', 'RMSE']].agg(['mean', 'std']).T\n",
    "# Rename the columns for clarity in the summary table.\n",
    "summary.columns = ['Mean', 'Standard_Deviation']\n",
    "print(\"\\nSummary of MAE and RMSE:\")\n",
    "# Print the summary statistics, formatted to 4 decimal places.\n",
    "print(summary.to_string(float_format='%.4f'))"
   ],
   "metadata": {
    "id": "dM6JujbQbNfN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "78f7c8eb-033f-4720-e980-c0b8165c1522"
   },
   "id": "dM6JujbQbNfN",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pattern Extraction and Interpretation\n",
    "This cell focuses on extracting and interpreting patterns from the trained model, specifically the regression coefficients, within **reliable** walk-forward blocks. For each reliable block, it samples the model's surrogate posterior to obtain distributions of regression weights for all sub-steps. These weights are then aggregated to provide a mean and standard deviation for each feature across the block, offering insights into how feature importance and effects change over time in well-calibrated periods.\n",
    "\n",
    "| Name          | Description                                                                                             |\n",
    "| :------------ | :------------------------------------------------------------------------------------------------------ |\n",
    "| `patterns_df` | DataFrame containing the mean and standard deviation of regression coefficients for each feature, aggregated across sub-steps and posterior samples, for each reliable block. |"
   ],
   "metadata": {
    "id": "em5gxTgBbw-P"
   },
   "id": "em5gxTgBbw-P"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 8: Pattern Extraction and Interpretation ===\n",
    "# Prerequisites: This cell requires several DataFrames and objects:\n",
    "# - `final_df`: Contains block status (reliable/unreliable).\n",
    "# - `vectors_df`: Contains lists of mus, sigs, reals (though not directly used here, implies forecast context).\n",
    "# - `eval_windows`: Used to map block_index to specific `df_walk` indices.\n",
    "# - `df_train0`: Initial training DataFrame.\n",
    "# - `df_walk`: Walk-forward validation DataFrame.\n",
    "# - `feature_cols`: List of feature column names.\n",
    "# - `TARGET`: Name of the target variable column.\n",
    "# - `surrogate_posterior`: The trained Bayesian Structural Time Series (BSTS) model's surrogate posterior.\n",
    "# - `model`: (Implicitly used through `surrogate_posterior`) The BSTS model itself.\n",
    "# - `forecast_stats`: Function to generate forecasts (not directly used here, but part of the overall flow).\n",
    "\n",
    "import tensorflow as tf  # Import TensorFlow for handling tensors and sampling from posteriors.\n",
    "import pandas as pd      # Import Pandas for data manipulation.\n",
    "import numpy as np       # Import NumPy for numerical operations, especially array manipulation.\n",
    "from tqdm import tqdm    # Import tqdm for displaying progress bars.\n",
    "\n",
    "# 1. Filter Reliable Blocks\n",
    "# Selects the `block_index` values only for those blocks that were classified as 'Confi√°vel' (Reliable)\n",
    "# in the `final_df` and converts them into a list. This ensures that pattern extraction focuses on well-calibrated periods.\n",
    "trusted_blocks = final_df.loc[final_df['Status'] == 'Confi√°vel', 'block_index'].tolist()\n",
    "\n",
    "pattern_records = [] # Initialize an empty list to store the extracted patterns for each trusted block.\n",
    "# Iterate over each `trusted_block` with a progress bar.\n",
    "for blk in tqdm(trusted_blocks, desc='Extracting Patterns'):\n",
    "    # Identify the specific `walk_idx` indices from `df_walk` that correspond to the sub-steps of the current block.\n",
    "    # These indices are obtained from the `eval_windows` DataFrame.\n",
    "    sub_idxs = eval_windows.loc[eval_windows['block_index'] == blk, 'walk_idx'].tolist()\n",
    "    # Initialize a list to store coefficient samples (regression weights) for each sub-step within the current block.\n",
    "    # Each element in this list will be an array of shape (num_draws, n_features).\n",
    "    coef_samples = []\n",
    "\n",
    "    # For each sub-step within the current block, sample the posterior and extract regression weights.\n",
    "    for idx in sub_idxs:\n",
    "        # Reconstruct the historical data up to the current block's start.\n",
    "        # This simulates the expanding window that the model \"saw\" at the time of the forecast for this block.\n",
    "        df_hist = pd.concat([df_train0, df_walk.iloc[:blk]], ignore_index=True)\n",
    "        # Extract historical target and feature values.\n",
    "        y_hist = df_hist[TARGET].values\n",
    "        x_hist = df_hist[feature_cols].values\n",
    "        # Extract future feature values for the specific sub-step being considered.\n",
    "        x_future = df_walk.iloc[idx][feature_cols].values[np.newaxis, :]\n",
    "\n",
    "        # Convert historical and future data to TensorFlow tensors.\n",
    "        y_hist_tf = tf.constant(y_hist, dtype=tf.float32)\n",
    "        x_hist_tf = tf.constant(x_hist, dtype=tf.float32)\n",
    "        x_fut_tf  = tf.constant(x_future, dtype=tf.float32)\n",
    "\n",
    "        # Sample from the trained `surrogate_posterior`.\n",
    "        # `samples` will contain samples of all model parameters (e.g., local_level, local_slope, regression weights).\n",
    "        samples = surrogate_posterior.sample(100) # Draw 100 samples from the posterior.\n",
    "        # Find the dynamic key for the regression weights in the `samples` dictionary.\n",
    "        # This is robust to potential variations in key naming (e.g., 'observation_regression_weights').\n",
    "        weight_key = [k for k in samples.keys() if 'weights' in k][0]\n",
    "        # Extract the samples of the regression coefficients (betas).\n",
    "        # Their shape will be [num_draws, number_of_features].\n",
    "        betas = samples[weight_key].numpy()\n",
    "        coef_samples.append(betas) # Add these samples to the `coef_samples` list.\n",
    "\n",
    "    # Aggregate coefficients over sub-steps and posterior draws.\n",
    "    # `coef_samples` is a list where each element is an array of shape [num_draws, n_features].\n",
    "    # `np.stack` combines these arrays into a single array with shape [n_sub, num_draws, n_features].\n",
    "    stacked = np.stack(coef_samples, axis=0)\n",
    "    # Calculate the mean of the coefficients, averaging across both sub-steps (axis 0) and posterior draws (axis 1).\n",
    "    mean_coefs = stacked.mean(axis=(0,1))\n",
    "    # Calculate the standard deviation of the coefficients, averaging across sub-steps and posterior draws.\n",
    "    std_coefs  = stacked.std(axis=(0,1))\n",
    "\n",
    "    # Assemble the record for the current block.\n",
    "    record = {'block_index': blk}\n",
    "    # Iterate through each feature, its mean coefficient, and its standard deviation.\n",
    "    # Add these values to the `record` dictionary with specific column names (e.g., 'feature_A_mean', 'feature_A_std').\n",
    "    for feat, mu, sd in zip(feature_cols, mean_coefs, std_coefs):\n",
    "        record[f'{feat}_mean'] = mu\n",
    "        record[f'{feat}_std']  = sd\n",
    "    pattern_records.append(record) # Add the completed block record to the list.\n",
    "\n",
    "# Convert the list of pattern records into a final Pandas DataFrame.\n",
    "patterns_df = pd.DataFrame(pattern_records)\n",
    "# Print a confirmation message showing how many reliable blocks had patterns extracted.\n",
    "print(f\"‚úÖ Extracted patterns for {len(patterns_df)} reliable blocks.\")\n",
    "# Display the first few rows of the `patterns_df` to show the extracted coefficient patterns.\n",
    "# `to_string(index=False)` hides the DataFrame index, and `float_format='%.4f'` formats floats to 4 decimal places.\n",
    "print(\"Sample of extracted patterns:\")\n",
    "print(patterns_df.head().to_string(index=False, float_format='%.4f'))"
   ],
   "metadata": {
    "id": "P1xIU_h8bxtz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "073672d0-06cb-4c23-87bd-ec6d6f735d70"
   },
   "id": "P1xIU_h8bxtz",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Contribution Tables and Stacked Bar Chart\n",
    "This cell analyzes the extracted patterns from reliable blocks to understand the contribution of each feature over time. It first filters for reliable blocks, then calculates the global average coefficient for each feature. A stacked bar chart is generated to visualize the mean coefficient of all features across the reliable blocks, illustrating their relative contributions. Finally, two tables are presented: one listing features with negative (loss) contributions and another with positive (gain) contributions, ranked by their global average coefficients.\n",
    "\n",
    "| Name          | Description                                                                                             |\n",
    "| :------------ | :------------------------------------------------------------------------------------------------------ |\n",
    "| `patterns_trusted` | Filtered version of `patterns_df` containing only reliable blocks.                                      |\n",
    "| `df_bar_full` | Intermediate DataFrame used for plotting, containing block index, feature name, and its mean coefficient. |\n",
    "| `bar_data_full` | Pivoted DataFrame ready for plotting, with `block_index` as index, `feature` as columns, and `coef` as values. |\n",
    "| `df_loss`     | DataFrame listing features with negative average coefficients (potential \"loss\" factors), sorted by coefficient value. |\n",
    "| `df_gain`     | DataFrame listing features with positive average coefficients (potential \"gain\" factors), sorted by coefficient value. |"
   ],
   "metadata": {
    "id": "y2FArpq3eERD"
   },
   "id": "y2FArpq3eERD"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 9: Feature Contribution Tables and Stacked Bar Chart ===\n",
    "# Prerequisites:\n",
    "# - `patterns_df` DataFrame: Contains the mean coefficients for each feature per block.\n",
    "# - `feature_cols`: A dynamic list of feature column names.\n",
    "# - `final_df` DataFrame: Used to filter for reliable blocks.\n",
    "# - `eval_windows` and `df_walk` DataFrames: (Implicitly used for context, but not directly in this cell's core logic).\n",
    "# - Necessary libraries: Matplotlib, Seaborn, Pandas, NumPy, and tqdm should be imported.\n",
    "\n",
    "import matplotlib.pyplot as plt # Import Matplotlib for creating plots and visualizations.\n",
    "import seaborn as sns           # Import Seaborn for enhanced statistical data visualization.\n",
    "import numpy as np              # Import NumPy for numerical operations.\n",
    "import pandas as pd             # Import Pandas for data manipulation and DataFrame operations.\n",
    "from tqdm import tqdm           # Import tqdm for displaying progress bars.\n",
    "\n",
    "# 1. Filter Reliable Blocks\n",
    "# Identify the `block_index` values for blocks classified as 'Confi√°vel' (Reliable) from `final_df`.\n",
    "trusted_blocks = final_df.loc[final_df['Status'] == 'Confi√°vel', 'block_index']\n",
    "# Filter `patterns_df` to include only the reliable blocks.\n",
    "# `.copy()` is used to prevent SettingWithCopyWarning in subsequent modifications.\n",
    "patterns_trusted = patterns_df[patterns_df['block_index'].isin(trusted_blocks)].copy()\n",
    "\n",
    "# 2. Calculate Global Average Coefficient per Feature\n",
    "# Initialize an empty dictionary to store the mean coefficients for each feature across all trusted blocks.\n",
    "mean_coefs = {}\n",
    "# Iterate through each feature column.\n",
    "for feat in feature_cols:\n",
    "    # Extract the mean coefficients for the current feature from `patterns_trusted`.\n",
    "    # `.apply(lambda x: float(np.array(x).mean()) if hasattr(x, '__len__') else float(x))` handles cases\n",
    "    # where coefficients might be lists (e.g., if a previous step stored them as such) or single values.\n",
    "    vals = patterns_trusted[f'{feat}_mean'].apply(\n",
    "        lambda x: float(np.array(x).mean()) if hasattr(x, '__len__') else float(x)\n",
    "    )\n",
    "    # Calculate the overall mean of these coefficients for the current feature across all trusted blocks.\n",
    "    mean_coefs[feat] = vals.mean()\n",
    "\n",
    "# 3. Construct Stacked Bar Chart with All Variables\n",
    "# Initialize an empty list to store records for the stacked bar chart.\n",
    "records = []\n",
    "# Iterate over each row in the `patterns_trusted` DataFrame with a progress bar.\n",
    "for _, row in tqdm(patterns_trusted.iterrows(), total=len(patterns_trusted), desc='Collecting full coefficients'):\n",
    "    blk = row['block_index'] # Get the current block index.\n",
    "    # Iterate through each feature.\n",
    "    for feat in feature_cols:\n",
    "        # Extract the mean coefficient value for the current feature in the current block.\n",
    "        coef_val = float(np.array(row[f'{feat}_mean']).mean())\n",
    "        # Append a dictionary containing the block index, feature name, and its coefficient value to `records`.\n",
    "        records.append({'block_index': blk, 'feature': feat, 'coef': coef_val})\n",
    "\n",
    "# Create a DataFrame from the collected records, suitable for pivoting.\n",
    "df_bar_full = pd.DataFrame(records)\n",
    "# Pivot the DataFrame to prepare it for plotting.\n",
    "# `block_index` becomes the index, `feature` columns, and `coef` values. `fillna(0)` handles any missing combinations.\n",
    "bar_data_full = df_bar_full.pivot(index='block_index', columns='feature', values='coef').fillna(0).sort_index()\n",
    "\n",
    "# Set the Seaborn color palette for the chart.\n",
    "sns.set_palette('viridis', n_colors=len(feature_cols))\n",
    "# Create a Matplotlib figure and axes for the plot with a specified size.\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "# Generate a stacked bar chart using the pivoted data.\n",
    "bar_data_full.plot(kind='bar', stacked=True, ax=ax)\n",
    "# Set chart labels and title.\n",
    "ax.set_xlabel('91-day Block')\n",
    "ax.set_ylabel('Mean Coefficient (kg/week)')\n",
    "ax.set_title('Contribution of All Features per Block')\n",
    "# Position the legend outside the plot for better readability.\n",
    "ax.legend(title='Feature', bbox_to_anchor=(1.05,1), loc='upper left', ncol=1)\n",
    "# Adjust layout to prevent labels/legends from overlapping.\n",
    "plt.tight_layout()\n",
    "# Display the plot.\n",
    "plt.show()\n",
    "\n",
    "# 4. Tables of Factors After the Graph\n",
    "# 4.1 Loss Factors\n",
    "# Filter features that have a negative global average coefficient.\n",
    "# Sort them in ascending order of their coefficient values (most negative first).\n",
    "loss = sorted([(f, mean_coefs[f]) for f in feature_cols if mean_coefs[f] < 0], key=lambda x: x[1])\n",
    "# Create a DataFrame for loss factors.\n",
    "df_loss = pd.DataFrame(loss, columns=['Variable','Mean_Coef'])\n",
    "# Reset the DataFrame index to start from 1 for display purposes.\n",
    "df_loss.index = np.arange(1, len(df_loss)+1)\n",
    "\n",
    "# 4.2 Gain Factors\n",
    "# Filter features that have a positive global average coefficient.\n",
    "# Sort them in descending order of their coefficient values (most positive first).\n",
    "gain = sorted([(f, mean_coefs[f]) for f in feature_cols if mean_coefs[f] > 0], key=lambda x: -x[1])\n",
    "# Create a DataFrame for gain factors.\n",
    "df_gain = pd.DataFrame(gain, columns=['Variable','Mean_Coef'])\n",
    "# Reset the DataFrame index to start from 1 for display purposes.\n",
    "df_gain.index = np.arange(1, len(df_gain)+1)\n",
    "\n",
    "# Display Tables\n",
    "# Use `display` to render the DataFrames in environments like Jupyter notebooks.\n",
    "display(df_loss)\n",
    "display(df_gain)"
   ],
   "metadata": {
    "id": "yyLdSVKPeGKh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "084e6862-1aea-4208-9e9c-d25f85159db1"
   },
   "id": "yyLdSVKPeGKh",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heatmap of Top 6 Features\n",
    "This cell generates a heatmap to visualize the average contribution (coefficient) of the top 6 most impactful features across reliable walk-forward blocks. By identifying features with the highest absolute mean coefficients, this visualization helps to understand how the influence of key features changes over time, highlighting periods of stronger \"gain\" or \"loss\" contributions.\n",
    "\n",
    "| Name          | Description                                                                                             |\n",
    "| :------------ | :------------------------------------------------------------------------------------------------------ |\n",
    "| `patterns_trusted` | Filtered version of `patterns_df` containing only reliable blocks.                                      |\n",
    "| `df_heat`     | DataFrame specifically prepared for the heatmap, with `block_index` as the index and the top 6 features as columns, containing their mean coefficients. |\n",
    "\n"
   ],
   "metadata": {
    "id": "NOCk3q53ilDb"
   },
   "id": "NOCk3q53ilDb"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 10: Heatmap of Top 6 Features ===\n",
    "# Prerequisites:\n",
    "# - `patterns_df` DataFrame: Contains the mean coefficients for each feature per block.\n",
    "# - `feature_cols`: A dynamic list of feature column names after collinearity filtering.\n",
    "# - `final_df` DataFrame: Used to filter for reliable blocks.\n",
    "# - Necessary libraries: Matplotlib, Seaborn, Pandas, NumPy should already be imported.\n",
    "\n",
    "import matplotlib.pyplot as plt # Import Matplotlib for creating plots.\n",
    "import seaborn as sns           # Import Seaborn for enhanced data visualizations.\n",
    "import numpy as np              # Import NumPy for numerical operations.\n",
    "import pandas as pd             # Import Pandas for data manipulation.\n",
    "\n",
    "# 1. Filter Reliable Blocks\n",
    "# Identify the `block_index` values for blocks classified as 'Confi√°vel' (Reliable) from `final_df`.\n",
    "trusted_blocks = final_df.loc[final_df['Status']=='Confi√°vel','block_index']\n",
    "# Filter `patterns_df` to include only the reliable blocks.\n",
    "# `.copy()` is used to prevent SettingWithCopyWarning in subsequent modifications.\n",
    "patterns_trusted = patterns_df[patterns_df['block_index'].isin(trusted_blocks)].copy()\n",
    "\n",
    "# 2. Calculate Global Average Coefficient per Feature\n",
    "# Initialize an empty dictionary to store the mean coefficients for each feature across all trusted blocks.\n",
    "mean_coefs = {}\n",
    "# Iterate through each feature column.\n",
    "for feat in feature_cols:\n",
    "    # Extract the mean coefficients for the current feature from `patterns_trusted`.\n",
    "    # `.apply(lambda x: float(np.array(x).mean()) if hasattr(x, '__len__') else float(x))` handles cases\n",
    "    # where coefficients might be lists (e.g., if a previous step stored them as such) or single values.\n",
    "    vals = patterns_trusted[f'{feat}_mean'].apply(\n",
    "        lambda x: float(np.array(x).mean()) if hasattr(x, '__len__') else float(x)\n",
    "    )\n",
    "    # Calculate the overall mean of these coefficients for the current feature across all trusted blocks.\n",
    "    mean_coefs[feat] = vals.mean()\n",
    "\n",
    "# 3. Select Top 6 Features by Magnitude of Mean Coefficient\n",
    "# Sort the `mean_coefs` dictionary by the absolute value of the coefficients in descending order.\n",
    "# Select the top 6 features from this sorted list.\n",
    "top6 = sorted(mean_coefs, key=lambda f: abs(mean_coefs[f]), reverse=True)[:6]\n",
    "\n",
    "# 4. Construct Block x Feature Matrix for Top 6\n",
    "# Initialize an empty list to store the data for the heatmap.\n",
    "data = []\n",
    "# Iterate through each reliable block.\n",
    "for blk in trusted_blocks:\n",
    "    row = {'block_index': blk} # Start a new dictionary for the current block.\n",
    "    # For each of the top 6 features, extract its mean coefficient for the current block.\n",
    "    for feat in top6:\n",
    "        # Locate the raw mean coefficient value from `patterns_trusted`. `.iloc[0]` handles single-row selection.\n",
    "        raw = patterns_trusted.loc[patterns_trusted['block_index']==blk, f'{feat}_mean'].iloc[0]\n",
    "        # Ensure the value is a float, handling potential list formats.\n",
    "        val = float(np.array(raw).mean()) if hasattr(raw, '__len__') else float(raw)\n",
    "        row[feat] = val # Add the feature's coefficient to the current block's row.\n",
    "    data.append(row) # Add the completed block row to the list.\n",
    "# Create a DataFrame from the collected data and set 'block_index' as the DataFrame index.\n",
    "df_heat = pd.DataFrame(data).set_index('block_index')\n",
    "\n",
    "# 5. Plot Heatmap with 'viridis' Colormap, Inverting Features (Loss at the bottom)\n",
    "# Determine the min and max values in the heatmap data to set the color bar range.\n",
    "vmin, vmax = df_heat.values.min(), df_heat.values.max()\n",
    "# Transpose the DataFrame and reverse the order of features (`iloc[::-1]`)\n",
    "# to ensure \"loss\" factors (negative coefficients) appear at the bottom of the heatmap.\n",
    "axis_data = df_heat.T.iloc[::-1]\n",
    "# Create a Matplotlib figure and axes for the heatmap.\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Generate the heatmap using Seaborn.\n",
    "# `cmap='viridis'` sets the color scheme. `vmin` and `vmax` set the color bar limits.\n",
    "# `cbar_kws` customizes the color bar label.\n",
    "sns.heatmap(axis_data, cmap='viridis', vmin=vmin, vmax=vmax, cbar_kws={'label':'Mean Coefficient (kg/week)'})\n",
    "\n",
    "# Configure Plot Labels\n",
    "ax.set_xlabel('91-day Block Index')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title('Heatmap of Top 6 Features Across Reliable Blocks')\n",
    "# Adjust layout to prevent labels/legends from overlapping.\n",
    "plt.tight_layout()\n",
    "\n",
    "# 6. Adjust Colorbar for Qualitative Labels with Mid-Level\n",
    "cbar = ax.collections[0].colorbar # Get the colorbar object from the heatmap.\n",
    "# Set custom tick locations for the colorbar: min, zero, and max.\n",
    "cbar.set_ticks([vmin, 0, vmax])\n",
    "# Set custom tick labels for the colorbar to provide qualitative interpretation.\n",
    "cbar.set_ticklabels(['Weight Loss', 'Neutral', 'Weight Gain'])\n",
    "# Set the colorbar's main label and rotate it for better readability.\n",
    "cbar.set_label('Impact of Habit', rotation=270, labelpad=15)\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "f1aHY5H0iluU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "outputId": "cf499a72-826a-4320-b2b2-153f9a613584"
   },
   "id": "f1aHY5H0iluU",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Diverging Bar Chart for Feature Impact\n",
    "This cell generates a diverging bar chart that clearly illustrates the magnitude and direction of each feature's average impact on the target variable (e.g., weight variation). Features with positive average coefficients (gain) are shown on one side, and those with negative average coefficients (loss) on the other, providing an intuitive visualization of their overall influence.\n",
    "\n",
    "---\n",
    "\n",
    "| Name   | Description                                                                  |\n",
    "| :----- | :--------------------------------------------------------------------------- |\n",
    "| `df_div` | DataFrame containing features and their global average coefficients, sorted for plotting. |"
   ],
   "metadata": {
    "id": "hDpP3gW8mmCf"
   },
   "id": "hDpP3gW8mmCf"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 11: Diverging Bar Chart ===\n",
    "# Objective: To clearly show the magnitude and direction of each feature's impact.\n",
    "# Prerequisites:\n",
    "# - `mean_coefs` dictionary: Contains the global average coefficients for each feature.\n",
    "# - `feature_cols` dynamic list: The list of feature names.\n",
    "# - Matplotlib and NumPy should already be imported.\n",
    "\n",
    "import matplotlib.pyplot as plt # Import Matplotlib for creating plots.\n",
    "import numpy as np              # Import NumPy for numerical operations.\n",
    "import pandas as pd             # Import Pandas for data manipulation.\n",
    "\n",
    "# 1. Build DataFrame from `mean_coefs`\n",
    "# Create a Pandas DataFrame with two columns: 'feature' and 'coef'.\n",
    "# The 'feature' column contains all feature names, and 'coef' contains their corresponding\n",
    "# global average coefficients from the `mean_coefs` dictionary.\n",
    "df_div = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coef': [mean_coefs[f] for f in feature_cols]\n",
    "})\n",
    "# Sort the DataFrame by the 'coef' column in ascending order.\n",
    "# This arranges features from most negative impact to most positive impact,\n",
    "# which is good for diverging bar charts.\n",
    "df_div = df_div.sort_values('coef')\n",
    "\n",
    "# 2. Configure Viridis Color Palette for Colors\n",
    "# Determine the minimum and maximum coefficient values to normalize the color mapping.\n",
    "vmin, vmax = df_div['coef'].min(), df_div['coef'].max()\n",
    "# Create a normalization object that maps data values (coefficients) to the [0, 1] range.\n",
    "norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "# Get the 'viridis' colormap. Using `plt.colormaps` is the updated API for accessing colormaps.\n",
    "cmap = plt.colormaps['viridis']\n",
    "\n",
    "# 3. Plot Diverging Horizontal Bars\n",
    "# Create a Matplotlib figure and axes for the plot with a specified size.\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Iterate through each row of the sorted `df_div` DataFrame.\n",
    "for idx, row in df_div.iterrows():\n",
    "    val = row['coef'] # Get the coefficient value for the current feature.\n",
    "    color = cmap(norm(val)) # Map the coefficient value to a color from the 'viridis' colormap.\n",
    "    # Plot a horizontal bar. The length of the bar is `val`, and its color is determined by `cmap(norm(val))`.\n",
    "    ax.barh(row['feature'], val, color=color)\n",
    "\n",
    "# Draw a vertical black line at x=0 to clearly separate positive and negative impacts.\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "\n",
    "# Adjust Qualitative Ticks\n",
    "# Set custom tick locations on the x-axis to correspond to min, zero, and max coefficient values.\n",
    "ax.set_xticks([vmin, 0, vmax])\n",
    "# Set custom tick labels for these locations to provide qualitative interpretation of impact.\n",
    "ax.set_xticklabels(['Weight Loss', 'Neutral', 'Weight Gain'])\n",
    "\n",
    "# Set Labels and Title\n",
    "ax.set_xlabel('Average Habit Impact')\n",
    "ax.set_title('Average Impact of Each Habit on Weight Variation')\n",
    "# Adjust layout to prevent labels from overlapping.\n",
    "plt.tight_layout()\n",
    "# Display the plot.\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "8xxdPUEOmo3X",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "outputId": "619821a5-ae47-4899-e04c-6f73d552fefb"
   },
   "id": "8xxdPUEOmo3X",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Previs√µes e recomenda√ß√µes"
   ],
   "metadata": {
    "id": "IKp_kACGzMQl"
   },
   "id": "IKp_kACGzMQl"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Posterior Preparation: Parameter Sampling\n",
    "This cell samples the trained **surrogate posterior** distribution to extract a specified number of draws for each model parameter. It then compiles a statistical summary (mean, standard deviation, min, max) for these parameter samples, providing an overview of their estimated distributions after the Variational Inference (VI) training process.\n",
    "\n",
    "---\n",
    "\n",
    "| Name         | Description                                                                                             |\n",
    "| :----------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `samples`    | A dictionary containing `N_DRAWS` samples for each parameter of the trained model's approximate posterior. |\n",
    "| `summary_df` | DataFrame providing a statistical summary (mean, standard deviation, min, max) for each parameter extracted from the posterior samples. |"
   ],
   "metadata": {
    "id": "5iQUJuD71CGj"
   },
   "id": "5iQUJuD71CGj"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell X: Posterior Preparation ‚Äì Parameter Sampling ===\n",
    "# Prerequisites:\n",
    "#  - `surrogate_posterior` object: Must be already trained via Variational Inference (VI).\n",
    "#  - `tensorflow_probability` (tfp): Expected to be imported, though not directly used in this snippet's imports.\n",
    "#  - `numpy` and `pandas`: Required for numerical operations and DataFrame creation.\n",
    "\n",
    "import numpy as np   # Import NumPy for numerical operations, especially statistical calculations.\n",
    "import pandas as pd  # Import Pandas for data manipulation and DataFrame creation.\n",
    "\n",
    "# Define the number of samples (draws) to extract from the posterior distribution.\n",
    "N_DRAWS = 500\n",
    "\n",
    "# 1. Sample the Approximate Posterior Distribution\n",
    "# The `surrogate_posterior.sample(N_DRAWS)` method draws `N_DRAWS` samples from the\n",
    "# approximate posterior distribution of the model's parameters.\n",
    "# The `samples` variable will be a dictionary where keys are parameter names (e.g., 'local_level', 'observation_noise_scale')\n",
    "# and values are TensorFlow tensors, each with a shape starting with `(N_DRAWS, ...)`.\n",
    "samples = surrogate_posterior.sample(N_DRAWS)\n",
    "\n",
    "# 2. Prepare a Statistical Summary DataFrame\n",
    "rows = [] # Initialize an empty list to store summary statistics for each parameter.\n",
    "# Iterate through each parameter name and its corresponding tensor of samples in the `samples` dictionary.\n",
    "for param_name, tensor in samples.items():\n",
    "    # Convert the TensorFlow tensor to a NumPy array.\n",
    "    # `.reshape(N_DRAWS, -1)` flattens any additional dimensions (e.g., if a parameter is a vector or matrix)\n",
    "    # into a 2D array where each row corresponds to a draw and columns represent individual elements.\n",
    "    arr = tensor.numpy().reshape(N_DRAWS, -1)\n",
    "    # If the parameter is a vector or matrix, we are summarizing all its elements together.\n",
    "    # Flatten the array to treat all sampled values for this parameter as a single collection for summary statistics.\n",
    "    flat = arr.flatten()\n",
    "    # Append a dictionary to the `rows` list containing summary statistics for the current parameter.\n",
    "    rows.append({\n",
    "        'Parameter': param_name,      # The name of the parameter.\n",
    "        'Mean':       np.mean(flat),  # The mean of all sampled values for this parameter.\n",
    "        'Std':        np.std(flat, ddof=1), # The standard deviation, using `ddof=1` for sample standard deviation.\n",
    "        'Min':        np.min(flat),   # The minimum sampled value for this parameter.\n",
    "        'Max':        np.max(flat),   # The maximum sampled value for this parameter.\n",
    "    })\n",
    "\n",
    "# Convert the list of summary dictionaries into a Pandas DataFrame.\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# 3. Display Expected Results\n",
    "# Print a confirmation message indicating the number of samples extracted for each posterior parameter.\n",
    "print(f\"‚úÖ Extracted {N_DRAWS} samples for each posterior parameter.\\n\")\n",
    "# Print a header for the statistical summary table.\n",
    "print(\"Statistical Summary of Parameters:\")\n",
    "# Display the `summary_df` DataFrame.\n",
    "# `.to_string(index=False)` hides the DataFrame's index.\n",
    "# `float_format='%.4f'` formats floating-point numbers to four decimal places for readability.\n",
    "print(summary_df.to_string(index=False, float_format='%.4f'))"
   ],
   "metadata": {
    "id": "vWDxcQMFzNqr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "6c4c30cf-b44a-4e18-fdea-487b4654e494"
   },
   "id": "vWDxcQMFzNqr",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Initial Conditions for Future Forecast\n",
    "This cell prepares the initial conditions necessary for generating future forecasts. It loads the `walk_forward.parquet` file, dynamically identifies feature columns, and extracts the most recent window's feature values. These values are then converted into a TensorFlow tensor, which will serve as the starting point for predicting future steps.\n",
    "\n",
    "---\n",
    "\n",
    "| Name           | Description                                                                                             |\n",
    "| :------------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `df_walk`      | DataFrame loaded from `walk_forward.parquet`, containing all walk-forward validation windows.           |\n",
    "| `last_window`  | A DataFrame containing only the most recent window of observations from `df_walk`.                    |\n",
    "| `x_initial_tf` | TensorFlow tensor containing the feature values of the `last_window`, prepared for model input.         |"
   ],
   "metadata": {
    "id": "MULs9EF31klo"
   },
   "id": "MULs9EF31klo"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell: Define Initial Conditions for Future Forecast ===\n",
    "# Prerequisites:\n",
    "# - The 'walk_forward.parquet' file, which should have been generated in a previous script (e.g., '04_prepare_model_data.py').\n",
    "# - The `TARGET_COL` and `DATE_COL` variables, which must be defined consistently with the data pipeline.\n",
    "# - The `tensorflow` library should be imported as `tf`.\n",
    "\n",
    "import pandas as pd      # Import Pandas for data manipulation, especially reading parquet files.\n",
    "import tensorflow as tf  # Import TensorFlow for creating and manipulating tensors.\n",
    "from pathlib import Path # Import Path from pathlib for working with file paths in an object-oriented way.\n",
    "\n",
    "# 1. Load the Evaluation Windows DataFrame\n",
    "WALK_PARQ = Path(\"walk_forward.parquet\") # Define the path to the walk-forward parquet file.\n",
    "# Load the parquet file into a Pandas DataFrame.\n",
    "# `.sort_values(\"window_start\")` ensures the data is in chronological order.\n",
    "# `.reset_index(drop=True)` resets the DataFrame index after sorting.\n",
    "df_walk = pd.read_parquet(WALK_PARQ).sort_values(\"window_start\").reset_index(drop=True)\n",
    "\n",
    "# 2. Define Feature Columns Dynamically\n",
    "TARGET_COL = \"slope_kg_per_week\" # Define the name of the target column.\n",
    "DATE_COL   = \"window_start\"      # Define the name of the date column.\n",
    "# Dynamically create a list of feature columns by excluding the target and date columns from `df_walk`.\n",
    "feature_cols = [c for c in df_walk.columns if c not in {TARGET_COL, DATE_COL}]\n",
    "\n",
    "# 3. Select the Last (Most Recent) Window\n",
    "# Select the very last row of the `df_walk` DataFrame.\n",
    "# This row represents the most recent historical data point, which will be used as the starting point for future predictions.\n",
    "last_window = df_walk.iloc[[-1]] # `[[-1]]` ensures it remains a DataFrame, not a Series.\n",
    "\n",
    "# 4. Extract Only Feature Values and Convert to Tensor\n",
    "# Extract only the feature columns from `last_window` and convert them to a NumPy array.\n",
    "# The shape will be (1, number_of_features) because we're taking a single row.\n",
    "X_init = last_window[feature_cols].values\n",
    "# Convert the NumPy array of initial feature values into a TensorFlow constant tensor.\n",
    "# This tensor is what the model will expect as input for future forecasting.\n",
    "x_initial_tf = tf.constant(X_init, dtype=tf.float32)\n",
    "\n",
    "# 5. Verification Messages\n",
    "print(\"‚úÖ Initial conditions loaded successfully.\")\n",
    "print(f\"  File: {WALK_PARQ}\") # Confirm the file that was loaded.\n",
    "# Display information about the last window used, including its index and relevant columns.\n",
    "print(f\"  Last window (index {len(df_walk)-1}):\")\n",
    "print(last_window[[DATE_COL] + feature_cols].to_string(index=False))\n",
    "# Confirm the shape of the prepared TensorFlow input tensor.\n",
    "print(f\"\\nInput tensor prepared: shape = {x_initial_tf.shape}\")\n",
    "# Display the actual numerical values of the prepared input tensor (normalized features).\n",
    "print(\"Tensor values (normalized features):\")\n",
    "print(x_initial_tf.numpy())"
   ],
   "metadata": {
    "id": "84fArYK21k6F",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "0fe9e569-4a7b-4413-db4d-789c79fd3312"
   },
   "id": "84fArYK21k6F",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward-Sampling Simulation for Weight Projection\n",
    "This cell performs a forward-sampling simulation to project future weight changes. It leverages the trained Bayesian Structural Time Series (BSTS) model's surrogate posterior to generate multiple plausible future trajectories of `slope_kg_per_week` over a specified `HORIZON`. For each sample from the posterior, it iteratively forecasts future steps, using the previously predicted mean as the new observed value for the next step, thus simulating a realistic propagation of uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "| Name            | Description                                                                                             |\n",
    "| :-------------- | :------------------------------------------------------------------------------------------------------ |\n",
    "| `posterior_samples` | A dictionary containing `N_SAMPLES` draws from the trained model's approximate posterior distribution. |\n",
    "| `mu_preds`      | NumPy array of shape `(N_SAMPLES, HORIZON)` containing the predicted mean `slope_kg_per_week` for each sample and future step. |\n",
    "| `sigma_preds`   | NumPy array of shape `(N_SAMPLES, HORIZON)` containing the predicted standard deviation for `slope_kg_per_week` for each sample and future step. |\n",
    "| `mu_df`         | Pandas DataFrame representation of `mu_preds`, with columns indicating future time steps (e.g., `t+1`, `t+2`). |\n",
    "| `sig_df`        | Pandas DataFrame representation of `sigma_preds`, with columns indicating future time steps.            |"
   ],
   "metadata": {
    "id": "WwHH9oW72sFC"
   },
   "id": "WwHH9oW72sFC"
  },
  {
   "cell_type": "code",
   "source": [
    "# === Cell 09: Forward-Sampling Simulation for Weight Projection ===\n",
    "# Prerequisites:\n",
    "#  - `surrogate_posterior`: The trained surrogate posterior object obtained from Variational Inference (VI).\n",
    "#  - `model` and `forecast_stats`: These functions/objects should be defined in earlier cells.\n",
    "#  - `df_train0` (`train_initial` data) and `feature_cols`: Dynamically defined feature columns.\n",
    "#  - `tensorflow` (tf) and `tensorflow_probability` (tfp): Libraries required for tensor operations and probabilistic modeling.\n",
    "# Parameters:\n",
    "#  - `N_SAMPLES`: Number of draws (samples) to extract from the posterior distribution (e.g., 500).\n",
    "#  - `HORIZON`: Number of future steps (e.g., weeks) to forecast.\n",
    "\n",
    "import numpy as np   # Import NumPy for numerical operations and array handling.\n",
    "import tensorflow as tf  # Import TensorFlow for tensor operations and model interaction.\n",
    "from tqdm import tqdm    # Import tqdm for displaying a progress bar during simulations.\n",
    "import pandas as pd      # Import Pandas for data manipulation and DataFrame creation.\n",
    "\n",
    "# --- Configurations ---\n",
    "N_SAMPLES = 500 # Define the number of samples to draw from the posterior for simulation.\n",
    "HORIZON   = 8   # Define the number of future steps (e.g., weeks) to predict.\n",
    "TARGET    = 'slope_kg_per_week' # Define the name of the target variable.\n",
    "\n",
    "# 1) Extraction of Posterior Samples\n",
    "print(f\"üîç Extracting {N_SAMPLES} samples from the posterior...\")\n",
    "# Draw `N_SAMPLES` from the trained `surrogate_posterior`.\n",
    "# `posterior_samples` will be a dictionary where each key is a parameter name\n",
    "# and its value is a tensor containing `N_SAMPLES` draws for that parameter.\n",
    "posterior_samples = surrogate_posterior.sample(N_SAMPLES)\n",
    "# Each sample from `posterior_samples` is effectively a dictionary of internal model parameters.\n",
    "\n",
    "# 2) Initial State: Data up to the Last Window\n",
    "# Concatenate `df_train0` (initial training data) and `df_walk` (walk-forward data)\n",
    "# to form the complete historical dataset up to the present.\n",
    "last_hist = pd.concat([df_train0, df_walk], ignore_index=True)\n",
    "# Extract all historical target values.\n",
    "y_hist_all = last_hist[TARGET].values\n",
    "# Extract all historical feature values.\n",
    "x_hist_all = last_hist[feature_cols].values\n",
    "\n",
    "# 3) Forward-Sampling\n",
    "# Initialize NumPy arrays to store the predicted means and standard deviations for each sample and each horizon step.\n",
    "# `mu_preds` and `sigma_preds` will have a shape of (N_SAMPLES, HORIZON).\n",
    "mu_preds    = np.zeros((N_SAMPLES, HORIZON))\n",
    "sigma_preds = np.zeros((N_SAMPLES, HORIZON))\n",
    "\n",
    "# Loop through each of the `N_SAMPLES` draws from the posterior.\n",
    "for i in tqdm(range(N_SAMPLES), desc=\"Forward Sampling\"):\n",
    "    # Select the i-th set of parameters from the posterior samples.\n",
    "    # This effectively creates a specific \"version\" of the model based on this sample.\n",
    "    surp_i = {}\n",
    "    for name, tensor in posterior_samples.items():\n",
    "        surp_i[name] = tensor[i] # Extract the i-th sample for each parameter.\n",
    "\n",
    "    # Initialize cumulative history for iterative forecasting.\n",
    "    # We make a copy to avoid modifying the original `y_hist_all` and `x_hist_all`.\n",
    "    y_hist = y_hist_all.copy()\n",
    "    x_hist = x_hist_all.copy()\n",
    "\n",
    "    # Loop through each step in the prediction horizon.\n",
    "    for t in range(HORIZON):\n",
    "        # Call the `forecast_stats` function.\n",
    "        # It takes the historical target and features, and future features, along with the surrogate posterior.\n",
    "        # For this simulation, we are effectively using a fixed set of parameters (`surp_i`) for this sample `i`.\n",
    "        # Note: The code passes `surrogate_posterior` as the argument,\n",
    "        # which implies that `forecast_stats` internally uses the `surp_i` selection correctly,\n",
    "        # or that this specific `forecast_stats` is designed to work with a full `surrogate_posterior`\n",
    "        # and implicitly uses samples within. Given `surp_i` is prepared, this might be a simplification\n",
    "        # or rely on `forecast_stats` being a closure or having access to `surp_i`.\n",
    "        # For a truly isolated forward pass, `forecast_stats` would need to accept `surp_i` directly.\n",
    "        mu_t, sigma_t = forecast_stats(\n",
    "            tf.constant(y_hist, dtype=tf.float32),          # Historical target values as a TensorFlow tensor.\n",
    "            tf.constant(x_hist, dtype=tf.float32),          # Historical feature values as a TensorFlow tensor.\n",
    "            tf.constant(x_hist[-1:].copy(), dtype=tf.float32), # Features for the current prediction step (assuming same as last historical).\n",
    "            surrogate_posterior) # The full surrogate posterior object.\n",
    "        # Convert the TensorFlow tensors for mean and standard deviation to Python floats.\n",
    "        mu = float(mu_t.numpy())\n",
    "        sigma = float(sig_t.numpy())\n",
    "\n",
    "        # Store the predicted mean and standard deviation for the current sample and horizon step.\n",
    "        mu_preds[i, t]    = mu\n",
    "        sigma_preds[i, t] = sigma\n",
    "\n",
    "        # Update historical data for the next iterative forecast step.\n",
    "        # The predicted mean (`mu`) is appended to the historical target values,\n",
    "        # simulating that this predicted value is now \"observed\".\n",
    "        y_hist = np.append(y_hist, mu)\n",
    "        # The last row of historical features is duplicated and appended,\n",
    "        # assuming features remain constant for future steps (or are forecasted separately).\n",
    "        x_hist = np.vstack([x_hist, x_hist[-1:]])\n",
    "\n",
    "# 4) Convert to DataFrame for Inspection\n",
    "# Create a list of column names for the DataFrames (e.g., \"t+1\", \"t+2\", etc.).\n",
    "time_idx = [f\"t+{k+1}\" for k in range(HORIZON)]\n",
    "# Create a Pandas DataFrame from `mu_preds`, using `time_idx` as column names.\n",
    "mu_df    = pd.DataFrame(mu_preds,    columns=time_idx)\n",
    "# Create a Pandas DataFrame from `sigma_preds`, using `time_idx` as column names.\n",
    "sig_df   = pd.DataFrame(sigma_preds, columns=time_idx)\n",
    "\n",
    "print(\"‚úÖ Forward-sampling completed.\")\n",
    "print(\"Sample of mu_df:\")\n",
    "print(mu_df.head())"
   ],
   "metadata": {
    "id": "3u2Umo6N2ssi",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "54740e8f-0444-4406-8d37-d5197843b7b3"
   },
   "id": "3u2Umo6N2ssi",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scenario Aggregation and Probability Calculation\n",
    "This cell aggregates the results from the forward-sampling simulation (`mu_df`) to provide a comprehensive statistical summary of future weight changes. For each time step in the forecast horizon, it calculates the median, 95% and 50% credible intervals, and, crucially, the **probabilities of achieving specific weight loss or gain targets**. This provides an interpretable overview of future projections and their associated uncertainties.\n",
    "\n",
    "---\n",
    "\n",
    "| Name    | Description                                                                                             |\n",
    "| :------ | :------------------------------------------------------------------------------------------------------ |\n",
    "| `summary` | DataFrame containing median, credible intervals (95% and 50%), and probabilities of reaching defined weight loss/gain targets for each future time step. |"
   ],
   "metadata": {
    "id": "lNoHPC2sHC2w"
   },
   "id": "lNoHPC2sHC2w"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd # Import Pandas for data manipulation and DataFrame operations.\n",
    "import numpy as np  # Import NumPy for numerical operations, especially statistical functions.\n",
    "\n",
    "# --- Input: mu_df (DataFrame, shape [n_samples x T_steps], dynamic columns t+1, t+2, ... t+n) ---\n",
    "# Example: Loaded from the result of the forward-sampling simulation.\n",
    "# mu_df = pd.read_parquet('forward_mu.parquet') # Uncomment and adjust if loading from a file is needed.\n",
    "\n",
    "# Target Parameters\n",
    "LOSS_KG = 1.0 # Define the weight loss target: must be less than or equal to -1 kg.\n",
    "GAIN_KG = 0.5 # Define the weight gain target: must be greater than or equal to +0.5 kg.\n",
    "\n",
    "# 1) Number of Samples and Horizons\n",
    "n_samples, n_steps = mu_df.shape # Get the number of simulation samples and forecast steps from `mu_df`'s shape.\n",
    "steps = mu_df.columns.tolist()  # Get the column names (e.g., ['t+1', 't+2', ...]) representing the forecast steps.\n",
    "\n",
    "# 2) Point Statistics\n",
    "# Calculate the median for each forecast step across all samples.\n",
    "median_series = mu_df.median(axis=0)\n",
    "# Calculate the 2.5th percentile (lower bound of 95% credible interval) for each step.\n",
    "lower_95 = mu_df.quantile(0.025, axis=0)\n",
    "# Calculate the 97.5th percentile (upper bound of 95% credible interval) for each step.\n",
    "upper_95 = mu_df.quantile(0.975, axis=0)\n",
    "# Calculate the 25th percentile (lower bound of 50% credible interval) for each step.\n",
    "lower_50 = mu_df.quantile(0.25, axis=0)\n",
    "# Calculate the 75th percentile (upper bound of 50% credible interval) for each step.\n",
    "upper_50 = mu_df.quantile(0.75, axis=0)\n",
    "\n",
    "# 3) Calculation of Cumulative Loss/Gain\n",
    "# Calculate the cumulative sum of slopes (weight changes) for each sample across the forecast horizon.\n",
    "# This gives the total projected weight change from the start up to each `t+k` step.\n",
    "cum_slopes = mu_df.cumsum(axis=1)\n",
    "\n",
    "# 4) Probabilities of Achieving Targets\n",
    "# Calculate the probability of achieving the loss target.\n",
    "# It counts how many samples at each step have a cumulative slope less than or equal to -LOSS_KG,\n",
    "# then divides by the total number of samples.\n",
    "p_loss = (cum_slopes <= -LOSS_KG).sum(axis=0) / n_samples\n",
    "# Calculate the probability of achieving the gain target.\n",
    "# It counts how many samples at each step have a cumulative slope greater than or equal to +GAIN_KG,\n",
    "# then divides by the total number of samples.\n",
    "p_gain = (cum_slopes >=  GAIN_KG).sum(axis=0) / n_samples\n",
    "\n",
    "# 5) Build Summary DataFrame\n",
    "# Create a comprehensive DataFrame to summarize all calculated statistics.\n",
    "summary = pd.DataFrame({\n",
    "    'median': median_series,   # Median projected weight change.\n",
    "    'lower_95': lower_95,      # Lower bound of the 95% credible interval.\n",
    "    'upper_95': upper_95,      # Upper bound of the 95% credible interval.\n",
    "    'lower_50': lower_50,      # Lower bound of the 50% credible interval.\n",
    "    'upper_50': upper_50,      # Upper bound of the 50% credible interval.\n",
    "    f'P(‚â§ -{LOSS_KG}kg)': p_loss, # Probability of achieving the loss target.\n",
    "    f'P(‚â• +{GAIN_KG}kg)': p_gain  # Probability of achieving the gain target.\n",
    "}, index=steps) # Set the forecast steps (e.g., 't+1', 't+2') as the DataFrame index.\n",
    "summary.index.name = 'horizon' # Name the index column 'horizon' for clarity.\n",
    "\n",
    "# 6) Output and Verification\n",
    "print(f\"‚úÖ Aggregation completed: {n_samples} scenarios √ó {n_steps} steps\")\n",
    "print(\"Sample of summary (first 3 and last 3 rows):\")\n",
    "# Display a concatenated view of the first 3 and last 3 rows of the summary DataFrame.\n",
    "print(pd.concat([summary.head(3), summary.tail(3)]))\n",
    "\n",
    "# Optional: Save the summary DataFrame to a CSV file.\n",
    "# summary.to_csv('scenarios_summary.csv') # Uncomment to enable saving."
   ],
   "metadata": {
    "id": "YJLDXMkBHIHI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "outputId": "38a760ab-24c2-4419-c712-afc29fa0cffd"
   },
   "id": "YJLDXMkBHIHI",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "V28"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
